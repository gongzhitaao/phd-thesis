# Part 1 Generate Adversarial Texts

* Introduction
:PROPERTIES:
:CUSTOM_ID: chp:textadv:introduction
:END:

** Motivation
:PROPERTIES:
:CUSTOM_ID: sec:textadv:motivation
:END:

Many work in literature focus on the study of adversarial images.  These work
have provided us new perspectives to understand the mechanism of deep neural
nets, e.g., linear hypothesis cite:goodfellow2014-explaining, rethinking
smoothness assumptions cite:szegedy2013-intriguing.  In addition, many
algorithms have been proposed to enhance the robustness of the deep neural nets,
e.g., adversarial training cite:goodfellow2014-explaining.  Only a few attempts
have been made in the text domain.  We think it is worthwhile to investigate
adversarial samples for text models as well.  In this work, we propose a simple
yet effective framework to adapt the adversarial methods for images to text
domain.  Specifically, we first focus on model gradient-based methods since they
are very fast in practice.

** Problems
:PROPERTIES:
:CUSTOM_ID: sec:textadv:problems
:END:

The major problems to generate adversarial texts are two-folds:
1. /The input space is discrete/.  As a result, it is not possible to accumulate
   small noise computed from gradient directly in the input space.  However, the
   algorithms rely on accumulated gradients to perturb the samples.  They work
   well in image domain as image models usually take inputs in a continuous
   domain \([0, 1]\).
2. /The quality evaluation is intrinsically difficult/.  This is mainly because
   the quality of a sentence is rather subjective.  Besides, the judging
   criteria, if any, may vary based on situations, and even evolve over time.
   To illustrate the intricacies involved, let's compare an actual utterance
   from Master Yoda, /Much to learn, you still have/, with the usually way of
   speaking, /You still have much to learn/.  Which is better?  Are they equally
   good?  Start Wars fans will definitely favor the Yoda-style, while both
   sentences successfully convey the same meaning.

The first problem is relatively easier to solve.  In this work, we propose a
general framework with slightly modified model gradient-based methods.  We first
search for adversarials in the text embedding space cite:mikolov2013-efficient
via gradient-based methods, and then reconstruct the adversarial texts.  For
word-level models, (approximate) nearest search is used to align noisy
embeddings to legit ones.  For character-level models, we directly reverse the
noisy embeddings by cosine distance.  The embedding space is continuous, thus
allowing us to accumulate small noise iteratively.

The second problem is open-ended.  We employ Word Mover's Distance
(WMD) cite:kusner2015-from to measure the difference between an adversarial text
piece with the original one.  Obtaining a score from a trained language model
was used in cite:anonymous2018-adversarial.  In machine translation, BLEU
score cite:papineni2002-bleu is used to quantify the quality of translated text
piece.  However, it is rather difficult (impossible, at least from my point of
view) to argue which metric is better.

** Outline

We briefly review recent work on generating adversarial images and texts in
Chapter ref:chp:textadv:related-work.  Our adversarial text framework is
detailed in Chapter ref:chp:textadv:adversarial-text-framework.  Experiments and
preliminary results are reported in Chapter ref:chp:textadv:experiment.

# We conclude this part and provide directions for future work in
# Chapter ref:chp:textadv:conclusion.

* Related Work
:PROPERTIES:
:CUSTOM_ID: chp:textadv:related-work
:END:

The existence of adversarial samples was first discussed
in cite:szegedy2013-intriguing.  There has been an abundance of work on methods
to generate adversarial images.  These adversarial images raise concerns about
the wide applications of deep neural nets cite:kurakin2016-adversarial.  As a
result, many work have investigated the defense against these adversarial
samples.  However, so far as we see in literature, the attacking is much easier
and cheaper than defense.

For notation, \(x\) denotes the input, \(y\) the prediction, \(f\) the target
model such that \(y = f(x)\), \(L\) the loss function, \(x^*\) the adversarial
sample generated based on \(x\).  \(\|\cdot\|_p\) denotes the \(p\)-norm.  We
slightly abuse the notation here, \(L_x\) denotes the loss with \(x\) as the
input.

** Generate Adversarial Images
:PROPERTIES:
:CUSTOM_ID: sec:textadv:generate-adversarial-image
:END:

The methods mainly fall into three categories, /model gradient-based/,
/adversarial noise minimization/ and /generative model-based/.  Generally
speaking, model gradient-based is much faster than the rest.  However, the other
two require much less knowledge about the model, thus more practical.  In
addition, noise minimization are usually more effective and generate more subtle
noise.

*** Model Gradient-based
:PROPERTIES:
:CUSTOM_ID: subsec:textadv:model-gradient-based
:END:

This class of methods compute the noise based on Jacobian or loss gradients of
the target models, thus requiring a full knowledge of the target model.  Fast
gradient method (FGM) cite:goodfellow2014-explaining and its variants, iterative
FGM and targeted FGM cite:kurakin2016-adversarial, add to the whole image the
noise that is proportional to either \(\nabla L_x\) (FGVM) or \(\sign(\nabla
L_x)\) (FGSM).  Jacobian-based saliency map approach
(JSMA) cite:papernot2015-limitations, on the contrary, perturbs one pixel at a
time.  It chooses the pixel with the highest saliency score, which is calculated
as \(-\nabla y_t\cdot\nabla y_o\) subject to \(\nabla y_t > 0\), where \(y_t\)
is the probability for the target class, and \(y_o\) is the sum of probabilities
of all other classes.  Intuitively, JSMA tries to increase the probability of
the target class while decreasing others.
DeepFool cite:moosavi-dezfooli2015-deepfool iteratively finds the optimal
direction in which we need to /travel/ the minimum distance to cross the
decision boundary of the target model.  Although in non-linear case, the
optimality is not guaranteed, in practice, however, DeepFool usually finds very
subtle noise compared to other gradient methods.

*** Adversarial Noise Minimization
:PROPERTIES:
:CUSTOM_ID: subsec:textadv:adversarial-noise-minimization
:END:

This class of methods are usually black-box, i.e., using the target model only
as an oracle without knowledge of its parameters, gradients, etc.  Generally
speaking, this class minimizes \(\|x^* - x\|_p\) subject to \(f(x^*)\neq f(x)\)
and \(x^*\) is in the input domain.  This formulation is a box-constrained
optimization problem, L-BFGS is used in cite:szegedy2013-intriguing to solve it.
In order to utilize more optimization methods, cite:carlini2016-towards proposes
a refinement of the above formulation.  Instead of working in the original image
space, it searches for noise in a transformed space by minimizing \(\|s(x^*) -
x\|_p - L_{s(x^*)}\), where \(s\) is a squashing function that keeps \(x^*\)
within the input domain, e.g., =sigmoid= for images in the domain \([0, 1]\).
Based on the reformulation, many work provide interesting insight into
adversarial images.  cite:moosavi-dezfooli2016-universal shows that, instead of
applying different noise to each image, it is possible to apply the same noise,
i.e., a universal perturbation, to different images, such that the resulting
images still trick the target model in most cases.  The one-pixel change may
also turn a clean image into an adversarial one cite:su2017-one.

*** Generative Model-based
:PROPERTIES:
:CUSTOM_ID: subsec:textadv:generative-model-based
:END:

Similar to the noise minimization, this class also formulates the problem as an
optimization problem.  The difference is that, instead of performing the
optimization directly, this class trains a separate model to map the input to
noise or adversarial samples.  Adversarial transformation network
(ATN) cite:baluja2017-adversarial trains a separate model \(g\) that minimizes
\(\beta\|x^*-x\|_p + \|f(x^*)-f(x)\|_{p^\prime}\), where \(g(x) = x^*\).  The
ATN may be used to generate adversarial noise or samples from the clean input.
cite:zhao2017-generating proposes to first create a mapping between the input
space and a random noise space, and then search in the noise space for potential
adversarials which are verified by being mapped back to the input space.  To
create the mapping between input and noise space, the authors propose an
autoencoder structure which consists of a) an encoder \(G\), a generator network
that maps the random noise \(z\) to the input \(x\), \(G(z) = x\), and b) a
decoder \(I\) (referred to as \textsl{inverter}), another generator network that
maps the input to the random noise, \(I(x) = z\).  Generative Adversarial
Network (GAN) cite:goodfellow2014-generative is used for both generator
networks.  The whole network is trained end-to-end by minimizing the loss
\(\mathbb{E}_x\|G(I(z)) - x\|_p + \lambda\mathbb{E}_z\|I(G(x)) - z\|_p\).

** Generate Adversarial Texts
:PROPERTIES:
:CUSTOM_ID: sec:textadv:generate-adversarial-texts
:END:

Most work in the previous section focus on image models.  As we have discussed,
the main problem to generate adversarial texts are the discrete input space and
the lack of quality measurement.  The aforementioned model
attack cite:zhao2017-generating is a viable workaround for the first problem
since the noise space is smooth.  However, the disadvantage with their method is
that they do not have an explicit control of the quality of the generated
adversarial samples.  As we have seen in cite:zhao2017-generating, the generated
adversarial images on complex dataset usually have large visual changes.
Generally, the proposal methods in literature can be classified into two
categories by the space where they search for the adversarial texts.  The first
class of methods work in the raw input text space, while the other in a
transformed space.

There are, in general, three ways to alter a sentence, /replacement/, /deletion/
and /insertion/.  Each has its own traits.  Replacement is most straightforward
and widely used since it is relatively easier to maintain the grammar and syntax
correctness compared to the other two.  Deletion is easier to implement since we
only need to identify in some way the important features.  Insertion is much
more difficult mostly because we need to carefully find the word or construct a
legit sentence that does not interfere with the meaning of original text piece.
The sequence generation is, in itself, an active research area.

*** Text-space Methods
:PROPERTIES:
:CUSTOM_ID: subsec:textadv:text-space-method
:END:

This class of methods follow a similar strategy.
1. Identify the features (e.g., characters, words) that have the most influence
   on the prediction, and then
2. follow different strategies to perturb these features according to a pool of
   candidates.

In essence, this class of methods are similar to
JSMA cite:papernot2015-limitations, in which the intensity of the pixel with the
highest score is increased or decreased.  The Jacobian value \(\nabla f\) or the
loss gradient \(\nabla L\) are usually employed to construct a measurement for
the feature importance, e.g., \(\nabla L\) is used in cite:liang2017-deep to
select important characters and phrases to perturb.  The perturbation candidates
usually include typos, synonyms, antonyms, frequent words in each category, and
other task-dependent features.  For example, typos, synonyms, and important
adverbs and adjectives are used as candidates for insertion and replacement
in cite:samanta2017-towards.  cite:jia2017-adversarial manually construct
distracting yet legit sentences to overshadow the important sentences.
cite:anonymous2018-adversarial iteratively replace each word with its nearest
neighbors in the embedding space until success or a threshold is reached.

Despite being intuitive, this class of methods are computationally expensive,
mainly because searching in a large yet discrete space is intrinsically
difficult.  Some of these methods also heavily rely on manual features which
does not scale in practice.

*** Transformed-space Methods
:PROPERTIES:
:CUSTOM_ID: subsec:textadv:transformed-space-method
:END:

In order to employ the powerful search strategies that only work in continuous
space, the other line of work follows a different strategy.
1. Map the raw texts into a continuous space,
2. search for potential adversarial in the transformed space,
3. reconstruct adversarial texts from the transformed space.

There are usually two ways to map from text space to a continuous space:
1. Word level encoding, i.e., text embedding cite:mikolov2013-linguistic.
2. Character level encoding, e.g., cite:kim2015-character.

In cite:liang2017-deep, the authors attempt applying FGM directly on
character-level CNN cite:zhang2015-character.  Although the labels of the text
pieces are altered, the texts are changed to random stream of characters beyond
recognition.  A black-box attack based on GAN is proposed cite:wong2017-dancin.
Hotflip cite:ebrahimi2017-hotflip focuses on character-level model.  It replaces
one character at a time which maximizes the increase in loss.  This is in
principle the same as JSMA.  The aforementioned work cite:zhao2017-generating
employs autoencoder structure, in which the encoder maps the input texts to a
Gaussian noise space, while the decoder maps the noise back to text space to
reconstruct the potential adversarial texts.

* Adversarial Text Framework
:PROPERTIES:
:CUSTOM_ID: chp:textadv:adversarial-text-framework
:END:

Our method is more general than aforementioned methods, beside, the computation
is really fast.  In this section, we present our framework that generates
adversarial texts by noise generated computed from model gradients.

** Discrete Input Space
:PROPERTIES:
:CUSTOM_ID: sec:textadv:discrete-input-space
:END:

In order to work in a continuous space, our framework first searches for
adversarial texts in the text or character embedding space, then reconstructs
the adversarial sentences with nearest neighbor search.  Searching for
adversarials in the embedding space is similar in principle to searching for
adversarial images.  However, the generated noisy embedding vectors usually do
not correspond to any tokens in the text space.  To construct the adversarial
texts, we align each embedding to its nearest one.  We can use (approximate)
nearest neighbor search if the vocabulary size is large, or direct embedding
reverse by cosine distance if the embedding matrix is relative small.  This
reconstructing process can be seen as a strong /denoising/ process.  With
appropriate noise scale, we would expect most of the tokens/characters remain
unchanged, with only few replaced.  This framework builds upon the following
observations.

1. In the gradient-based methods, the input features (e.g., pixels, tokens,
   characters) that are relatively more important for the final predictions will
   receive more noise, while others relatively less noise.  The is actually the
   core property of the gradient-based methods.
2. The embedded word vectors preserve the subtle semantic relationships among
   words cite:mikolov2013-efficient,mikolov2013-distributed.  For example,
   =vec("clothing")= is closer to =vec("shirt")= as =vec("dish")= to
   =vec("bowl")=, while =vec("clothing")= is far away from =vec("dish")= in
   terms of \(p\)-norm, since they are not semantically
   related cite:mikolov2013-linguistic.  This property assures that it is more
   likely to replace the victim words with a semantically related one rather
   than a random one.

** Word Mover's Distance (WMD)
:PROPERTIES:
:CUSTOM_ID: sec:textadv:wmd
:END:

The second problem we need to resolve is the choice of quality metric for
generated adversarial texts, so that we have a scalable way to measure the
effectiveness of our framework.  We employ the Word Mover's Distance
(WMD) cite:kusner2015-from as the metric.  WMD measures the dissimilarity
between two text documents as the minimum amount of distance that the embedded
words of one document need to /travel/ to reach the embedded words of another
document.  WMD can be considered as a special case of Earth Mover's Distance
(EMD) cite:rubner2000-earth.  Intuitively, it quantifies the semantic similarity
between two text bodies.  In this work, WMD is closely related to the ratio of
number of words changed to the sentence length.  However, we plan to extend our
framework with paraphrasing and insertion/deletion, where the sentence length
may change.  In that case, WMD is more flexible and accurate.

* Experiment
:PROPERTIES:
:CUSTOM_ID: chp:textadv:experiment
:END:

We evaluate our framework on three text classification problems.
Section ref:sec:textadv:dataset details on the data preprocessing.  The
adversarial algorithms we use are (FGM) cite:goodfellow2014-explaining and
DeepFool cite:moosavi-dezfooli2015-deepfool.  We tried JSMA, however, due to the
mechanism of JSMA, it is not directly applicable in our framework.  We report in
Section ref:sec:textadv:results the original model accuracy, accuracy on
adversarial embeddings, and accuracy on reconstructed adversarial texts in our
experiment.  Only a few examples of generated adversarial texts are shown in
this paper due to the space constraint.  The complete sets of adversarial texts
under different parameter settings and the code to reproduce the experiment are
available online[fn:1].

Computation-wise, the bottleneck in our framework is the nearest neighbor
search.  Word vector spaces, such as GloVe cite:pennington2014-glove, usually
have millions or billions of tokens embedded in very high dimensions.  The
vanilla nearest neighbor search is almost impractical.  Instead, we employ the
an approximate nearest neighbor (ANN) technique in our experiment.  The ANN
implementation which we use in our experiment is Approximate Nearest Neighbors
Oh Yeah (=annoy=)[fn:2], which is well integrated into =gensim=
cite:rek2010-software package.

** Dataset
:PROPERTIES:
:CUSTOM_ID: sec:textadv:dataset
:END:

We use three text datasets in our experiments.  The datasets are summarized in
Table ref:tab:datasets.  The last column shows our target model accuracy on
clean test data.

#+ATTR_LaTeX: :booktabs t :width .8\textwidth
#+CAPTION: Dataset Summary
#+NAME: tab:datasets
| Dataset   | Labels | Training | Testing | Max Length | Accuracy |
|-----------+--------+----------+---------+------------+----------|
| IMDB      |      2 |    25000 |   25000 |        300 |   0.8787 |
| Reuters-2 |      2 |     3300 |    1438 |        100 |   0.9854 |
| Reuters-5 |      5 |     1735 |     585 |        100 |   0.8701 |

*** IMDB Movie Reviews
:PROPERTIES:
:CUSTOM_ID: subsec:textadv:generative-model-based
:END:

This is a dataset for binary sentiment classification cite:maas2011-learning.
It contains a set of 25,000 highly polar (positive or negative) movie reviews
for training, and 25,000 for testing.  No special preprocessing is used for this
dataset except that we truncate/pad all the sentences to a fixed maximum
length, 400.  This max length is chosen empirically.

*** Reuters
:PROPERTIES:
:CUSTOM_ID: subsec:textadv:reuters
:END:

This is a dataset of 11,228 newswires from Reuters, labeled over 90 topics.  We
load this dataset through the NLTK cite:bird2009-natural package.  The raw
Reuters dataset is highly unbalanced.  Some categories contain over a thousand
samples, while others may contain only a few.  The problem with such highly
unbalanced data is that the texts that belong to under-populated categories are
almost always get classified incorrectly.  Even though our model may still
achieve high accuracy with 90 labels, it would be meaningless to include these
under-populated categories in the experiment since we are mainly interested in
perturbation of those samples that are already being classified correctly.
Keras[fn:3] uses 46 categories out of 90.  However, the 46 categories are still
highly unbalanced.  In our experiment, we preprocess Reuters and extract two
datasets from it.

**** Reuters-2

It contains two most populous categories, i.e., =acq= and =earn=.  The =acq=
category contains 1650 training samples and 719 test samples.  Over 71%
sentences in the =acq= category have less than 160 tokens.  The =earn= category
contains 2877 training samples and 1087 test samples.  Over 83% sentences in
=earn= category have less then 160 tokens.  In order to balance the two
categories, for =earn=, we use 1650 samples out of 2877 for training, and 719
for testing.  The maximum sentence length of this binary classification dataset
is set to 160.

**** Reuters-5

It contains five categories, i.e., =crude=, =grain=, =interest=, =money-fx= and
=trade=.  Similar to Reuters-2, we balance the five categories by using 347
examples (the size of =interest= categories) for each category during training,
and 117 each for testing.  The maximum sentence length is set to 350.

** Embedding
:PROPERTIES:
:CUSTOM_ID: sec:textadv:embedding
:END:

Our framework relies heavily on the /size/ and /quality/ of the embedding space.
More semantic alternatives would be helpful to improve the quality of generated
adversarial texts.  As a result, we use the GloVe cite:pennington2014-glove
pre-trained embedding in our experiment.  Specifically, we use the largest GloVe
embedding, =glove.840B.300d=, which embeds 840 billion tokens (approximately 2.2
million cased vocabularies) into a vector space of 300 dimensions.  The value
range of the word vectors are roughly \((-5.161, 5.0408)\).

** Model
:PROPERTIES:
:CUSTOM_ID: subsec:textadv:model-gradient-based
:END:

In this work, we focus on feedforward architectures.  Specifically, we use CNN
model for the classification tasks.  The model structure is summarized in
Figure ref:fig:model-imdb.

#+ATTR_LaTeX: :width .6\textwidth :placement [!ht]
#+CAPTION: CNN model for text classification.
#+NAME: fig:model-imdb
[[file:img/model-imdb.pdf]]

Where \(B\) denotes batch size, \(L\) the maximum sentence length, \(D\) the
word vector space dimension.  In our experiment, we have \(B=128\), and
\(D=300\) since we are using the pre-trained embedding =glove.840B.300d=.

Note that for models trained for binary classification tasks, DeepFool assumes
the output in the range \([-1, 1]\), instead of \([0, 1]\).  Thus we have two
slightly different models for each of the binary classification task (IMDB and
Reuters-2), one with =sigmoid= output, and the other with =tanh=.  The model
with =tahn= output is trained with Adam cite:kingma2014-adam by minimizing the
mean squared error (MSE), while all the other models are trained with Adam by
minimizing the cross-entropy loss.  Despite the small difference in
architecture, =sigmoid=- and =tanh=-models on the same task have almost
identical accuracy.  As a result, in Table ref:tab:datasets, we report only one
result for IMDB and Reuters-2.

All our models have \(N=256\) and \(M=512\), except for the one with =tanh=
output on the IMDB classification task, in which we have \(N=128\) and
\(M=256\).  The reason that we change to a smaller model is that the larger one
always gets stuck during the training.  We are not yet clear what causes this
problem and why a smaller model helps.

** Preliminary Results
:PROPERTIES:
:CUSTOM_ID: sec:textadv:results
:END:

#+BEGIN_EXPORT latex
\begin{table*}[ht]
 \caption{\label{tab:acc} Model accuracy under different parameter settings.}
\centering
\small
\begin{tabular}{rl*{5}{c}}
  \toprule
  Method
  & Dataset
  &
  & \multicolumn{4}{c}{\(acc_1/acc_2\)} \\
  \midrule

  \multirow{5}{*}{FGSM}
  &
  & \(\epsilon\) & 0.40 & 0.35 & 0.30 & 0.25 \\
  \cmidrule(r){3-7}
  & IMDB      & & 0.1213 / 0.1334 & 0.1213 / 0.1990 & 0.1213 / 0.4074 & 0.1213 / 0.6770 \\
  & Reuters-2 & & 0.0146 / 0.6495 & 0.0146 / 0.7928 & 0.0146 / 0.9110 & 0.0146 / 0.9680 \\
  & Reuters-5 & & 0.1128 / 0.5880 & 0.1128 / 0.7162 & 0.1128 / 0.7949 & 0.1128 / 0.8462 \\
  \cmidrule(lr){1-7}

  \multirow{5}{*}{FGVM}
  &
  & \(\epsilon\) & 15 & 30 & 50 & 100 \\
  \cmidrule(r){3-7}
  & IMDB      & & 0.6888 / 0.8538 & 0.6549 / 0.8354 & 0.6277 / 0.8207 & 0.5925 / 0.7964 \\
  & Reuters-2 & &  0.7747 / 0.7990 & 0.7337 / 0.7538 & 0.6975 / 0.7156 & 0.6349 / 0.6523 \\
  & Reuters-5 & &  0.5915 / 0.7983 & 0.5368 / 0.6872 & 0.4786 / 0.6085 & 0.4000 / 0.5111\\
  \cmidrule(lr){1-7}

  \multirow{5}{*}{DeepFool}
  &
  & \(\epsilon\) & 20 & 30 & 40 & 50 \\
  \cmidrule(r){3-7}
  & IMDB      & & 0.5569 / 0.8298 & 0.5508 / 0.7225 & 0.5472 / 0.6678 & 0.5453 / 0.6416 \\
  & Reuters-2 & & 0.4416 / 0.6766 & 0.4416 / 0.5236 & 0.4416 / 0.4910 & 0.4416 / 0.4715 \\
  & Reuters-5 & & 0.1163 / 0.4034 & 0.1162 / 0.2222 & 0.1162 / 0.1641 & 0.1162 / 0.1402 \\
  \bottomrule
\end{tabular}
\end{table*}
#+END_EXPORT

The model accuracy on adversarial embeddings before and after the nearest
neighbor search under different parameter settings are summarized in
Table ref:tab:acc.  \(\epsilon\) is the noise scaling factor.  We report two
accuracy measurements per parameter setting in the format \(acc_1/acc_2\), where
\(acc_1\) is the model accuracy on adversarial embeddings before nearest
neighbor search, \(acc_2\) the accuracy on adversarial embeddings that are
reconstructed by nearest neighbor search.  In other words, \(acc_2\) is the
model accuracy on generated adversarial texts.

In the adversarial text examples, to aid reading, we omit the parts that are not
changed, denoted by \textbf{[\(\boldsymbol\ldots\)]} in the texts.  The
"(\textsc{IMDB})" at the end of each clean text piece denotes the dataset that
this piece of text belongs to.  In addition to Word Mover's Distance (WMD), we
also report the change rate, \(\frac{n}{L}\), where \(n\) is the number of
changed words, \(L\) the sentence length.  The corresponding changed words are
\colorbox{red!10}{highlighted} in the figures.

*** Fast Gradient Method
:PROPERTIES:
:CUSTOM_ID: subsec:result-fgm
:END:

#+ATTR_LaTeX: :float multicolumn :width \textwidth
#+CAPTION: Adversarial texts generated via FGVM.
#+NAME: fig:textdemo-fgvm
[[file:img/fgvm-eps50.pdf]]

We first evaluate two versions of FGM, i.e., FGSM and FGVM.  Their example
results are shown in Figure ref:fig:textdemo-fgsm and
Figure ref:fig:textdemo-fgvm, respectively.  For FGVM, it was proposed in
cite:miyato2015-distributional to use \(\frac{\nabla L}{\|\nabla L\|_2}\) to
FGVM usually needs much larger noise scaling factor since most gradients are
close to zero.

#+ATTR_LaTeX: :float multicolumn :width \textwidth
#+CAPTION: Adversarial texts generated via FGSM.
#+NAME: fig:textdemo-fgsm
[[file:img/fgsm-eps35.pdf]]

*** DeepFool
:PROPERTIES:
:CUSTOM_ID: subsec:result-deepfool
:END:

# should be in subsec:result-deepfool, placed here for typesetting
#+ATTR_LaTeX: :float multicolumn :width \textwidth
#+CAPTION: Adversarial texts generated via DeepFool.
#+NAME: fig:textdemo-deepfool
[[file:img/deepfool-eps40.pdf]]

Adversarial examples are shown in Figure ref:fig:textdemo-deepfool.  We
experiment with different overshoot values (also denoted as \epsilon in the
table).  Usually, for images, we tend to use very small overshoot values, e.g.,
1.02, which creates just enough noise to cross the decision boundary.  However,
in our framework, the reconstructing process is a very strong denoising process,
where much of the subtle noise will be smoothed.  To compensate for this, we
experiment with very large overshoot values.  In practice, this works very well.
As we can see, labels are altered by replacing just one word in many cases.

** COMMENT Discussion
:PROPERTIES:
:CUSTOM_ID: subsec:discussion
:END:

In contrary to the experiment in cite:liang2017-deep, our framework generates
much better adversarial texts with gradient methods.  One main reason is that
the embedding space preserves semantic relations among tokens.

Based on the generated text samples, DeepFool generates the adversarial texts
with the highest quality.  Our experiment confirms that the DeepFool's strategy
to search for the optimal direction is still effective in text models.  On the
other hand, the strong denoising process will help to smooth unimportant noise.
FGVM is slightly better than FGSM, which is quite similar to what we saw in
Figure ref:fig:mnistdemo.  By using \(\sign\nabla L\), FGSM applies the same
amount of noise to every feature it finds to be important, which ignores the
fact that some features are more important than others.  Since FGVM does not
follow the optimal direction as DeepFool does, it usually needs larger
perturbation.  In other words, compared to DeepFool, FGVM may change more words
in practice.

* Next Step
:PROPERTIES:
:CUSTOM_ID: chp:textadv:next-step
:END:

1. Test our framework against seq2seq models, possibly in the domain of machine
   translation.  The hallucination in neural machine translation (NMT) is a
   related question.  When fed with some meaningless and repeated tokens, NMT
   models may output some legit translations.  As far as I know, searching for
   such triggering tokens are mainly by trial-and-error.  It is possible that we
   could generate such triggering tokens in our framework.
2. Incorporate other classes of image adversarial methods into our framework.
3. Provide more strong arguments for our choice of WMD as the evaluation metric.

* COMMENT Conclusion
:PROPERTIES:
:CUSTOM_ID: chp:textadv:conclusion
:END:

In this work, we proposed a framework to adapt image attacking methods to
generate high-quality adversarial texts in an end-to-end fashion, without
relying on any manually selected features.  In this framework, instead of
constructing adversarials directly in the raw text space, we first search for
adversarial embeddings in the embedding space, and then reconstruct the
adversarial texts via nearest neighbor search.  We demonstrate the effectiveness
of our method on three texts benchmark problems.  In all experiments, our
framework can successfully generate adversarial samples with only a few words
changed.  In addition, we also empirically demonstrate Word Mover's Distance
(WMD) as a valid quality measurement for adversarial texts.  In the future, we
plan to extend our work in the following directions.
1. WMD is demonstrated to be a viable quality metric for the generated
   adversarial texts.  We can employ the optimization and model attacking
   methods by minimizing the WMD.
2. We use a general embedding space in our experiments.  A smaller embedding
   that is trained on the specific task may help to speed up the computation
   needed to reconstruct the texts.

* Footnotes

[fn:1] https://github.com/gongzhitaao/adversarial-text

[fn:2] https://github.com/spotify/annoy

[fn:3] https://keras.io/

[fn:4] http://www.daviddlewis.com/resources/testcollections/reuters21578/
