# Part 1 Generate Adversarial Texts

* Motivation
:PROPERTIES:
:CUSTOM_ID: chp:motivation
:END:

We have seen that the investigation of adversarial samples for image models has
provided us new perspectives to understand the mechanism of deep neural
networks, e.g., linear hypothesis cite:goodfellow2014-explaining, rethinking
smoothness assumptions cite:szegedy2013-intriguing.  In addition, many
algorithms have been proposed to enhance the robustness of the deep models,
e.g., adversarial training cite:goodfellow2014-explaining.  However, most of the
previous work focused on images.  Only a few attempts have been made in text
domain.  We think it is worthwhile to investigate adversarial samples for text
models as well.  In this work, we propose a simple and effective framework to
adapt the adversarial methods for images to text domain.  Specifically, we focus
on model gradient-based method since they are very fast in practice.  There is a
major problem we need to resolve before we can plugin the gradient-based
methods: /the input space is discrete/.  As a result, it is not possible to
accumulate small noise computed with gradient methods directly in the input
space.  They work well in image domain since image models usually take input in
a continuous domain \([0, 1]\).

In this work, we propose a general framework in which we generate adversarial
texts via slightly modified gradient-based methods.  We first search for
adversarials in the text embedding space cite:mikolov2013-efficient via
gradient-based methods, and then reconstruct the adversarial texts.  For
word-level models, (approximate) nearest search is used to align noisy
embeddings to legit ones.  For character-level models, we directly reverse the
noisy embeddings by cosine distance.

This paper is organized as follows.  We briefly review recent work on generating
adversarial images and texts in Section ref:sec:related-work.  Our adversarial
text framework is detailed in Section ref:sec:our-method.  We evaluate our
method on various text benchmarks and report the results in
Section ref:sec:experiment.  We conclude this paper and provide directions for
future work in Section ref:sec:conclusion.

* Related Work
:PROPERTIES:
:CUSTOM_ID: chp:related-work
:END:

The existence of adversarial samples was first discussed
in cite:szegedy2013-intriguing.  There has been an abundance of work on
attacking methods to generate adversarial images.  These adversarial images
raise security concerns about the wide application of deep neural
networks cite:kurakin2016-adversarial.  As a result, many work have investigated
defense against these adversarial samples.  However, so far as we see in
literature, the attacking is much easier and cheaper than defense.

For notation, \(x\) denotes the input, \(y\) the prediction, \(f\) the target
model such that \(y = f(x)\), \(L\) the loss function, \(x^*\) the adversarial
sample.  \(\|\cdot\|_p\) denotes the \(p\)-norm.  We slightly abuse the notation
here, \(L_x\) denotes the loss with \(x\) as the input.

** Generate Adversarial Images
:PROPERTIES:
:CUSTOM_ID: subsec:adversarial-image
:END:

The methods mainly fall into three categories, /model gradient-based/,
/optimization attack/ and /model attack/.  Generally speaking, gradient attack
is faster than the others.  However, the other two require much less knowledge
about the model, thus more practical.  In addition, optimization attacks are
usually more effective and generate more subtle noise.

*** Model Gradient-based
:PROPERTIES:
:CUSTOM_ID: subsec:gradient-attack
:END:

This class of attacking methods rely on target model gradients, thus requiring
full knowledge of the target model.  Fast gradient method
(FGM) cite:goodfellow2014-explaining and its variants, iterative FGM and
targeted FGM cite:kurakin2016-adversarial, add to the whole image the noise that
is proportional to either \(\nabla L_x\) (FGVM) or \(\sign(\nabla L_x)\) (FGSM).
Jacobian-based saliency map approach (JSMA) cite:papernot2015-limitations, on
the contrary, perturbs one pixel at a time.  It chooses the pixel with the
highest saliency score, which is calculated as \(-\nabla y_t\cdot\nabla y_o\)
subject to \(\nabla y_t > 0\), where \(y_t\) is the probability for the target
class, and \(y_o\) is the sum of probabilities of all other classes.
Intuitively, JSMA tries to increase the probability of the target class while
decreasing others.  DeepFool cite:moosavi-dezfooli2015-deepfool iteratively
finds the optimal direction in which we need to /travel/ the minimum distance to
cross the decision boundary of the target model.  Although in non-linear case,
the optimality is not guaranteed, in practice, however, DeepFool usually finds
very subtle noise compared to other gradient methods.

*** Optimization Attack
:PROPERTIES:
:CUSTOM_ID: subsec:optimization-attack
:END:

This class of attacks is usually black-box attacks, using the target model only
as an oracle.  Generally speaking, this class minimizes \(\|x^* - x\|_p\)
subject to \(f(x^*)\neq f(x)\) and \(x^*\) is in the input domain.  Following
this formulation, this is a box-constrained optimization problem, L-BFGS is used
in cite:szegedy2013-intriguing to solve it.  In order to utilize more
optimization methods, CW cite:carlini2016-towards proposes a refinement of the
above formulation by minimizing \(\|s(x^*) - x\|_p - L_{s(x^*)}\), where \(s\)
is a squashing function that keeps \(x^*\) within the input domain, e.g.,
=sigmoid= for images in the domain \([0, 1]\).  Many advance attacking
algorithms have been proposed based on the optimization formulation.
cite:moosavi-dezfooli2016-universal shows that, instead of applying different
noise to each image, it is possible to apply the same noise, i.e., a universal
perturbation, to different images, such that the resulting images still trick
the target model in most cases.  The one-pixel attack is also shown to be
possible cite:su2017-one.

*** Model Attack
:PROPERTIES:
:CUSTOM_ID: subsec:model-attack
:END:

Similar to the optimization attack, this class also formulates the adversarial
attack as an optimization problem.  The difference is that, instead of
performing the optimization directly, this class trains a separate model to map
the input to noise or adversarial samples.  Adversarial transformation network
(ATN) cite:baluja2017-adversarial trains a separate model \(g\) that minimizes
\(\beta\|x^*-x\|_p + \|f(x^*)-f(x)\|_{p^\prime}\), where \(g(x) = x^*\).
cite:zhao2017-generating proposes to first create a mapping between the input
space and a random noise space, and then search in the noise space for potential
adversarials which are mapped back to the input space.  To create the mapping
between input and noise space, the authors propose an autoencoder structure
which consists of
#+BEGIN_EXPORT latex
\begin{enumerate*}
 \item an encoder \(G\), a generator network that maps the random noise \(z\) to
 the input \(x\), \(G(z) = x\), and
 \item a decoder \(I\) (referred to as \textsl{inverter}), another generator
 network that maps the input to the random noise, \(I(x) = z\).
\end{enumerate*}
#+END_EXPORT
Generative Adversarial Network (GAN) cite:goodfellow2014-generative is used for
both generator networks.  The whole network is trained end-to-end by minimizing
the loss \(\mathbb{E}_x\|G(I(z)) - x\|_p + \lambda\mathbb{E}_z\|I(G(x)) -
z\|_p\).

** Adversarial Text Method
:PROPERTIES:
:CUSTOM_ID: subsec:adversarial-text
:END:

# Should be in experiment section, placed here for typesetting.
#+BEGIN_EXPORT latex
\begin{table*}[ht]
 \caption{\label{tab:acc} Model accuracy under different parameter settings.
   \(\epsilon\) is the noise scaling factor.  We report two accuracy
   measurements per parameter setting in the format \(acc_1/acc_2\), where
   \(acc_1\) is the model accuracy on adversarial embeddings before nearest
   neighbor search, \(acc_2\) the accuracy on adversarial embeddings that are
   reconstructed by nearest neighbor search.  In other words, \(acc_2\) is the
   model accuracy on generated adversarial texts.}
\centering
\small
\begin{tabular}{rl*{5}{c}}
  \toprule
  Method
  & Dataset
  &
  & \multicolumn{4}{c}{\(acc_1/acc_2\)} \\
  \midrule

  \multirow{5}{*}{FGSM}
  &
  & \(\epsilon\) & 0.40 & 0.35 & 0.30 & 0.25 \\
  \cmidrule(r){3-7}
  & IMDB      & & 0.1213 / 0.1334 & 0.1213 / 0.1990 & 0.1213 / 0.4074 & 0.1213 / 0.6770 \\
  & Reuters-2 & & 0.0146 / 0.6495 & 0.0146 / 0.7928 & 0.0146 / 0.9110 & 0.0146 / 0.9680 \\
  & Reuters-5 & & 0.1128 / 0.5880 & 0.1128 / 0.7162 & 0.1128 / 0.7949 & 0.1128 / 0.8462 \\
  \cmidrule(lr){1-7}

  \multirow{5}{*}{FGVM}
  &
  & \(\epsilon\) & 15 & 30 & 50 & 100 \\
  \cmidrule(r){3-7}
  & IMDB      & & 0.6888 / 0.8538 & 0.6549 / 0.8354 & 0.6277 / 0.8207 & 0.5925 / 0.7964 \\
  & Reuters-2 & &  0.7747 / 0.7990 & 0.7337 / 0.7538 & 0.6975 / 0.7156 & 0.6349 / 0.6523 \\
  & Reuters-5 & &  0.5915 / 0.7983 & 0.5368 / 0.6872 & 0.4786 / 0.6085 & 0.4000 / 0.5111\\
  \cmidrule(lr){1-7}

  \multirow{5}{*}{DeepFool}
  &
  & \(\epsilon\) & 20 & 30 & 40 & 50 \\
  \cmidrule(r){3-7}
  & IMDB      & & 0.5569 / 0.8298 & 0.5508 / 0.7225 & 0.5472 / 0.6678 & 0.5453 / 0.6416 \\
  & Reuters-2 & & 0.4416 / 0.6766 & 0.4416 / 0.5236 & 0.4416 / 0.4910 & 0.4416 / 0.4715 \\
  & Reuters-5 & & 0.1163 / 0.4034 & 0.1162 / 0.2222 & 0.1162 / 0.1641 & 0.1162 / 0.1402 \\
  \bottomrule
\end{tabular}
\end{table*}
#+END_EXPORT

Almost all the work in the previous section focus on image models.  As we have
discussed, the main problem to generate adversarial texts are the discrete input
space and the lack of quality measurement.  The aforementioned model
attack cite:zhao2017-generating is a viable workaround for the first problem
since the noise space is smooth.  However, the disadvantage with their method is
that they do not have an explicit control of the quality of the generated
adversarial samples.  As we have seen in cite:zhao2017-generating, the generated
adversarial images on complex dataset usually have large visual changes.

Most work cite:liang2017-deep,samanta2017-towards,jia2017-adversarial on
attacking text models follow a similar strategy,
#+BEGIN_EXPORT latex
\begin{enumerate*}
 \item first identify the features (characters, words, sentences, etc.) that
 have the most influence on the prediction, and then
 \item follow different strategies to perturb these features according to
 \textsl{manually} constructed perturbation candidates.
\end{enumerate*}
#+END_EXPORT
This strategy is similar to JSMA, in which the intensity of the pixel with the
highest saliency score is increased or decreased.  The Jacobian value \(\nabla
f\) or the loss gradient \(\nabla L\) are usually employed to construct a
measurement for the feature importance, e.g., \(\nabla L\) is used in
cite:liang2017-deep to select important characters and phrases to perturb.  The
perturbation candidates usually include typos, synonyms, antonyms, frequent
words in each category, and other task-dependent features.  For example, typos,
synonyms, and important adverbs and adjectives are used as candidates for
insertion and replacement in cite:samanta2017-towards.  The strategies to apply
the perturbation generally include /insertion/, /deletion/, and /replacement/.

A slightly different strategy is used in cite:jia2017-adversarial.  The authors
add to the samples /manually/ constructed legit distracting sentences, which
introduce fake information that does not contradict with the samples.  This
strategy, despite being effective, is not scalable.

In cite:liang2017-deep, the authors attempt applying FGM directly on
character-level CNN cite:zhang2015-character.  Although the labels of the text
pieces are altered, the texts are changed to basically random stream of
characters.

cite:anonymous2018-adversarial employs a brutal-force way to find perturbation.
They iteratively replace each word with its nearest neighbor in the embedding
space until success or a threshold is reached.  The computation is very
expensive.  A black-box attack based on GAN is proposed cite:wong2017-dancin.  A
highly related work is also report in cite:ebrahimi2017-hotflip where the
authors conduct character-level and word-level attack based on gradients.  The
difference is that we use nearest neighbor search to reconstruct the adversarial
sentences, while they search for adversarial candidates directly based on
certain constraints.  Thus the word-level attack was not very successfully in
cite:ebrahimi2017-hotflip.

* Our Method
:PROPERTIES:
:CUSTOM_ID: sec:our-method
:END:

In this section, we propose a general framework that generates high-quality
adversarial texts by noise generated via gradient-based methods.

** Discrete Input Space
:PROPERTIES:
:CUSTOM_ID: subsec:discrete-input-space
:END:

The first problem we need to resolve is how we can accumulate small noise to
change the input.  The idea comes from the observation that the first layer for
most text models is the embedding layer.  Thus, instead of working on the raw
input texts, we first search for adversarials in the embedding space via
gradient-based methods, and then reconstruct the adversarial sentences.
Searching for adversarials in the embedding space is similar in principle to
searching for adversarial images.  However, the generated noisy embedding
vectors usually do not correspond to any tokens in the text space.  To construct
the adversarial texts, we align each embedding to its nearest one.  We can use
(approximate) nearest neighbor search if the vocabulary size is large, or direct
embedding reverse by cosine distance if the embedding matrix is relative small.
This reconstructing process can be seen as a strong /denoising/ process.  With
appropriate noise scale, we would expect most of the tokens/characters remain
unchanged, with only few replaced.  This framework builds upon the following
observations.

1. In the gradient-based methods, the input features (e.g., pixels, tokens,
   characters) that are relatively more important for the final predictions will
   receive more noise, while others relatively less noise.  The is actually the
   core property of the gradient-based methods.  For example, in
   Figure ref:fig:mnistdemo, usually only a subset of the pixels are perturbed.
2. The embedded word vectors preserve the subtle semantic relationships among
   words cite:mikolov2013-efficient,mikolov2013-distributed.  For example,
   =vec("clothing")= is closer to =vec("shirt")= as =vec("dish")= to
   =vec("bowl")=, while =vec("clothing")= is far away, in the sense of
   \(p\)-norm, from =vec("dish")= since they are not semantically
   related cite:mikolov2013-linguistic.  This property assures that it is more
   likely to replace the victim words with a semantically related one rather
   than a random one.

** Word Mover's Distance (WMD)
:PROPERTIES:
:CUSTOM_ID: subsec:wmd
:END:

The second problem we need to resolve is the choice of quality metric for
generated adversarial texts, so that we have a scalable way to measure the
effectiveness of our framework.  We employ the Word Mover's Distance
(WMD) cite:kusner2015-from as the metric.  WMD measures the dissimilarity
between two text documents as the minimum amount of distance that the embedded
words of one document need to /travel/ to reach the embedded words of another
document.  WMD can be considered as a special case of Earth Mover's Distance
(EMD) cite:rubner2000-earth.  Intuitively, it quantifies the semantic similarity
between two text bodies.  In this work, WMD is closely related to the ratio of
number of words changed to the sentence length.  However, we plan to extend our
framework with paraphrasing and insertion/deletion, where the sentence length
may change.  In that case, WMD is more flexible and accurate.

* Experiment
:PROPERTIES:
:CUSTOM_ID: sec:experiment
:END:

# should be in subsec:result-deepfool, placed here for typesetting
#+ATTR_LaTeX: :float multicolumn :width \textwidth
#+CAPTION: Adversarial texts generated via DeepFool.  Refer to Section ref:subsec:results for notations and discussions.
#+NAME: fig:textdemo-deepfool
[[file:img/deepfool-eps40.pdf]]

We evaluate our framework on three text classification problems.
Section ref:subsec:dataset details on the data preprocessing.  The adversarial
attacking algorithms which we use are (FGM) cite:goodfellow2014-explaining and
DeepFool cite:moosavi-dezfooli2015-deepfool.  We tried JSMA, however, due to the
mechanism of JSMA, it is not directly applicable in our framework.  We report in
Section ref:subsec:results the original model accuracy, accuracy on adversarial
embeddings, and accuracy on reconstructed adversarial texts in our experiment.
Only a few examples of generated adversarial texts are shown in this paper due
to the space constraint.  The complete sets of adversarial texts under different
parameter settings and the code to reproduce the experiment are available on our
website[fn:1].

Computation-wise, the bottleneck in our framework is the nearest neighbor
search.  Word vector spaces, such as GloVe cite:pennington2014-glove, usually
have millions or billions of tokens embedded in very high dimensions.  The
vanilla nearest neighbor search is almost impractical.  Instead, we employ the
an approximate nearest neighbor (ANN) technique in our experiment.  The ANN
implementation which we use in our experiment is Approximate Nearest Neighbors
Oh Yeah (=annoy=)[fn:2], which is well integrated into =gensim=
cite:rek2010-software package.

** Dataset
:PROPERTIES:
:CUSTOM_ID: subsec:dataset
:END:

We use three text datasets in our experiments.  The datasets are summarized in
Table ref:tab:datasets.  The last column shows our target model accuracy on
clean test data.

#+ATTR_LaTeX: :booktabs t :width \linewidth
#+CAPTION: Dataset Summary
#+NAME: tab:datasets
| Dataset   | Labels | Training | Testing | Max Length | Accuracy |
|-----------+--------+----------+---------+------------+----------|
| IMDB      |      2 |    25000 |   25000 |        300 |   0.8787 |
| Reuters-2 |      2 |     3300 |    1438 |        100 |   0.9854 |
| Reuters-5 |      5 |     1735 |     585 |        100 |   0.8701 |

*** IMDB Movie Reviews
:PROPERTIES:
:CUSTOM_ID: subsec:data-imdb
:END:

This is a dataset for binary sentiment classification cite:maas2011-learning.
It contains a set of 25,000 highly polar (positive or negative) movie reviews
for training, and 25,000 for testing.  No special preprocessing is used for this
dataset except that we truncate/pad all the sentences to a fixed maximum
length, 400.  This max length is chosen empirically.

*** Reuters
:PROPERTIES:
:CUSTOM_ID: subsec:data-reuters
:END:

This is a dataset of 11,228 newswires from Reuters, labeled over 90 topics.  We
load this dataset through the NLTK cite:bird2009-natural package.  The raw
Reuters dataset is highly unbalanced.  Some categories contain over a thousand
samples, while others may contain only a few.  The problem with such highly
unbalanced data is that the texts that belong to under-populated categories are
almost always get classified incorrectly.  Even though our model may still
achieve high accuracy with 90 labels, it would be meaningless to include these
under-populated categories in the experiment since we are mainly interested in
perturbation of those samples that are already being classified correctly.
Keras[fn:3] uses 46 categories out of 90.  However, the 46 categories are still
highly unbalanced.  In our experiment, we preprocess Reuters and extract two
datasets from it.

**** Reuters-2
:PROPERTIES:
:CUSTOM_ID: subsec:data-reuters-2
:END:

It contains two most populous categories, i.e., =acq= and =earn=.  The =acq=
category contains 1650 training samples and 719 test samples.  Over 71%
sentences in the =acq= category have less than 160 tokens.  The =earn= category
contains 2877 training samples and 1087 test samples.  Over 83% sentences in
=earn= category have less then 160 tokens.  In order to balance the two
categories, for =earn=, we use 1650 samples out of 2877 for training, and 719
for testing.  The maximum sentence length of this binary classification dataset
is set to 160.

**** Reuters-5
:PROPERTIES:
:CUSTOM_ID: subsec:data-reuters-5
:END:

It contains five categories, i.e., =crude=, =grain=, =interest=, =money-fx= and
=trade=.  Similar to Reuters-2, we balance the five categories by using 347
examples (the size of =interest= categories) for each category during training,
and 117 each for testing.  The maximum sentence length is set to 350.

# should be in subsec:result-fgm, placed here for typesetting
#+ATTR_LaTeX: :float multicolumn :width \textwidth
#+CAPTION: Adversarial texts generated via FGSM.  Refer to Section ref:subsec:results for notations and discussions.
#+NAME: fig:textdemo-fgsm
[[file:img/fgsm-eps35.pdf]]

** Embedding
:PROPERTIES:
:CUSTOM_ID: subsec:embedding
:END:

Our framework relies heavily on the /size/ and /quality/ of the embedding space.
More semantic alternatives would be helpful to improve the quality of generated
adversarial texts.  As a result, we use the GloVe cite:pennington2014-glove
pre-trained embedding in our experiment.  Specifically, we use the largest GloVe
embedding, =glove.840B.300d=, which embeds 840 billion tokens (approximately 2.2
million cased vocabularies) into a vector space of 300 dimensions.  The value
range of the word vectors are roughly \((-5.161, 5.0408)\).

** Model
:PROPERTIES:
:CUSTOM_ID: subsec:model
:END:

In this work, we focus on feedforward architectures.  Specifically, we use CNN
model for the classification tasks.  The model structure is summarized in
Figure ref:fig:model-imdb.

#+ATTR_LaTeX: :width \linewidth :placement [!ht]
#+CAPTION: CNN model for text classification.
#+NAME: fig:model-imdb
[[file:img/model-imdb.pdf]]

Where \(B\) denotes batch size, \(L\) the maximum sentence length, \(D\) the
word vector space dimension.  In our experiment, we have \(B=128\), and
\(D=300\) since we are using the pre-trained embedding =glove.840B.300d=.

Note that for models trained for binary classification tasks, DeepFool assumes
the output in the range \([-1, 1]\), instead of \([0, 1]\).  Thus we have two
slightly different models for each of the binary classification task (IMDB and
Reuters-2), one with =sigmoid= output, and the other with =tanh=.  The model
with =tahn= output is trained with Adam cite:kingma2014-adam by minimizing the
mean squared error (MSE), while all the other models are trained with Adam by
minimizing the cross-entropy loss.  Despite the small difference in
architecture, =sigmoid=- and =tanh=-models on the same task have almost
identical accuracy.  As a result, in Table ref:tab:datasets, we report only one
result for IMDB and Reuters-2.

All our models have \(N=256\) and \(M=512\), except for the one with =tanh=
output on the IMDB classification task, in which we have \(N=128\) and
\(M=256\).  The reason that we change to a smaller model is that the larger one
always gets stuck during the training.  We are not yet clear what causes this
problem and why a smaller model helps.

** Results
:PROPERTIES:
:CUSTOM_ID: subsec:results
:END:

The model accuracy on adversarial embeddings before and after the nearest
neighbor search under different parameter settings are summarized in
Table ref:tab:acc.

In the adversarial text examples, to aid reading, we omit the parts that are not
changed, denoted by \textbf{[\(\boldsymbol\ldots\)]} in the texts.  The
"(\textsc{IMDB})" at the end of each clean text piece denotes the dataset that
this piece of text belongs to.  In addition to Word Mover's Distance (WMD), we
also report the change rate, \(\frac{n}{L}\), where \(n\) is the number of
changed words, \(L\) the sentence length.  The corresponding changed words are
\colorbox{red!10}{highlighted} in the figures.

*** Fast Gradient Method
:PROPERTIES:
:CUSTOM_ID: subsec:result-fgm
:END:

#+ATTR_LaTeX: :float multicolumn :width \textwidth
#+CAPTION: Adversarial texts generated via FGVM.  Refer to Section ref:subsec:results for notations and discussions.
#+NAME: fig:textdemo-fgvm
[[file:img/fgvm-eps50.pdf]]

We first evaluate two versions of FGM, i.e., FGSM and FGVM.  Their example
results are shown in Figure ref:fig:textdemo-fgsm and
Figure ref:fig:textdemo-fgvm, respectively.  For FGVM, it was proposed in
cite:miyato2015-distributional to use \(\frac{\nabla L}{\|\nabla L\|_2}\) to
FGVM usually needs much larger noise scaling factor since most gradients are
close to zero.

*** DeepFool
:PROPERTIES:
:CUSTOM_ID: subsec:result-deepfool
:END:

Adversarial examples are shown in Figure ref:fig:textdemo-deepfool.  We
experiment with different overshoot values (also denoted as \epsilon in the
table).  Usually, for images, we tend to use very small overshoot values, e.g.,
1.02, which creates just enough noise to cross the decision boundary.  However,
in our framework, the reconstructing process is a very strong denoising process,
where much of the subtle noise will be smoothed.  To compensate for this, we
experiment with very large overshoot values.  In practice, this works very well.
As we can see, labels are altered by replacing just one word in many cases.

** Discussion
:PROPERTIES:
:CUSTOM_ID: subsec:discussion
:END:

In contrary to the experiment in cite:liang2017-deep, our framework generates
much better adversarial texts with gradient methods.  One main reason is that
the embedding space preserves semantic relations among tokens.

Based on the generated text samples, DeepFool generates the adversarial texts
with the highest quality.  Our experiment confirms that the DeepFool's strategy
to search for the optimal direction is still effective in text models.  On the
other hand, the strong denoising process will help to smooth unimportant noise.
FGVM is slightly better than FGSM, which is quite similar to what we saw in
Figure ref:fig:mnistdemo.  By using \(\sign\nabla L\), FGSM applies the same
amount of noise to every feature it finds to be important, which ignores the
fact that some features are more important than others.  Since FGVM does not
follow the optimal direction as DeepFool does, it usually needs larger
perturbation.  In other words, compared to DeepFool, FGVM may change more words
in practice.

* Conclusion
:PROPERTIES:
:CUSTOM_ID: sec:conclusion
:END:

In this work, we proposed a framework to adapt image attacking methods to
generate high-quality adversarial texts in an end-to-end fashion, without
relying on any manually selected features.  In this framework, instead of
constructing adversarials directly in the raw text space, we first search for
adversarial embeddings in the embedding space, and then reconstruct the
adversarial texts via nearest neighbor search.  We demonstrate the effectiveness
of our method on three texts benchmark problems.  In all experiments, our
framework can successfully generate adversarial samples with only a few words
changed.  In addition, we also empirically demonstrate Word Mover's Distance
(WMD) as a valid quality measurement for adversarial texts.  In the future, we
plan to extend our work in the following directions.
1. WMD is demonstrated to be a viable quality metric for the generated
   adversarial texts.  We can employ the optimization and model attacking
   methods by minimizing the WMD.
2. We use a general embedding space in our experiments.  A smaller embedding
   that is trained on the specific task may help to speed up the computation
   needed to reconstruct the texts.

* Footnotes

[fn:1] https://github.com/gongzhitaao/adversarial-text

[fn:2] https://github.com/spotify/annoy

[fn:3] https://keras.io/

[fn:4] http://www.daviddlewis.com/resources/testcollections/reuters21578/
