#+TITLE: On the Machine Illusion
#+SUBTITLE: Proposal of Study on Adversarial Samples
#+DATE: April 12, 2018
#+AUTHOR: Zhitao Gong
#+EMAIL: gong@auburn.edu
#+OPTIONS: H:2 ^:{} toc:nil
#+STARTUP: hideblocks showcontent

#+LATEX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [dvipsnames]

#+LATEX_HEADER: \usepackage{svg}
#+LATEX_HEADER: \usepackage{mathtools}
#+LATEX_HEADER: \usepackage{clrscode3e}
#+LATEX_HEADER: \usepackage{lmodern}
#+LATEX_HEADER: \usepackage{xcolor}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{physics}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage[backend=biber,style=alphabetic]{biblatex}
#+LATEX_HEADER: \usepackage[scaled=0.85]{newtxtt}

#+LATEX_HEADER: \institute{Auburn University}
#+LATEX_HEADER: \addbibresource{~/.local/data/bibliography/nn.bib}
#+LATEX_HEADER: \AtBeginSection[]{\begin{frame}<beamer>\frametitle{Outline}\tableofcontents[currentsection]\end{frame}}
#+LATEX_HEADER: \beamertemplatenavigationsymbolsempty
#+LATEX_HEADER: \setbeamertemplate{footline}[frame number]
#+LATEX_HEADER: \setbeamertemplate{background}{\tikz[overlay,remember picture]\node at(current page.north east)[anchor=north east]{\includegraphics[width=1cm]{img/au-15.png}};}
#+LATEX_HEADER: \setbeamersize{description width=0.5cm}

#+LATEX_HEADER: \defbeamertemplate*{bibliography item}{triangletext}{\insertbiblabel}
#+LATEX_HEADER: \renewcommand*{\bibfont}{\tiny}
#+LATEX_HEADER: \renewcommand*{\citesetup}{\scriptsize}
#+LATEX_HEADER: \makeatletter\def\mathcolor#1#{\@mathcolor{#1}}\def\@mathcolor#1#2#3{\protect\leavevmode\begingroup\color#1{#2}#3\endgroup}\makeatother

#+LATEX_HEADER: \DeclareMathOperator{\sign}{sign}
#+LATEX_HEADER: \DeclareMathOperator{\sigmoid}{sigmoid}
#+LATEX_HEADER: \DeclareMathOperator{\softmax}{softmax}
#+LATEX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LATEX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}

#+MACRO: empty @@latex:@@
#+MACRO: cs231n [[http://cs231n.stanford.edu][cs231n]]
#+MACRO: colah-blog [[http://colah.github.io/posts/2015-08-Understanding-LSTMs][colah's blog]]

* Introduction

** Neural Networks

It is a connectionist model.
1. Any state can be described as an \(N\)-dimensional vector of numeric
   activation values over neural units in a network.
2. Memory is created by modifying the strength of the connections between neural
   units.

#+ATTR_LaTeX: :width \textwidth
#+CAPTION: Biological neuron versus neuron model (credit: {{{cs231n}}})
[[file:img/neuron.png]]

** Architectures: Multi-Layer Perceptron (MLP)

MLP is one of the most simple feedfoward architectures.
1. Each neuron outputs to the neurons in the next layer.
2. Neurons in the same layer have no connections.

#+ATTR_LaTeX: :width .6\textwidth
#+CAPTION: Multi-layer perceptron (credit: {{{cs231n}}})
[[file:img/mlp.jpg]]

** Architectures: Convolutional Neural Network (CNN)

CNN is inspired by eye structure, widely used in computer vision.
1. Each neuron receives inputs from a pool of neurons in previous layer, just
   like the convolution operation.
2. Neurons in the same layer have no connections

#+CAPTION: LetNet-5 \cite{lecun1998-gradient}
[[file:img/cnn.png]]

** Architectures: Recurrent Neural Network (RNN)

Some neurons get part of input from its output.

*** {{{empty}}} :B_only:
:PROPERTIES:
:BEAMER_act: 1
:BEAMER_env: only
:END:

#+CAPTION: Dynamic unrolling of recurrent cells. (credit: {{{colah-blog}}})
[[file:img/rnn-unrolled.png]]

*** {{{empty}}} :B_only:
:PROPERTIES:
:BEAMER_act: 2
:BEAMER_env: only
:END:

#+CAPTION: The double-edged sword: long term dependencies between outputs and inputs. (credit: {{{colah-blog}}})
[[file:img/RNN-longtermdependencies.png]]

** Notations

For clarity, we use the following notations in this slide.
- \(f\) denotes the neural nets model, \(\theta\) the model's parameters,
  and sometimes \(f_\theta\) for brevity.
- \(x\) is the input, \(y\) the model's output, such that \(y = f(x)\) or \(y =
  f(x; \theta)\) to emphasize the parameters.
- \(z\) is the un-normalized logits, i.e., \(y = \sigmoid(z)\) or \(y =
  \softmax(z)\).
- \(L\) denotes the loss function, e.g., cross-entropy, mean-squared error.  For
  simplicity, we use \(L_x\) to denote the loss value when \(x\) is the input.
- \(x^*\) denotes the adversarial sample crafted based on \(x\).
- In a targeted method, \(y_t\) denotes the
  @@latex:\textsl{\textcolor{red}{t}}@@arget class value, \(y_o\) the
  @@latex:\textsl{\textcolor{red}{o}}@@ther class values.  For example, \(y =
  [0.2, 0.5, 0.3]\) and \(t = 0\), then \(y_t = 0.2\) and \(y_o\in\{0.5,
  0.3\}\).  Same for \(z\).

* Problem Overview

** Adversarial Samples
:PROPERTIES:
:BEAMER_opt: allowframebreaks
:END:

[[file:img/mnist-compare-all.png]]

1. Visually very close, noises are very subtle.
2. Trick machines into wrong predictions with high confidence.

#+LaTeX: \framebreak

#+CAPTION: Adversarial texts by our framework.
#+ATTR_LaTeX: :width \textwidth
[[file:img/demo-adv-text.pdf]]

The \colorbox{red!10}{highlighted} words are changed.  The \(n/L\) is the number
of words changed divided by the total number of words.

** Adversarial Patterns for Machines

#+CAPTION: Adversarial patterns for different neural nets \cite{moosavi-dezfooli2016-universal}.
#+NAME: fig:adv-machine
[[file:img/adv-machine.png]]

** Adversarial Patterns For Humans

#+CAPTION: The blue lines are parallel.  This illusion is possibly caused by the fringed edges \cite{kitaoka2004-contrast}.
#+NAME: fig:adv-human
[[file:img/adv-human.jpg]]

More examples: http://www.psy.ritsumei.ac.jp/~akitaoka.

** Why Study Adversarial Samples

This phenomenon is interesting both in practice and in theory.
1. It undermines the models' reliability.
2. Hard to ignore due to it being transferable and universal.
3. It provides new insights into neural networks:
   - Local generalization does not seem to hold.
   - Data distribution: they appear in dense regions.
   - Trade-off between robustness and generalization.
   - \(\cdots\)

* Generate Adversarial Images

** Model Gradient-based Methods

A class of /white-box/ methods, i.e., they need to access the model's parameters
in order to generate the adversarial samples.

- Fast gradient method (FGSM) cite:goodfellow2014-explaining adds to the input
  the noise that is proportional to either \(\nabla L_x\) or \(\sign(\nabla
  L_x)\).
- DeepFool cite:moosavi-dezfooli2015-deepfool iteratively finds the optimal
  direction in which we need to /travel/ the minimum distance to cross the
  decision boundary of the target model.
- Jacobian-based saliency map approach (JSMA) cite:papernot2015-limitations
  perturbs one pixel at a time, the one with the highest score which is
  calculated as \(-\nabla y_t\cdot\sum\nabla y_o\) subject to \(\nabla y_t > 0\).

** Noise Minimization Methods

*** {{{empty}}} :B_only:
:PROPERTIES:
:BEAMER_act: 1
:BEAMER_env: only
:END:

This class directly formulates the problem as an optimization problem.

#+BEGIN_EXPORT latex
\begin{equation*} \label{eq:noise-minimization}
 \begin{aligned}
  \text{minimize } & \|x^* - x\|_p \\
  \text{ s.t. } & f(x^*)\neq f(x) \text{ and } x^*\in\mathcal D
 \end{aligned}
\end{equation*}
#+END_EXPORT

\(\mathcal D\) is the input domain, e.g., \([0, 1]\) for images.
\(\|\cdot\|_p\) is the \(p\)-norm.  This is difficult to solve in itself because
1. it is a box-constrained optimization, and
2. the constraint \(f(x^*)\neq f(x)\) is not smooth.

*** {{{empty}}} :B_only:
:PROPERTIES:
:BEAMER_act: 2
:BEAMER_env: only
:END:

Instead of solving the optimization directly, cite:carlini2016-towards removes
the constraints by a variable substitution trick.

#+BEGIN_EXPORT latex
\begin{equation*}
 \begin{aligned}
  \text{minimize } & \|x^* - x\|^2_2 + c\cdot f(x) \text{ where }\\
  x^* & = \sigmoid(w) \\
  f(x) & = \max\left(\max\left(z_o - z_t\right), -\kappa\right)
 \end{aligned}
\end{equation*}
#+END_EXPORT

In this case, \(w\) is unconstrained.

** Generative Model-based Methods

This class use another model to generate the adversarials or noise.
- Adversarial transformation network cite:baluja2017-adversarial
- GAN-based cite:xiao2018-generating,zhao2017-generating

** Summary

Intuitions behind the adversarial methods
1. Move the data point across the decision boundary.
2. Perturb the data point so that the loss increases, e.g.,
   - reverse direction of gradients on the loss surface, or
   - direction where the probability for the correct(wrong) class
     decreases(increases).

#+ATTR_LaTeX: :width .8\textwidth
#+CAPTION: Data space hypothesis \cite{nguyen2014-deep}
[[file:img/image-space.png]]

* Generate Adversarial Texts

** Text Embedding Layer

#+CAPTION: Architecture for sentence classification with CNN \cite{kim2014-convolutional}
#+ATTR_LaTeX: :width \textwidth
[[file:img/textcnn.png]]

** Text Embedding Example

"wait for the video" \(\xrightarrow{\text{tokenize}}\) ["wait", "for", "the",
"video"] \(\xrightarrow{\text{indexer}}\) [2, 20, 34, 8]
\(\xrightarrow{\text{embedding}}\) \(\mathbb{R}^{4\times D}\), where \(D\) is
the embedding size.

- Each sentence with be converted to \(\mathbb{R}^{L\times D}\) before being fed
  into the convolution layer, where \(L\) is the sentence length.
- We usually truncate/pad sentences to the same length so that we could do
  /batch training/.
- Embedding may also be on the character-level.

** Problem Overview

Difficulties we face:
1. The text space is discrete.  Moving the data points in small steps following
   a certain direction does not work, directly.
2. Text quality is hard to measure.  /Much to learn, you still have/ (the
   Yoda-style) v.s. /You still have much to learn/ (the mundane-style)

General directions:
1. Three basic operations are available, /replacement/, /insertion/, and
   /deletion/.
2. They may work at character, word or sentence level.

** Methods in Text Space

This class of methods need to solve two problems:
1. what to change, e.g., random cite:anonymous2018-adversarial, \(\nabla
   L\) cite:liang2017-deep, manually picking cite:samanta2017-towards.
2. change to what, e.g., random, synonyms cite:samanta2017-towards or nearest
   neighbors in embedding space cite:anonymous2018-adversarial, or forged
   facts cite:jia2017-adversarial,liang2017-deep.

** Methods in Transformed Space

Autoencoder cite:hinton2006-reducing is used to map between texts and a
continuous space cite:zhao2017-generating.  The embedded space is smooth.

#+ATTR_LaTeX: :width .7\textwidth
[[file:img/Autoencoder_structure.png]]

** Adversarial Text Framework

We propose another method in the embedding space.

#+BEGIN_EXPORT latex
{\small
  \begin{codebox}
   \Procname{$\proc{Generate-Adversarial-Texts}(f, x)$}
   \li \For $i \gets 1$ \To $\attrib{x}{length}$
   \li \Do $z_i \gets \proc{Embedding}(x_i)$\End
   \li $z^\prime \gets \proc{Adv}(f, z)$
   \li \For $i \gets 1$ \To $\attrib{z^\prime}{length}$
   \li \Do $x^\prime_i \gets \proc{Nearest-Embedding}(z^\prime_i)$
   \li $s_i \gets \proc{Reverse-Embedding}(x^\prime_i) $\End
   \li \Return $s$
  \end{codebox}
}
#+END_EXPORT

Assumptions:
1. The text embedding space preserve the semantic relations.
2. Important features get more noise.

Result: https://github.com/gongzhitaao/adversarial-text

** Next Step

1. Find appropriate quality measurement for texts, e.g., language model scores,
   Word Mover's Distance (WMD).
2. Find a way to control the quality of generated adversarial texts.
3. Test the transferability of adversarial texts.

* Defend Adversarial Samples

** Enhance Model

Basic ideas: incorporate adversarial samples during training process, and/or
improve architectures.

Given a training set \(\mathcal{X}\), instead of minimizing

\[\theta^* = \argmin_\theta\mathbb{E}_{x\in\mathcal{X}}L(x; f_\theta)\]

we expand each data point a bit

\[\theta^* =
\argmin_\theta\mathbb{E}_{x\in\mathcal{X}}\left[\mathcolor{red}{\max_{\delta \in
[-\epsilon,\epsilon]^N}} L(x \mathcolor{red}{+ \delta}; f_\theta)\right]\]

cite:goodfellow2014-explaining,madry2017-towards solve the inner maximization
problem by mixing dynamically generated adversarial samples into training data.

** Preprocess Inputs

Without re-training the models, this direction focuses on the inputs.
1. Transform inputs to (hopefully) recover the bad samples.
2. Filter out bad samples by image statistics.

** Binary Classifier as A Defense

Taking advantage of the observation that the adversarial noise follows a
specific direction cite:goodfellow2014-explaining.  We build a simple classifier
to separate adversarial from clean data cite:gong2017-adversarial.

#+BEGIN_EXPORT latex
\begin{table}[htbp]
  \caption{\label{tbl:eps-sensitivity-cifar10}
    FGSM \(\epsilon\) sensitivity on CIFAR10}
  \centering\small
  \begin{tabular}{lcll}
    \toprule
    & \phantom{a} & \multicolumn{2}{c}{\(\eval{f_2}_{\epsilon=0.03}\)} \\
    \cmidrule{3-4}
    \(\epsilon\) && \(X_{test}\) & \(X^{adv(f_1)}_{test}\)\\
    \midrule
    0.3 && 0.9996 & 1.0000\\
    0.1 && 0.9996 & 1.0000\\
    0.03 && 0.9996 & 0.9997\\
    0.01 && 0.9996 & \textbf{0.0030}\\
    \bottomrule
  \end{tabular}
\end{table}
#+END_EXPORT

*Limitation*: different hyperparameters, different adversarial algorithms may
elude the binary classifier or adversarial training.

Results: https://github.com/gongzhitaao/adversarial-classifier

** Next Step

1. Closely investigate the limitation of binary classifier approach.
2. Detect and/or recover adversarial texts

** {{{empty}}}

#+BEGIN_CENTER
GENERATION IS CHEAP,\\
DEFENSE IS DIFFICULT.
#+END_CENTER

* Summary

** Adversarial Samples

1. All classification models are affected.
2. Seems to exist in dense regions.
3. Distribute along only certain directions.
4. Transfer to different models or techniques.
5. \(\cdots\)

#+BEGIN_CENTER
ALL EMPIRICAL AND HYPOTHESIS SO FAR
#+END_CENTER

* Bibliography

** {{{empty}}}
:PROPERTIES:
:BEAMER_opt: allowframebreaks
:END:

#+LaTeX: \printbibliography
