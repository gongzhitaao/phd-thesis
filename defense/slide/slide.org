#+TITLE: On the Machine Illusion
#+SUBTITLE: Empirical Study on Adversarial Samples
#+DATE: March 26, 2019
#+AUTHOR: Zhitao Gong
#+EMAIL: gong@auburn.edu
#+OPTIONS: H:2 ^:{} toc:nil
#+STARTUP: hideblocks showcontent

#+LATEX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [dvipsnames]

#+LATEX_HEADER: \usepackage{svg}
#+LATEX_HEADER: \usepackage{mathtools}
#+LATEX_HEADER: \usepackage{clrscode3e}
#+LATEX_HEADER: \usepackage{lmodern}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{physics}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage[backend=biber,style=alphabetic]{biblatex}
#+LATEX_HEADER: \usepackage[scaled=0.85]{newtxtt}
#+LATEX_HEADER: \usepackage{multirow}

#+LATEX_HEADER: \usetikzlibrary{calc}

#+LATEX_HEADER: \addbibresource{refdb.bib}
#+LATEX_HEADER: \addbibresource{local.bib}
#+LATEX_HEADER: \graphicspath{{img/}}

#+LATEX_HEADER: \institute{Auburn University}
#+LATEX_HEADER: \AtBeginSection[]{\begin{frame}<beamer>\frametitle{Outline}\tableofcontents[currentsection]\end{frame}}
#+LATEX_HEADER: \beamertemplatenavigationsymbolsempty
#+LATEX_HEADER: \setbeamertemplate{footline}[frame number]
#+LATEX_HEADER: \setbeamertemplate{background}{\tikz[overlay,remember picture]\node at (current page.north east)[anchor=north east]{\includegraphics[width=1cm]{au-15.png}};}
#+LATEX_HEADER: \setbeamersize{description width=0.5cm}

#+LATEX_HEADER: \defbeamertemplate*{bibliography item}{triangletext}{\insertbiblabel}
#+LATEX_HEADER: \renewcommand*{\bibfont}{\tiny}
#+LATEX_HEADER: \renewcommand*{\citesetup}{\scriptsize}
#+LATEX_HEADER: \makeatletter\def\mathcolor#1#{\@mathcolor{#1}}\def\@mathcolor#1#2#3{\protect\leavevmode\begingroup\color#1{#2}#3\endgroup}\makeatother

#+LATEX_HEADER: \DeclareMathOperator{\sign}{sign}
#+LATEX_HEADER: \DeclareMathOperator{\sigmoid}{sigmoid}
#+LATEX_HEADER: \DeclareMathOperator{\softmax}{softmax}
#+LATEX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LATEX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LATEX_HEADER: \newcommand\pred[1]{\overline{#1}}
#+LATEX_HEADER: \newcommand\adv[1]{\widetilde{#1}}
#+LATEX_HEADER: \newcommand\given{\:\vert\:}
#+LATEX_HEADER: \titlegraphic{\includegraphics[width=2.5cm]{tachikoma}}

#+begin_src latex-macro
\newcommand{\advclf}{(credit: \cite{gong2017-adversarial})}
\newcommand{\advtxt}{(credit: \cite{gong2018-adversarial})}
\newcommand{\advnat}{(credit: \cite{alcorn2018-strike})}
#+end_src

#+MACRO: empty {{{tex}}}
#+MACRO: tag {{{tex({\small\uppercase{$1}})}}}
#+MACRO: cs231n [[http://cs231n.stanford.edu][cs231n]]
#+MACRO: colah-blog [[http://colah.github.io/posts/2015-08-Understanding-LSTMs][colah's blog]]

* Introduction

* Problem Overview

** Adversarial Samples
:PROPERTIES:
:BEAMER_opt: allowframebreaks
:END:

file:mnist-compare-all.png

1. Noises are very subtle, visually indistinguishable.
2. Trick machines into wrong predictions with high confidence.

\framebreak

#+ATTR_LaTeX: :width \textwidth
file:demo-adv-text.pdf

The \colorbox{red!10}{highlighted} words are changed.  \(n/L\) is the number of
words changed divided by the total number of words.  \advtxt{}

\framebreak

Objects in weird poses are also tricky!  \advnat{}

#+ATTR_LATEX: :width .7\textwidth
file:demo-natadv.png

** Adversarial Patterns for Machines

#+ATTR_LATEX: :width .8\textwidth
#+CAPTION: Adversarial patterns for different neural nets cite:moosavi-dezfooli2016-universal.
file:adv-machine.png

** Adversarial Patterns for Humans

Illusion possibly caused by the fringed edges cite:kitaoka2004-contrast.  More
examples http://www.psy.ritsumei.ac.jp/~akitaoka

*** {{{empty}}}                                                             :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.65
:END:

#+ATTR_LATEX: :height 3.5cm
file:adv-human-0.jpg

*** {{{empty}}}                                                       :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.35
:END:

#+ATTR_LATEX: :height 3.6cm
file:adv-human-1.jpg

** Motivation

This phenomenon is interesting both in practice and in theory.
1. It undermines the models' reliability.
2. Hard to ignore due to it being transferable and universal.
3. It provides new insights into neural networks:
   - Local generalization does not seem to hold.
   - Data distribution: they appear in dense regions.
   - Trade-off between robustness and generalization.
   - \(\cdots\)

* Background

** Neural Networks

It is a connectionist model.
1. Any state can be described as an \(N\)-dimensional vector of numeric
   activation values over neural units in a network.
2. Memory is created by modifying the strength of the connections between neural
   units.

#+ATTR_LaTeX: :width \textwidth
#+CAPTION: Biological neuron versus neuron model (credit: {{{cs231n}}})
file:neuron-bio.png

** Case Study: Multi-Layer Perceptron (MLP)

MLP is one of the most simple feedfoward architectures.
1. Each neuron outputs to the neurons in the next layer.
2. Neurons in the same layer have no connections.

#+ATTR_LaTeX: :width .6\textwidth
#+CAPTION: Multi-layer perceptron (credit: {{{cs231n}}})
file:mlp.jpg

** Case Study: Convolutional Neural Network (CNN)

CNN is inspired by eye structure, widely used in computer vision.
1. Each neuron receives inputs from a pool of neurons in previous layer, just
   like the convolution operation.
2. Neurons in the same layer have no connections

#+CAPTION: LetNet-5 cite:lecun1998-gradient
file:cnn.png

** Case Study: Recurrent Neural Network (RNN)

Some neurons get part of input from its output.

*** {{{empty}}}                                                      :B_only:
:PROPERTIES:
:BEAMER_act: 1
:BEAMER_env: only
:END:

#+CAPTION: Dynamic unrolling of recurrent cells. (credit: {{{colah-blog}}})
file:rnn-unrolled.png

*** {{{empty}}} :B_only:
:PROPERTIES:
:BEAMER_act: 2
:BEAMER_env: only
:END:

#+CAPTION: The double-edged sword: long term dependencies between outputs and inputs. (credit: {{{colah-blog}}})
file:RNN-longtermdependencies.png

** Generate Adversarial Images

Intuitions behind the adversarial methods
1. Move the data points
   - towards the decision
     boundary cite:moosavi-dezfooli2015-deepfool,moosavi-dezfooli2016-universal,
   - in the direction where loss increases for the clean
     samples cite:goodfellow2014-explaining,kurakin2016-adversarial-examples, or
     decreases for the adversarial samples cite:szegedy2013-intriguing, or
   - where the probability of the correct label increases or the probability of
     the target label
     increases cite:papernot2015-limitations,carlini2016-towards.
2. Map between clean and adversarial data
   points cite:zhao2017-generating,baluja2017-adversarial,xiao2018-generating.

** Intuition

#+ATTR_LaTeX: :width .9\textwidth
#+CAPTION: Data space hypothesis cite:nguyen2014-deep
file:image-space.png

* Defend against Adversarial Samples

** Gist

We investigate binary classifier as a defense method
- Recognizes adversarial samples of the same distribution.
- Does not generalize to arbitrary adversarial samples.

** Related Work

- Adversarial training :: Augment training data with adversarial
     samples cite:goodfellow2014-explaining,madry2017-towards.  \[\theta^* =
     \argmin_\theta\mathbb{E}_{x\in\mathcal{X}}\left[\mathcolor{red}{\max_{\delta
     \in [-\epsilon,\epsilon]^N}} L(x \mathcolor{red}{+ \delta};
     f_\theta)\right]\]
- Preprocess :: Transform input images, e.g.,
     denoising cite:xie2018-feature,liang2017-detecting,
     compression cite:prakash2018-protecting, quilting cite:guo2017-countering.
- Detecting :: classifier cite:metzen2017-detecting, density
     ratio estimation cite:gondara2017-detecting.

** Binary Classifier as A Defense

We propose to use a binary classifier to separate adversarial samples from clean
ones cite:gong2017-adversarial based on the following observations:
1. The adversarial noise follows a specific
   direction cite:goodfellow2014-explaining.
2. The neural nets are sensitive to individual pixel
   values cite:szegedy2013-intriguing.
Code: https://github.com/gongzhitaao/adversarial-classifier

** Adversarial Examples

# TODO[2019-02-04 Mon]: Insert MNIST, CIFAR-10, SVHN dataset examples and
# adversarial examples.  Include the adversarial results here.

#+ATTR_LATEX: :booktabs t
#+CAPTION: The target model accuracy.
| Dataset  |  \(X\) | \(\adv{X}\) |
|----------+--------+-------------|
| MNIST    | 0.9914 |      0.0213 |
| CIFAR-10 | 0.8279 |      0.1500 |
| SVHN     | 0.9378 |      0.2453 |

** Classifier Efficiency and Robustness

#+ATTR_LATEX: :booktabs t
#+CAPTION: The classifier \(g\)'s accuracy.  \(f\) is the target model.  And \(\adv{X}_f\) denotes adversarial examples targeting model \(f\).
| Dataset  | \(X\) | \(\adv{X}_f\) | \(\adv{X}_g\) | \(\{\adv{X}_f\}_g\) |
|----------+-------+---------------+---------------+---------------------|
| MNIST    |  1.00 |          1.00 |          0.00 |                1.00 |
| CIFAR-10 |  0.99 |          1.00 |          0.01 |                1.00 |
| SVHN     |  1.00 |          1.00 |          0.00 |                1.00 |

1. \(X\) and \(\adv{X}_f\) columns shows the classifier \(g\) is effective.
2. \(\adv{X}_g\) and \(\{\adv{X}_f\}_g\) columns shows the classifier \(g\) is
   robust.

** Problem with Classifier Defense

*Limitation*: different hyperparameters, different adversarial algorithms may
elude the binary classifier or adversarial training.

#+ATTR_LATEX: :booktabs t
#+CAPTION: The binary classifier, trained with FGSM adversarials with \(\epsilon = 0.03\), is unable to recognize the adversarials with \(\epsilon = 0.01\) (more subtle noise).
| \epsilon |  \(X\) | \(\adv{X}\) |
|----------+--------+-------------|
|      0.3 | 0.9996 |      1.0000 |
|      0.1 | 0.9996 |      1.0000 |
|     *0.03* | 0.9996 |      0.9997 |
|     0.01 | 0.9996 |      *0.0030* |

** Problem with Adversarial Training

#+ATTR_LATEX: :width \textwidth
#+CAPTION: Adversarial training cite:huang2015-learning,kurakin2016-adversarial-machine is not sufficient.  In the church window plot cite:warde-farley2016-adversarial, each pixel \((i, j)\) is a data point \(\adv{x}\) such that \(\adv{x} = x + \vb{h}\epsilon_j + \vb{v}\epsilon_i\), where \(\vb{h}\) is the FGSM direction and \(\vb{v}\) is a random orthogonal direction.  The \epsilon ranges from \([-0.5, 0.5]\).  \advclf{}
file:adv-training-not-working.pdf

1. {{{tex(\tikz[baseline=0.5ex]{\draw (0\,0) rectangle (2ex\,2ex)})}}} (
   {{{tex(\tikz[baseline=0.5ex]{\draw[fill=black!10] (0\,0) rectangle (2ex\,2ex)})}}}
   ) always correct (incorrectly).
2. {{{tex(\tikz[baseline=0.5ex]{\draw[fill=green!10] (0\,0) rectangle
   (2ex\,2ex)})}}} correct with adversarial training.
3. {{{tex(\tikz[baseline=0.5ex]{\draw[fill=red!10] (0\,0) rectangle (2ex\,2ex)})}}}
   correct without adversarial training.

* Generate Adversarial Texts

** Text Classification

** Text Embedding Layer

#+CAPTION: Architecture for sentence classification with CNN cite:kim2014-convolutional
#+ATTR_LaTeX: :width \textwidth
file:textcnn.png

** Text Embedding Example

=wait for the video= \(\xrightarrow{\text{tokenize}}\) [ =wait=, =for=, =the=, =video= ]
\(\xrightarrow{\text{indexer}}\) [2, 20, 34, 8]
\(\xrightarrow{\text{embedding}}\) \(\mathbb{R}^{4\times D}\), where \(D\) is
the embedding size.

- Each sentence with be converted to \(\mathbb{R}^{L\times D}\) before being fed
  into the convolution layer, where \(L\) is the sentence length.
- We usually truncate/pad sentences to the same length so that we could do
  /batch training/.
- Embedding may also be on the character-level.

** Problem Overview

Difficulties we face:
1. The text space is discrete.  Moving the data points in small steps following
   a certain direction does not work, directly.
2. Text quality is hard to measure.  /Much to learn, you still have/ (the
   Yoda-style) v.s. /You still have much to learn/ (the mundane-style)

General directions:
1. Three basic operations are available, /replacement/, /insertion/, and
   /deletion/.
2. They may work at character, word or sentence level.

** Methods

- In text space :: This class of methods need to solve two problems:
  1. what to change, e.g., random, \(\nabla L\) cite:liang2017-deep, manually
     picking cite:samanta2017-towards.
  2. change to what, e.g., random, synonyms cite:samanta2017-towards or nearest
     neighbors in embedding space, or forged
     facts cite:jia2017-adversarial,liang2017-deep.
- In latent space :: GAN cite:goodfellow2014-generative is used to map from a
     latent space (e.g., Gaussian noise) to sentences cite:zhao2017-generating.

** Adversarial Text Framework

We propose another method in the embedding space.

*** {{{empty}}}                                                             :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:

**** {{{empty}}}                                                           :B_only:
:PROPERTIES:
:BEAMER_env: only
:BEAMER_act: 1
:END:

[[file:advtext-demo-embedding.pdf]]

**** {{{empty}}}                                                           :B_only:
:PROPERTIES:
:BEAMER_env: only
:BEAMER_act: 2
:END:

[[file:advtext-demo-noise.pdf]]

**** {{{empty}}}                                                           :B_only:
:PROPERTIES:
:BEAMER_env: only
:BEAMER_act: 3
:END:

[[file:advtext-demo-knn.pdf]]

*** {{{empty}}}                                                             :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:

**** Embedding                                                       :B_only:
:PROPERTIES:
:BEAMER_env: only
:BEAMER_act: 1
:END:

Vocabulary is represented by a point
{{{tex(\tikz[baseline=-0.5ex]{\draw[fill={rgb\,255:red\,213;green\,232;blue\,212}\,line
width=0.4mm\, draw={rgb\,255:red\,130;green\,179;blue\,102}] (0\,0) circle
(0.9mm)})}}} in a high dimensional space.  Each word of =I AM HUNGRY= is first mapped
into embedding space.

**** Perturbation                                                    :B_only:
:PROPERTIES:
:BEAMER_env: only
:BEAMER_act: 2
:END:

Each point of the input word is perturbed to a new position
{{{tex(\tikz[baseline=-0.5ex]{\draw[fill={rgb\,255:red\,255;green\,242;blue\,204}\,line
width=0.4mm\, draw={rgb\,255:red\,214;green\,182;blue\,86}] (0\,0) circle
(0.9mm)})}}} following a small displacement calculated by our framework.

**** k Nearest Neighbor                                              :B_only:
:PROPERTIES:
:BEAMER_env: only
:BEAMER_act: 3
:END:

We then replace each
{{{tex(\tikz[baseline=-0.5ex]{\draw[fill={rgb\,255:red\,255;green\,242;blue\,204}\,line
width=0.4mm\, draw={rgb\,255:red\,214;green\,182;blue\,86}] (0\,0) circle
(0.9mm)})}}} with its nearest neighbor since
{{{tex(\tikz[baseline=-0.5ex]{\draw[fill={rgb\,255:red\,255;green\,242;blue\,204}\,line
width=0.4mm\, draw={rgb\,255:red\,214;green\,182;blue\,86}] (0\,0) circle
(0.9mm)})}}} usually does not correspond to a word.  After the nearest neighbor
search, we got =I AM FULL=.

** COMMENT Adversarial Text Framework

We propose another method in the embedding space.

#+begin_export latex
{\small
  \begin{codebox}
   \Procname{$\proc{Generate-Adversarial-Texts}(f, x)$}
   \li \For $i \gets 1$ \To $\attrib{x}{length}$
   \li \Do $z_i \gets \proc{Embedding}(x_i)$\End
   \li $z^\prime \gets \proc{Adv}(f, z)$
   \li \For $i \gets 1$ \To $\attrib{z^\prime}{length}$
   \li \Do $x^\prime_i \gets \proc{Nearest-Embedding}(z^\prime_i)$
   \li $s_i \gets \proc{Reverse-Embedding}(x^\prime_i) $\End
   \li \Return $s$
  \end{codebox}
}
#+end_export

Assumptions:
1. The text embedding space preserve the semantic relations.
2. Important features get more noise.

Result: https://github.com/gongzhitaao/adversarial-text

** Results On Word-Level

#+begin_export latex
\begin{table}[ht]
  \footnotesize
  \centering
  \begin{tabular}{rl*{5}{c}}
    \toprule
    Method
    & Dataset
    &
    & \multicolumn{4}{c}{Accuracy} \\
    \midrule

    \multirow{5}{*}{FGSM}
    &
    & \(\epsilon\) & 0.40 & 0.35 & 0.30 & 0.25 \\
    \cmidrule(r){3-7}
    & IMDB      & & 0.1334 & 0.1990 & 0.4074 & 0.6770 \\
    & Reuters-2 & & 0.6495 & 0.7928 & 0.9110 & 0.9680 \\
    & Reuters-5 & & 0.5880 & 0.7162 & 0.7949 & 0.8462 \\
    \cmidrule(lr){1-7}

    \multirow{5}{*}{FGVM}
    &
    & \(\epsilon\) & 15 & 30 & 50 & 100 \\
    \cmidrule(r){3-7}
    & IMDB      & & 0.8538 & 0.8354 & 0.8207 & 0.7964 \\
    & Reuters-2 & & 0.7990 & 0.7538 & 0.7156 & 0.6523 \\
    & Reuters-5 & & 0.7983 & 0.6872 & 0.6085 & 0.5111\\
    \cmidrule(lr){1-7}

    \multirow{5}{*}{DeepFool}
    &
    & \(\epsilon\) & 20 & 30 & 40 & 50 \\
    \cmidrule(r){3-7}
    & IMDB      & & 0.8298 & 0.7225 & 0.6678 & 0.6416 \\
    & Reuters-2 & & 0.6766 & 0.5236 & 0.4910 & 0.4715 \\
    & Reuters-5 & & 0.4034 & 0.2222 & 0.1641 & 0.1402 \\
    \bottomrule
  \end{tabular}
  \caption{\label{tab:acc}Word-level CNN accuracy under different parameter
    settings.  \(\epsilon\) is the noise scaling factor.}
\end{table}

#+end_export

** Case Study: DeepFool
:PROPERTIES:
:BEAMER_opt: allowframebreaks
:END:

#+begin_export latex
\begin{figure}[ht]
  \centering
  \begin{minipage}{0.45\linewidth}
    \centering
    \includegraphics[width=\textwidth]{deepfool-acc-wmd.pdf}
  \end{minipage}\hfill
  \begin{minipage}{0.45\linewidth}
    \centering
    \includegraphics[width=\textwidth]{deepfool-acc-n.pdf}
  \end{minipage}
  \caption{\label{fig:wordcnn-deepfool-acc}Word-level model's accuracy with
    varying DeepFool overshoot value.  The WMD and \(N\) (number of words
    changed) empirically show the quality of the adversarial texts.  \advtxt{}}
\end{figure}
#+end_export

\framebreak

#+ATTR_LATEX: :width \textwidth
#+CAPTION: Adversarial texts sample from Reuters-5 dataset.  \colorbox[HTML]{FFCCCC}{Original} is the original token, \colorbox[HTML]{CCFFCC}{replaced} is the adversarial token.  *[...]* denotes omitted tokens due to space constraint.  \(\epsilon=50\) in DeepFool.
file:deepfool-showcase.pdf

More results: https://gongzhitaao.org/adversarial-text

** Transferability

#+begin_export latex
\begin{figure}[ht]
  \centering
  \begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{word-deepfool-transfer.pdf}
    \caption{\footnotesize\label{fig:word-deepfool-transfer}Transferability of
      adversarial texts generated via our framework on word-level.}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{char-hotflip-transfer.pdf}
    \caption{\footnotesize\label{fig:char-hotflip-transfer}Transferability of
      adversarial texts generated via Hotflip on character-level.}
  \end{minipage}
\end{figure}
#+end_export

\(^*\) denotes the target model.  \advtxt{}

* Generate /Natural/ Adversarials

** Overview

#+ATTR_LATEX: :width .7\textwidth
#+CAPTION: Objects in weird poses.  \advnat{}
file:teaser.pdf

** Gist

A /descriptive/ study on the adversarial pose properties:
1. Effectiveness.  Only 3% are correctly recognized.
2. Imperceptible.  Small rotation (10.30\textdegree{} in yaw) results in
   an adversarial sample.
3. Good transferability.  99% against Inception-v3 transfer to AlexNet and
   ResNet-50, 75% transfer to YOLO-v3.
4. Adversarial training is not a silver bullet.

Intuition: https://gongzhitaao.org/strike-with-a-pose

** Framework

#+ATTR_LATEX: :width \textwidth
file:concept.pdf

#+begin_center
\(\Downarrow\)
#+end_center

#+ATTR_LATEX: :width .6\textwidth
file:advnat-framework.pdf

\(X\) pose parameters, 6D, \((x, y, z, \theta_x, \theta_y, \theta_z)\)\\
\(y\) prediction, a probability distribution over all labels.

** Methods

- Random search :: \\
     Randomly sample the 6D space.
- Gradient descent ::
     \[X_{k+1} = X_k + \nabla_{X_k}L(y_k, \adv{y})\]
  - Differentiable renderer, neural renderer cite:kato2018-neural
  - Non-differentiable renderer, ModernGL cite:dombi2019-moderngl

** Random Search

The distributions of each pose parameters for high-confidence (\(p \geq 0.7\))
correct/wrong classifications.  \advnat{}

*** {{{empty}}}                                                             :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:

**** {{{empty}}}                                                        :B_onlyenv:
:PROPERTIES:
:BEAMER_env: onlyenv
:BEAMER_act: 1-2
:END:

#+ATTR_LATEX: :width .98\linewidth
#+CAPTION: Correct
file:high_conf_correct_params.pdf

*** {{{empty}}}                                                             :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:

**** {{{empty}}}                                                        :B_onlyenv:
:PROPERTIES:
:BEAMER_env: onlyenv
:BEAMER_act: 1
:END:

#+ATTR_LATEX: :width .98\linewidth
#+CAPTION: Wrong
file:high_conf_params.pdf

**** {{{empty}}}                                                        :B_onlyenv:
:PROPERTIES:
:BEAMER_act: 2
:BEAMER_env: onlyenv
:END:

\footnotesize
\vspace*{-1cm}
#+ATTR_LATEX: :booktabs t
| Parameter      | Fail % | \Delta_{min}     |
|----------------+--------+------------------|
| \(x_{\delta}\) |     42 | 2.0              |
| \(y_{\delta}\) |     49 | 4.5              |
| \(z_{\delta}\) |     81 | 5.4%             |
| \(\theta_{y}\) |     69 | 10.31\textdegree |
| \(\theta_{p}\) |     83 | 8.02\textdegree  |
| \(\theta_{r}\) |     81 | 9.17\textdegree  |

\normalsize

** Methods Comparison

ZRS: z-focused random search\\
FD-G: finite difference approximated gradient\\
DR-G: differentiable renderer

\footnotesize

#+ATTR_LATEX: :booktabs t
|                  | Hit Rate % | Target Probability |
|------------------+------------+--------------------|
| ZRS              |         78 |               0.29 |
| *FD-G*             |         *92* |               *0.41* |
| DR-G\(^\dagger\) |         32 |               0.22 |

\normalsize

** Problem with Adversarial Training (again)

PT: AlexNet trained with vanilla ImageNet\\
AT: training data augmented with adversarial samples

*** {{{empty}}}                                                             :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:

#+ATTR_LATEX: :booktabs t
|                | Error |    PT |   AT |
|----------------+-------+-------+------|
| All            | Train | 99.67 |  6.7 |
|                | Test  | 99.81 | 89.2 |
|----------------+-------+-------+------|
| \(p \geq 0.7\) | Train |  87.8 |  1.9 |
|                | Test  |  48.2 | 33.3 |

*** {{{empty}}}                                                             :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:

Conclusion: adversarial training does not help models generalize to unseen
adversarial samples.

* Summary

** Summary

1. Binary classifier as a defense is effective and limited.
2. Text adversarials are also not difficult to generate.
3. Objects in weird poses are also difficult for neural nets.

** Future Work

*** {{{empty}}}                                                             :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:END:

Image credit cite:karparthy2016-connecting
file:scale.png

*** {{{empty}}}                                                             :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:

**** {{{empty}}}                                                        :B_onlyenv:
:PROPERTIES:
:BEAMER_act: 1
:BEAMER_env: onlyenv
:END:

Machine detects
- objects
- faces
- figure components
- \(\dots\)

**** {{{empty}}}                                                        :B_onlyenv:
:PROPERTIES:
:BEAMER_env: onlyenv
:BEAMER_act: 2
:END:

Cannot understand
- mirror
- shadows
- jokes
- \(\dots\)

#+begin_export latex
\tikz[remember picture, overlay] \node[anchor=center] at ($(current page.center) - (4,4)$) {\includegraphics[width=3cm]{tachikoma}};
#+end_export

** {{{empty}}}

#+ATTR_LATEX: :width .7\textwidth
file:deepmind_logo.png

#+begin_center
Fall 2019: Research Engineer at Google DeepMind (Montreal)
#+end_center

* Bibliography

** {{{empty}}}
:PROPERTIES:
:BEAMER_opt: allowframebreaks
:END:

#+LaTeX: \printbibliography

\tiny\(\begin{array}{l}\includegraphics[height=.8cm]{tachikoma}\end{array}\) by
arrghman.deviantart.com @DeviantArt\normalsize
