#+TITLE: On the Machine Illusion
#+SUBTITLE: Empirical Study on Adversarial Samples
#+DATE: 2019 Feburary
#+AUTHOR: Zhitao Gong
#+EMAIL: gong@auburn.edu
#+OPTIONS: H:2 ^:{} _:{} toc:nil
#+STARTUP: hideblocks showcontent

#+LATEX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [dvipsnames]

#+LATEX_HEADER: \usepackage{svg}
#+LATEX_HEADER: \usepackage{mathtools}
#+LATEX_HEADER: \usepackage{clrscode3e}
#+LATEX_HEADER: \usepackage{lmodern}
# #+LATEX_HEADER: \usepackage{xcolor}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{physics}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage[backend=biber,style=alphabetic]{biblatex}
#+LATEX_HEADER: \usepackage[scaled=0.85]{newtxtt}

#+LATEX_HEADER: \institute{Auburn University}
#+LATEX_HEADER: \addbibresource{refdb.bib}
#+LATEX_HEADER: \AtBeginSection[]{\begin{frame}<beamer>\frametitle{Outline}\tableofcontents[currentsection]\end{frame}}
#+LATEX_HEADER: \beamertemplatenavigationsymbolsempty
#+LATEX_HEADER: \setbeamertemplate{footline}[frame number]
#+LATEX_HEADER: \setbeamertemplate{background}{\tikz[overlay,remember picture]\node at (current page.north east)[anchor=north east]{\includegraphics[width=1cm]{img/au-15.png}};}
#+LATEX_HEADER: \setbeamersize{description width=0.5cm}

#+LATEX_HEADER: \defbeamertemplate*{bibliography item}{triangletext}{\insertbiblabel}
#+LATEX_HEADER: \renewcommand*{\bibfont}{\tiny}
#+LATEX_HEADER: \renewcommand*{\citesetup}{\scriptsize}
#+LATEX_HEADER: \makeatletter\def\mathcolor#1#{\@mathcolor{#1}}\def\@mathcolor#1#2#3{\protect\leavevmode\begingroup\color#1{#2}#3\endgroup}\makeatother

#+LATEX_HEADER: \DeclareMathOperator{\sign}{sign}
#+LATEX_HEADER: \DeclareMathOperator{\sigmoid}{sigmoid}
#+LATEX_HEADER: \DeclareMathOperator{\softmax}{softmax}
#+LATEX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LATEX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LATEX_HEADER: \newcommand\pred[1]{\overline{#1}}
#+LATEX_HEADER: \newcommand\adv[1]{\widetilde{#1}}
#+LATEX_HEADER: \newcommand\given{\:\vert\:}

#+MACRO: empty {{{tex}}}
#+MACRO: tag {{{tex({\small\uppercase{$1}})}}}
#+MACRO: cs231n [[http://cs231n.stanford.edu][cs231n]]
#+MACRO: colah-blog [[http://colah.github.io/posts/2015-08-Understanding-LSTMs][colah's blog]]

* Introduction

** Neural Networks

It is a connectionist model.
1. Any state can be described as an \(N\)-dimensional vector of numeric
   activation values over neural units in a network.
2. Memory is created by modifying the strength of the connections between neural
   units.

#+ATTR_LaTeX: :width \textwidth
#+CAPTION: Biological neuron versus neuron model (credit: {{{cs231n}}})
[[file:img/neuron.png]]

** Architectures: Multi-Layer Perceptron (MLP)

MLP is one of the most simple feedfoward architectures.
1. Each neuron outputs to the neurons in the next layer.
2. Neurons in the same layer have no connections.

#+ATTR_LaTeX: :width .6\textwidth
#+CAPTION: Multi-layer perceptron (credit: {{{cs231n}}})
[[file:img/mlp.jpg]]

** Architectures: Convolutional Neural Network (CNN)

CNN is inspired by eye structure, widely used in computer vision.
1. Each neuron receives inputs from a pool of neurons in previous layer, just
   like the convolution operation.
2. Neurons in the same layer have no connections

#+CAPTION: LetNet-5 \cite{lecun1998-gradient}
[[file:img/cnn.png]]

** Architectures: Recurrent Neural Network (RNN)

Some neurons get part of input from its output.

*** {{{empty}}} :B_only:
:PROPERTIES:
:BEAMER_act: 1
:BEAMER_env: only
:END:

#+CAPTION: Dynamic unrolling of recurrent cells. (credit: {{{colah-blog}}})
[[file:img/rnn-unrolled.png]]

*** {{{empty}}} :B_only:
:PROPERTIES:
:BEAMER_act: 2
:BEAMER_env: only
:END:

#+CAPTION: The double-edged sword: long term dependencies between outputs and inputs. (credit: {{{colah-blog}}})
[[file:img/RNN-longtermdependencies.png]]

** Notations

For clarity, we use the following notations in this slide.
- \(f\) denotes the neural nets model, \(\theta\) the model's parameters,
  and sometimes \(f_\theta\) for brevity.
- \(x\) is the input, \(\pred{y}\) the model's output (i.e., \(\pred{y} =
  f(x)\)).
- \(z\) is the un-normalized logits, i.e., \(y = \sigmoid(z)\) or \(y =
  \softmax(z)\).
- \(L\) denotes the loss function, e.g., cross-entropy, mean-squared error.  For
  simplicity, we use \(L_x\) to denote the loss value when \(x\) is the input.
- \(\adv{x}\) denotes the adversarial sample from \(x\).
- In a targeted method, \(y_t\) denotes the \textcolor{red}{t}arget class value,
  \(y_o\) the \textcolor{red}{o}ther class values.  For example, \(y = [0.2,
  0.5, 0.3]\) and \(t = 0\), then \(y_t = 0.2\) and \(y_o\in\{0.5, 0.3\}\).
  Same for \(z\).

* Problem Overview

** Adversarial Samples
:PROPERTIES:
:BEAMER_opt: allowframebreaks
:END:

[[file:img/mnist-compare-all.png]]

1. Visually very close, noises are very subtle.
2. Trick machines into wrong predictions with high confidence.

#+LaTeX: \framebreak

#+CAPTION: Adversarial texts by our framework.
#+ATTR_LaTeX: :width \textwidth
[[file:img/demo-adv-text.pdf]]

The \colorbox{red!10}{highlighted} words are changed.  The \(n/L\) is the number
of words changed divided by the total number of words.

** Adversarial Patterns for Machines

#+CAPTION: Adversarial patterns for different neural nets \cite{moosavi-dezfooli2016-universal}.
#+NAME: fig:adv-machine
[[file:img/adv-machine.png]]

** Adversarial Patterns For Humans

#+CAPTION: The blue lines are parallel.  This illusion is possibly caused by the fringed edges \cite{kitaoka2004-contrast}.
#+NAME: fig:adv-human
[[file:img/adv-human.jpg]]

More examples: http://www.psy.ritsumei.ac.jp/~akitaoka.

** Why Study Adversarial Samples

This phenomenon is interesting both in practice and in theory.
1. It undermines the models' reliability.
2. Hard to ignore due to it being transferable and universal.
3. It provides new insights into neural networks:
   - Local generalization does not seem to hold.
   - Data distribution: they appear in dense regions.
   - Trade-off between robustness and generalization.
   - \(\cdots\)

* Generate Adversarial Images

** Overview

Intuitions behind the adversarial methods
1. Move the data points
   - towards the decision
     boundary cite:moosavi-dezfooli2015-deepfool,moosavi-dezfooli2016-universal,
   - in the direction where loss increases for the clean
     samples cite:goodfellow2014-explaining,kurakin2016-adversarial-examples, or
     decreases for the for the adversarial
     decreases cite:szegedy2013-intriguing, or
   - increase the probability for the correct label and/or decrease the
     others cite:papernot2015-limitations,carlini2016-towards.
2. Map between clean and adversarial data
   points cite:zhao2017-generating,baluja2017-adversarial,xiao2018-generating.

** Intuition

#+ATTR_LaTeX: :width .9\textwidth
#+CAPTION: Data space hypothesis cite:nguyen2014-deep
[[file:img/image-space.png]]

* Defend against Adversarial Samples

** {{{tag(Related)}}} Adversarial Training

Basic ideas: incorporate adversarial samples during training.

Given a training set \(\mathcal{X}\), instead of solving the following problem

\[\theta^* = \argmin_\theta\mathbb{E}_{x\in\mathcal{X}}L(x; f_\theta)\]

we expand each data point to include its vicinity.

\[\theta^* =
\argmin_\theta\mathbb{E}_{x\in\mathcal{X}}\left[\mathcolor{red}{\max_{\delta \in
[-\epsilon,\epsilon]^N}} L(x \mathcolor{red}{+ \delta}; f_\theta)\right]\]

cite:goodfellow2014-explaining,madry2017-towards solve the inner maximization
problem by mixing dynamically generated adversarial samples into training data.

** {{{tag(Related)}}} Pre-process Inputs

Without re-training the models, this direction focuses on the inputs.
1. Transform inputs to (hopefully) recover the bad samples.
2. Filter out bad samples by image statistics.

** Binary Classifier as A Defense

We propose to use a binary classifier to separate adversarial samples from clean
ones cite:gong2017-adversarial based on the following observations:
1. The adversarial noise follows a specific
   direction cite:goodfellow2014-explaining.
2. The neural nets are sensitive to individual pixel
   values cite:szegedy2013-intriguing.
Code: https://github.com/gongzhitaao/adversarial-classifier

** Adversarial Examples

# TODO[2019-02-04 Mon]: Insert MNIST, CIFAR-10, SVHN dataset examples and
# adversarial examples.  Include the adversarial results here.

#+ATTR_LATEX: :booktabs t
#+CAPTION: The target model accuracy.
| Dataset  |  \(X\) | \(\adv{X}\) |
|----------+--------+-------------|
| MNIST    | 0.9914 |      0.0213 |
| CIFAR-10 | 0.8279 |      0.1500 |
| SVHN     | 0.9378 |      0.2453 |

** Classifier Efficiency and Robustness

#+ATTR_LATEX: :booktabs t
#+CAPTION: The classifier \(g\)'s accuracy.  \(f\) is the target model.  And \(\adv{X}_f\) denotes adversarial examples targeting model \(f\).
| Dataset  | \(X\) | \(\adv{X}_f\) | \(\adv{X}_g\) | \(\{\adv{X}_f\}_g\) |
|----------+-------+---------------+---------------+---------------------|
| MNIST    |  1.00 |          1.00 |          0.00 |                1.00 |
| CIFAR-10 |  0.99 |          1.00 |          0.01 |                1.00 |
| SVHN     |  1.00 |          1.00 |          0.00 |                1.00 |

1. \(X\) and \(\adv{X}_f\) columns shows the classifier \(g\) is effective.
2. \(\adv{X}_g\) and \(\{\adv{X}_f\}_g\) columns shows the classifier \(g\) is
   robust.

** Problem with Classifier Defense

*Limitation*: different hyperparameters, different adversarial algorithms may
elude the binary classifier or adversarial training.

#+ATTR_LATEX: :booktabs t
#+CAPTION: The binary classifier, trained with FGSM adversarials with \(\epsilon = 0.03\), is unable to recognize the adversarials with \(\epsilon = 0.01\) (more subtle noise).
| \epsilon |  \(X\) | \(\adv{X}\) |
|----------+--------+-------------|
|      0.3 | 0.9996 |      1.0000 |
|      0.1 | 0.9996 |      1.0000 |
|     *0.03* | 0.9996 |      0.9997 |
|     0.01 | 0.9996 |      *0.0030* |

** Problem with Adversarial Training

#+ATTR_LATEX: :width \textwidth
#+CAPTION: Adversarial training cite:huang2015-learning,kurakin2016-adversarial-machine is not sufficient.  In the church window plot cite:warde-farley2016-adversarial, each pixel \((i, j)\) is a data point \(\adv{x}\) such that \(\adv{x} = x + \vb{h}\epsilon_j + \vb{v}\epsilon_i\), where \(\vb{h}\) is the FGSM direction and \(\vb{v}\) is a random orthogonal direction.  The \epsilon ranges from \([-0.5, 0.5]\).
file:img/adv-training-not-working.pdf

1. {{{tex(\tikz[baseline=0.5ex]{\draw (0\,0) rectangle (2ex\,2ex)})}}} (
   {{{tex(\tikz[baseline=0.5ex]{\draw[fill=black!10] (0\,0) rectangle (2ex\,2ex)})}}}
   ) always correct (incorrectly).
2. {{{tex(\tikz[baseline=0.5ex]{\draw[fill=green!10] (0\,0) rectangle
   (2ex\,2ex)})}}} correct with adversarial training.
3. {{{tex(\tikz[baseline=0.5ex]{\draw[fill=red!10] (0\,0) rectangle (2ex\,2ex)})}}}
   correct without adversarial training.

* Generate Adversarial Texts

** Text Embedding Layer

#+CAPTION: Architecture for sentence classification with CNN cite:kim2014-convolutional
#+ATTR_LaTeX: :width \textwidth
[[file:img/textcnn.png]]

** Text Embedding Example

"wait for the video" \(\xrightarrow{\text{tokenize}}\) ["wait", "for", "the",
"video"] \(\xrightarrow{\text{indexer}}\) [2, 20, 34, 8]
\(\xrightarrow{\text{embedding}}\) \(\mathbb{R}^{4\times D}\), where \(D\) is
the embedding size.

- Each sentence with be converted to \(\mathbb{R}^{L\times D}\) before being fed
  into the convolution layer, where \(L\) is the sentence length.
- We usually truncate/pad sentences to the same length so that we could do
  /batch training/.
- Embedding may also be on the character-level.

** Problem Overview

Difficulties we face:
1. The text space is discrete.  Moving the data points in small steps following
   a certain direction does not work, directly.
2. Text quality is hard to measure.  /Much to learn, you still have/ (the
   Yoda-style) v.s. /You still have much to learn/ (the mundane-style)

General directions:
1. Three basic operations are available, /replacement/, /insertion/, and
   /deletion/.
2. They may work at character, word or sentence level.

** Methods in Text Space

This class of methods need to solve two problems:
1. what to change, e.g., random, \(\nabla L\) cite:liang2017-deep, manually
   picking cite:samanta2017-towards.
2. change to what, e.g., random, synonyms cite:samanta2017-towards or nearest
   neighbors in embedding space, or forged
   facts cite:jia2017-adversarial,liang2017-deep.

** Methods in Transformed Space

Autoencoder cite:hinton2006-reducing is used to map between texts and a
continuous space cite:zhao2017-generating.  The embedded space is smooth.

#+ATTR_LaTeX: :width .7\textwidth
[[file:img/Autoencoder_structure.png]]

** Adversarial Text Framework

We propose another method in the embedding space.

#+BEGIN_EXPORT latex
{\small
  \begin{codebox}
   \Procname{$\proc{Generate-Adversarial-Texts}(f, x)$}
   \li \For $i \gets 1$ \To $\attrib{x}{length}$
   \li \Do $z_i \gets \proc{Embedding}(x_i)$\End
   \li $z^\prime \gets \proc{Adv}(f, z)$
   \li \For $i \gets 1$ \To $\attrib{z^\prime}{length}$
   \li \Do $x^\prime_i \gets \proc{Nearest-Embedding}(z^\prime_i)$
   \li $s_i \gets \proc{Reverse-Embedding}(x^\prime_i) $\End
   \li \Return $s$
  \end{codebox}
}
#+END_EXPORT

Assumptions:
1. The text embedding space preserve the semantic relations.
2. Important features get more noise.

Result: https://github.com/gongzhitaao/adversarial-text

** COMMENT Next Step

1. Find appropriate quality measurement for texts, e.g., language model scores,
   Word Mover's Distance (WMD).
2. Find a way to control the quality of generated adversarial texts.
3. Test the transferability of adversarial texts.

* Generate /Natural/ Adversarials

* Summary

** Adversarial Samples

1. All classification models are affected.
2. Seems to exist in dense regions.
3. Distribute along only certain directions.
4. Transfer to different models or techniques.
5. \(\cdots\)

#+BEGIN_CENTER
ALL EMPIRICAL AND HYPOTHESIS SO FAR
#+END_CENTER

* Bibliography

** {{{empty}}}
:PROPERTIES:
:BEAMER_opt: allowframebreaks
:END:

#+LaTeX: \printbibliography
