#+TITLE: On the Machine Illusion
#+AUTHOR: Zhitao Gong
#+DATE: April 12, 2018

#+KEYWORDS: Adversarial, Security, Deep Learning, Computer Vision, NLP

#+STARTUP: content hideblocks
#+OPTIONS: toc:nil H:4

#+LATEX_CLASS: report
#+LATEX_CLASS_OPTIONS: [12pt, dvipsnames]
#+LATEX_HEADER: \usepackage{auphd}
#+LATEX_HEADER: \usepackage{afterpage}
#+LATEX_HEADER: \usepackage{algorithmic}
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage[backend=biber]{biblatex}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{caption}
#+LATEX_HEADER: \usepackage[inline]{enumitem}
#+LATEX_HEADER: \usepackage{makeidx}
#+LATEX_HEADER: \usepackage{multirow}
#+LATEX_HEADER: \usepackage{physics}
#+LATEX_HEADER: \usepackage{subcaption}
#+LATEX_HEADER: \usepackage{threeparttable}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage{xcolor}

#+LATEX_HEADER: \graphicspath{{img/}}

#+LATEX_HEADER: \renewcommand\maketitle{}
#+LATEX_HEADER: \addbibresource{refdb.bib}
#+LATEX_HEADER: \addbibresource{local.bib}

#+LATEX_HEADER: \DeclareMathOperator{\argmax}{arg\,max}
#+LATEX_HEADER: \DeclareMathOperator{\argmin}{arg\,min}
#+LATEX_HEADER: \DeclareMathOperator{\sign}{sign}
#+LATEX_HEADER: \newcommand\pred[1]{\overline{#1}}
#+LATEX_HEADER: \newcommand\given{\:\vert\:}

#+LATEX_HEADER: \copyrightyear{2019}
#+LATEX_HEADER: \keywords{Adversarial, Security, Deep Learning, Computer Vision, Natural Language Processing}
#+LATEX_HEADER: \adviser{Dr. Wei-Shinn Ku}
#+LATEX_HEADER: \professor{Wei-Shinn Ku, Associate Professor of Computer Science and Software Engineering}
#+LATEX_HEADER: \professor{Anh Nguyen, Assistant Professor of Computer Science and Software Engineering}
#+LATEX_HEADER: \professor{Shiwen Mao, Professor of Electrical and Computer Engineering}
#+LATEX_HEADER: \professor{Xiao Qin, Professor of Computer Science and Software Engineering}

#+LaTeX: \setcounter{page}{0}\afterpage{\pagenumbering{Roman}}
#+LaTeX: \TitlePage
#+LaTeX: \singlespacing

* Abstract                                                           :ignore:

#+BEGIN_abstract

The existence of adversarial samples reveals yet another quirk in neural nets.
The clean samples, when perturbed with very small carefully chosen noise (e.g.,
change of color for a few pixels in an image, or replacement of a few words in a
text piece), may be turned into adversarial ones.  Despite that they are almost
the same (visually or semantically) as the original one from the perspective of
human beings, the adversarial samples will trick the well-trained neural nets
into wrong predictions with very high confidence (probabilities).  The
investigation into this phenomenon has important implications both in practice
and in theory.  In the real word, more and more tasks are automated by neural
nets, e.g., inappropriate comments and images filtering, computer virus
detection, spam filtering, etc.  For example, the adversarial samples might be
wrongly leveraged to bypass the machine models.  The replacement of a few
seemingly non-important words in a sentence could turn a hateful comment into a
"good" one, from the machine's point of view.  This may potentially cause severe
social problems if our models are not made more robust and intelligent.  On the
other hand, to explain this phenomenon rigorously, some of our preconceived
intuitions and hypothesis about neural nets need to be revised, e.g., the
generalization hypothesis, training dynamics, etc.  We are still far away from a
unified theory explaining how neural nets work, this study will at least provide
us more insight towards our final goal.

Some pieces of my work are finished.  In our first work, we propose a simple yet
effective binary classifier to filter out the adversarial samples.  Furthermore,
we also discuss in details the limitations of our approach, which is,
unfortunately, shared among many other proposed defense methods.  In our second
and ongoing work, we propose a model gradient-based framework to generate
adversarial samples for text models.  The main difficulty to generate
adversarial texts with model gradient-based methods is that the input space is
discrete, which makes it unclear to how to accumulate the small noise directly
on the inputs.  We work around this problem by searching for adversarials in the
embedding space and then reconstruct the adversarial texts from the noise
embeddings.  Our third work is yet concretized.  The high level direction will
be that we first study the adversarial samples in classical machine learning
models (e.g., linear models, support vector machine (SVM), nearest neighbors),
for which the training dynamics and solutions are well-understood, and for which
the solutions can usually be laid out in explicit forms.  With intuitions and
ideas gathered from these models, we then search for possible analogies in the
realm of neural network.

#+END_abstract

* COMMENT Acknowledgments                                            :ignore:

#+BEGIN_acknowledgments

I would like to thank Ms. Jiao Yu for her insightful discussion and
contribution.  We both work on this work.  Ms. Yu worked on the particle filter
and snapshot queries, while I focus on Kalman filter and continuous queries.

In addition, I would also like to thank Dr. Ku for his invaluable guidance
during my research.  I learned how to think like a researcher and how to narrow
down areas of focus through our weekly discussion.  I also greatly appreciate
his patience for my really slow research progress.

#+END_acknowledgments

* Table of Contents                                                  :ignore:

#+LaTeX: \tableofcontents

* List of Figures                                                    :ignore:

#+LaTeX: \listoffigures

* List of Tables                                                     :ignore:

#+LaTeX: \listoftables
#+LaTeX: \afterpage{\setcounter{page}{0}\pagenumbering{arabic}}

* Introduction
:PROPERTIES:
:CUSTOM_ID: part:introduction
:END:

** Problem Overview
:PROPERTIES:
:CUSTOM_ID: chp:problem-overview
:END:

Artificial intelligence (AI) helps us in many challenging tasks, e.g.,
recommendation systems, search, computer vision (CV), natural language
processing (NLP), machine translations (MT), etc.  The workhorse behind AI are
the numerous machine learning models, including classical models (e.g.,
generalized linear models, SVM) and deep learning models (e.g., neural nets,
deep reinforcement learning).  Deep learning models, especially neural
nets-based models, achieve state-of-the-art results in many fields.  However,
these models are not well understood yet.

cite:szegedy2013-intriguing shows that the state-of-the-art image models may be
tricked into wrong predictions when the test images are perturbed with carefully
crafted noise.  Furthermore, these perturbed images appear visually almost the
same as the original ones from the perspective of human beings.  These images
are called /adversarial images/ cite:szegedy2013-intriguing.  Many followup work
show that the adversarial samples are more universal than expected.  Figure
ref:fig:mnistdemo gives an example of the adversarial images.  In Figure
ref:fig:mnistdemo, the first row of column FGSM, FGVM, JSMA and DeepFool show
adversarial images crafted from a clean image from MNIST cite:lecun2010-mnist,
the first image in the first row.  The second row visualizes the noise scale
(i.e., the pixel difference between adversarial image and the original one) in
heatmap.  Since the pixel values are normalized to \((0, 1)\) before being fed
into the classification models, the noise value range is \((-1, 1)\).  The
predicted label and confidence for each image is shown at the bottom of each
column.

#+ATTR_LaTeX: :width .8\textwidth
#+CAPTION: Adversarial images from MNIST dataset.
#+NAME: fig:mnistdemo
[[file:imgdemo.pdf]]

All the neural nets-based image classification models are vulnerable to
adversarial samples.  Another study cite:papernot2016-transferability shows that
even classical machine learning models are affected by the adversarial samples
to some degree.  Furthermore, many work on generating adversarial samples
indicate that it is very cheap and easy to compute the noise.  Worse still,
cite:papernot2016-transferability demonstrates that the adversarial samples
exhibit transferability.  In other words, adversarial samples crafted for one
model are likely to be adversarial for a different model, e.g., another model
with different hyperparameters, or even different techniques.

** Road Maps
:PROPERTIES:
:CUSTOM_ID: chp:road-maps
:END:

In a series of work, we plan to investigate the adversarial phenomenon in
detail, both empirically and theoretically.  Concretely, we have planned out
three projects, each with a different focus regarding this problem.
1. *Generate adversarial texts*.  Lots of work in literature focus on generating
   images adversarials.  The difficulty of generating adversarial texts are
   two-folds.  First, the input space is discrete, which makes it unclear how to
   perturb the input by iteratively accumulating small noise, as is commonly
   done in generating adversarial images.  Second, the quality evaluation of the
   generated texts are intrinsically difficult.  Besides human evaluation, we
   have yet found better ways to compare the quality of two text piece.  We
   propose a framework to workaround the first problem.  Preliminary results
   show that our framework can be applied to a wide range of text models.  The
   details are discussed in Part ref:part:generate-adversarial-samples.
2. *Defend adversarials*.  As the adversarial samples may severely degrade the
   performance of machine learning models, we are empirically evaluating
   different defensive methods to detect adversarial samples.  Specifically, in
   one of our work, we are experimenting with binary classifier to separate
   adversarial samples from clean ones.  The preliminary results demonstrate
   that it works well in practice.  However, there are limitations to this
   binary-classifier approach.  The details are in Part
   ref:part:defend-adversarial-samples.
3. *Input space topology*.  A few work
   cite:tabacof2015-exploring,goodfellow2014-explaining,warde-farley2016-adversarial
   have empirically studied the distribution of adversarial samples.  We plan to
   start off from the previous work and follow this direction further.  The
   details are still vague and need further refinement.

* Generating Adversarial Samples
:PROPERTIES:
:CUSTOM_ID: part:generate-adversarial-samples
:END:

** Introduction
:PROPERTIES:
:CUSTOM_ID: h1-introduction-71377
:END:

The phenomenon of adversarial samples was first discussed
in cite:szegedy2013-intriguing.  The authors show that images perturbed with
carefully crafted noise may trick the deep neural nets into wrong predictions
with very high confidence.  There has been an abundance of followup work on
methods to generate adversarial images.  This adversarial phenomenon arouses
great interest among researchers since it is of great importance both in
practice and in theory.  On the one hand, these adversarial samples undermines
the reliability of deep models.  It seems that deep models may fail unexpected
in some conditions which we struggle to understand.  This would raise concerns
about the application of deep models to some critical areas, e.g., self-driving
cars.  On the other hand, we assume implicitly /good local generalization/ when
our model generalizes well to test data cite:szegedy2013-intriguing.  However,
some work empirically show that adversarial samples may exist in dense regions
around the clean samples, which seems to contradicts the aforementioned
hypothesis.  Further study of this phenomenon, both empirically and
theoretically, will help us understand more about the dynamics of deep models.

The adversarial images have been extensively studied.  Many adversarial
generating methods have been proposed in the literature, e.g, fast gradient
method (FGM) cite:goodfellow2014-explaining, Jacobian-based saliency map
approach (JSMA) cite:papernot2015-limitations,
DeepFool cite:moosavi-dezfooli2015-deepfool, CW cite:carlini2016-towards, etc.
Many theoretical explanation of adversarial samples also focused on image data
and architectures cite:peck2017-lower,goodfellow2014-explaining.  Some work have
expanded the study to other domains, e.g, speech-to-text cite:carlini2018-audio,
neural translation cite:zhao2017-generating, reinforcement
learning cite:lin2017-tactics, etc.  These extended work will give us a more
thorough understanding of the adversarial samples.  To this end, we propose a
simple yet effective framework to adapt the adversarial methods for images to
generating adversarial texts.  Specifically, we focus on adversarial samples for
text classification models.  There are two major difficulties to generate
adversarial texts:

1. The input space is discrete.  As a result, it is unclear how to (iteratively)
   accumulate small noise to perturb the input.  Working with Image domain is
   easier since we usually normalize the input to a continuous domain \([0,
   1]\).
2. The text quality measurement and control is intricate in itself.  It is a
   very subjective matter.  For example, let's compare the Master Yoda-style way
   of speaking, /Much to learn, you still have/, with the mundane-style, /You
   still have much to learn/.  Which is better?  Which gets a high score?  Star
   Wars fans will definitely favor the Yoda-style, although both sentences
   successfully convey exactly the same meaning.

To resolve the first problem, we propose a general framework in which we
generate adversarial texts via slightly modified methods borrowed from image
domain.  We first search for adversarials in the text embedding space (e.g.,
word-level embedding cite:mikolov2013-efficient, character-level
embedding cite:kim2015-character), and then reconstruct the adversarial texts
with nearest neighbor search.  The second problem is open-ended, we employ two
metrics to quantify the results, i.e., the Word Mover's Distance
(WMD) cite:kusner2015-from and change ratio (the number of words changed).  In
our experiments, they serve their purpose well at a rather coarse level.  These
two metrics, however, does not perform consistently when two text pieces are
about the same quality (e.g., the aforementioned Yoda-style and mundane-style).
The text quality is controlled empirically by the noise level in our
experiments.

The contribution of our work lies in two-folds:

1. We propose a general framework to generate adversarial texts.  Any of the
   existing adversarial methods may be adapted to generate adversarial texts
   under our framework.
2. We empirically compare the word-level and character-level adversarial texts,
   e.g., transferability, text quality, etc.

This paper is organized as follows.  we survey recent work on generating
adversarial images and texts in Section [[ref:h1-related-work-81bde]].  A brief
review about defending against adversarials is included in
Section [[ref:h1-related-work-81bde]].  Our adversarial text framework is detailed
in Section [[ref:h1-adversarial-text-framework-774a3]].  We thoroughly evaluate our
framework and Hotflip cite:ebrahimi2017-hotflip on various text benchmarks and
report the results in Section [[ref:h1-experiment-2f800]].  We then conclude this
paper and provide directions for future work in Section [[ref:h1-conclusion-2763d]].

** Related Work
:PROPERTIES:
:CUSTOM_ID: h1-related-work-81bde
:END:

The phenomenon of adversarial samples was first discussed
in cite:szegedy2013-intriguing.  Many followup work propose different methods to
generate adversarial samples.  In addition, many work investigate defense
methods due to the security concern raised by adversarial samples.  Generally
speaking, so far as we see in literature, the attacking is much easier and
cheaper than defense.

For notation, \(x\) denotes the input, \(y\) the prediction, \(f\) the target
model such that \(y = f(x)\), \(L\) the loss function, \(x^*\) the adversarial
sample.  \(\|\cdot\|_p\) denotes the \(p\)-norm.  We slightly abuse the notation
here, \(L_x\) denotes the loss with \(x\) as the input.

#+ATTR_LaTeX: :width .7\linewidth
#+CAPTION: Random MNIST adversarial images generated via different attacking algorithms.  The upper image in /Clean/ column is the original clean image.   The upper images in the following columns are adversarial images generated by the corresponding attacking algorithm based on the first clean image, respectively.  The lower image in each column is the difference between the adversarial image and the clean image, illustrated in heatmap.  Below each column is the label predicted by the target model, along with probability in parenthesis.
#+NAME: fig:mnistdemo
[[file:imgdemo.pdf]]

*** Generate Adversarial Images
:PROPERTIES:
:CUSTOM_ID: h2-generate-adversarial-images-d136a
:END:

Generally speaking, the proposal methods in literature fall into two strategies,
the first one is to move data points around till the label changes, and the
other is to create a mapping between clean and adversarial samples (or noises).

**** Move Data Points
:PROPERTIES:
:CUSTOM_ID: h3-move-data-points-9a4e4
:END:

Essentially, this class of methods move the data points along a carefully chosen
direction.  It has been shown that it is very unlikely to arrive at adversarial
samples following a random walk cite:szegedy2013-intriguing.
1. The direction may be where the loss for clean samples increases, e.g.,
   FGSM cite:goodfellow2014-explaining and its
   variants cite:kurakin2016-adversarial,miyato2015-distributional,kurakin2016-adversarial),
   or where the loss for adversarial samples decreases, e.g.,
   cite:szegedy2013-intriguing.
2. The direction may also be where the probability of the correct label
   decreases (or the probabilities of the target label increases), e.g.,
   JSMA cite:papernot2015-limitations, CW cite:carlini2016-towards.
3. It could also be the direction towards the decision boundary (e.g.,
   DeepFool cite:moosavi-dezfooli2015-deepfool, one-pixel
   attack cite:su2017-one).

**** Map Clean Samples to Adversarial
:PROPERTIES:
:CUSTOM_ID: h3-map-clean-samples-to-adversarial-b4e9b
:END:

This class of methods are relatively less explored.  Adversarial transformation
network (ATN) cite:baluja2017-adversarial employs an autoencoder to generate
adversarial samples or noises.  cite:xiao2018-generating,zhao2017-generating
employs a generative model (i.e., GAN cite:goodfellow2014-generative) to map
from clean samples to adversarial ones.  The advantages of this class of methods
are
#+BEGIN_EXPORT latex
\begin{enumerate*}
 \item that the generation is usually fast since only one pass of forward
 computation is needed, and
 \item that the adversarial sames may be of great diversity if a generative
 network is used.
\end{enumerate*}
#+END_EXPORT

*** Generate Adversarial Texts
:PROPERTIES:
:CUSTOM_ID: h2-generate-adversarial-texts-f1b71
:END:

**** Text-space Methods
:PROPERTIES:
:CUSTOM_ID: h3-text-space-method-e741b
:END:

This class of methods perturbs the input texts directly.  One disadvantage is
that the computation cost is usually very high.  To perturb the input texts
directly, two decisions need to be made:
1. /What to change/.  Generally speaking, the words that have more influence on
   the result should be altered first.  Similar to JSMA,
   cite:liang2017-deep,samanta2017-towards compute importance score for each
   word based on \(\nabla L\) or \(\nabla f\). In cite:jia2017-adversarial, the
   author manually construct fake facts around the sentence that contains the
   answer.  cite:anonymous2018-adversarial alters the input sentence in a
   brutal-force way, where each word is altered in sequence until an adversarial
   sample is found or a threshold on the maximum number of words to change is
   reached.
2. /Change to what/.  Typos usually achieve good results, as shown in
   cite:samanta2017-towards,liang2017-deep.  The disadvantage of typos is that
   they are relatively easier to be corrected by the auto spelling correction
   applications, e.g., Grammerly.  Replacing with synonyms and antonyms (e.g.,
   from Thesaurus) is also a good choice
   cite:liang2017-deep,samanta2017-towards.  cite:anonymous2018-adversarial uses
   semantically related words as potential replacements.  As text
   embeddings cite:mikolov2013-efficient have been shown to preserve semantic
   relations among words, the semantically related words can be approximated by
   nearest neighbor search in the embedding space.

**** Transformed-space Methods
:PROPERTIES:
:CUSTOM_ID: h3-transformed-space-methods-76e1e
:END:

This class of methods first map text inputs to a smooth space and search for
potential adversarial samples in the smooth space via methods borrowed from
adversarial images generation.  Then the adversarial texts are reconstructed
and further verified in the original text space.  Usually some portion of the
reconstructed texts are unsuccessful adversarial samples and are filtered out.

cite:zhao2017-generating employs an autoencoder to map between the input text
and a Gaussian noise space.  The decoder is a generator (i.e.,
GAN cite:goodfellow2014-generative), while the encoder is an MLP (called
inverter in the paper).  They search in the noise space with random walk.
However, the disadvantage is that they do not have an explicit control of the
quality of the adversarial samples.  As we have see in cite:zhao2017-generating,
the generated adversarial images on complex dataset usually have large visual
changes.  Similarly, another generator-based method is proposed
in cite:wong2017-dancin where the whole network is trained with
REINFORCE cite:williams1992-simple algorithms.

In cite:liang2017-deep, the authors attempt FGM directly on character-level
convolution networks cite:zhang2015-character.  Although the labels of the text
pieces are successfully altered, the texts are changed to basically random
stream of characters which is beyond understanding.

A highly related work is also report in cite:ebrahimi2017-hotflip where the
authors conduct character-level and word-level attack based on gradients.  The
difference is that we use nearest neighbor search to reconstruct the adversarial
sentences, while they search for adversarial candidates directly in the text
space.  Furthermore, the word-level adversarial texts were not very successfully
in cite:ebrahimi2017-hotflip.  Moreover, in our experiment, we also find that
Hotflip has label leaking problem cite:kurakin2016-adversarial-1 as is the
vanilla FGSM where the true labels are used to generate the adversarial texts.
We fix this problem as suggested in cite:kurakin2016-adversarial-1 by using the
predicted labels instead of the true ones to generate adversarial texts.

** Adversarial Text Framework
:PROPERTIES:
:CUSTOM_ID: h1-adversarial-text-framework-774a3
:END:

In this section, we propose a general framework that generates adversarial texts
with adapted methods for adversarial images.  Our framework focuses on
/replacing/ words.

*** System Overview

Our system consists mainly of three parts, the embedding part, the adversarial
generator, and the reverse embedding part.  The embedding part maps raw input
texts into a continuous space.  The reverse embedding part maps the perturbed
embedding vectors back to texts.

*** Discrete Input Space
:PROPERTIES:
:CUSTOM_ID: h2-discrete-input-space-ed243
:END:

The first problem we need to resolve is how we can accumulate small noise to
change the input.  The idea comes from the observation that the first layer for
most text models is the embedding layer.  Thus, instead of working on the raw
input texts, we first search for adversarials in the embedding space via
gradient-based methods, and then reconstruct the adversarial sentences.
Searching for adversarials in the embedding space is similar in principle to
searching for adversarial images.  However, the generated noisy embedding
vectors usually do not correspond to any tokens in the text space.  To construct
the adversarial texts, we align each embedding to its nearest one via
(approximate) nearest neighbor search.  This reconstructing process can be seen
as a strong /denoising/ process.  With appropriate noise scale, we would expect
most of the words/characters remain unchanged, while only few are replaced.
This framework builds upon the following observations.

1. When generating adversarial samples, the input features (e.g., pixels, words,
   characters) that are relatively more important for the final predictions will
   receive more noise, while others less noise.  This property is intuitively
   illustrated in Figure ref:fig:mnistdemo, where usually only a subset of the
   pixels are perturbed.  Despite that most pixels are perturbed in FGM, only a
   few pixels receive very large noise.
2. The embedded word vectors preserve the subtle semantic relationships among
   words cite:mikolov2013-efficient,mikolov2013-distributed.  For example,
   =vec("clothing")= is closer to =vec("shirt")= as =vec("dish")= to
   =vec("bowl")=, while =vec("clothing")= is farther way from =vec("dish")= or
   =vec("bowl")=, in the sense of \(p\)-norm, since they are not semantically
   close cite:mikolov2013-linguistic.  This property assures that it is more
   likely to replace the victim words with a semantically related one rather
   than a random one.

*** Word Mover's Distance (WMD)
:PROPERTIES:
:CUSTOM_ID: h2-word-movers-distance-wmd-eab60
:END:

For the second problem, we use two metrics to quantify the adversarial texts'
quality, the Word Mover's Distance (WMD) cite:kusner2015-from and the change
ratio (i.e., the number of words changed divided by the maximum sequence
length).  WMD measures the dissimilarity between two text documents as the
minimum amount of distance that the embedded words of one document need to
/travel/ to reach the embedded words of another document.  WMD can be considered
as a special case of Earth Mover's Distance (EMD) cite:rubner2000-earth.
Intuitively, it quantifies the semantic similarity between two text bodies.  A
lower WMD score means a better adversarial samples.  As we will see in our
experiments, WMD is only good as a coarse-level metric.

** Experiment
:PROPERTIES:
:CUSTOM_ID: h1-experiment-2f800
:END:

We evaluate our framework on three text classification problems.
Section [[ref:h2-dataset-ead0c]] details on the data preprocessing.  The adversarial
methods we use in our experiment are (FGM) cite:goodfellow2014-explaining and
DeepFool cite:moosavi-dezfooli2015-deepfool.  We report the model accuracy on
clean sample as well as adversarial texts.

Detailed discussion follow each experiment results.  Only a few examples of
generated adversarial texts are shown in this paper due to the space constraint.
More samples of adversarial texts under different parameter settings and the
code to reproduce the experiment are available online[fn:1].

Computation-wise, the bottleneck in our framework is the nearest neighbor
search.  Word vector spaces, such as GloVe cite:pennington2014-glove, usually
have millions or billions of tokens embedded in very high dimensions.  The
nearest neighbor search is slow.  Instead, we employ the approximate nearest
neighbor (ANN) technique in our experiment.  The ANN implementation which we use
in our experiment is Approximate Nearest Neighbors Oh Yeah (=annoy=)[fn:2],
which is well integrated into =gensim= cite:rek2010-software package.

*** Dataset
:PROPERTIES:
:CUSTOM_ID: h2-dataset-ead0c
:END:

We use three text datasets in our experiments.  The datasets are summarized in
Table ref:tab:datasets.  The last column shows our target model accuracy on
clean test data.

#+ATTR_LaTeX: :booktabs t
#+CAPTION: Dataset Summary
#+NAME: tab:datasets
| Dataset   | Labels | Training | Testing | Seq. Len. | Word Len. | Accuracy |
|-----------+--------+----------+---------+-----------+-----------+----------|
| IMDB      |      2 |    25000 |   25000 |       300 |        20 |   0.8787 |
| Reuters-2 |      2 |     3300 |    1438 |       100 |        20 |   0.9854 |
| Reuters-5 |      5 |     1735 |     585 |       100 |        20 |   0.8701 |

**** IMDB Movie Reviews
:PROPERTIES:
:CUSTOM_ID: h3-imdb-movie-reviews-a4a29
:END:

This is a dataset for binary sentiment classification cite:maas2011-learning.
It contains a set of 25,000 highly polar (positive or negative) movie reviews
for training, and 25,000 for testing.  No special preprocessing is used for this
dataset except that we truncate/pad all the sentences to a fixed maximum length.

**** Reuters
:PROPERTIES:
:CUSTOM_ID: h3-reuters-5b0ea
:END:

This is a dataset of 11,228 newswires from Reuters, labeled over 90 topics.  We
load this dataset through the NLTK cite:bird2009-natural package.  The raw
Reuters dataset is highly unbalanced.  Some categories contain over a thousand
samples, while others may contain only a few.  The problem with such highly
unbalanced data is that the texts that belong to under-populated categories are
almost always get classified incorrectly.  Even though our model may still
achieve high accuracy with 90 labels, it would be meaningless to include these
under-populated categories in the experiment since we are mainly interested in
perturbation of those samples that are already being classified correctly.
Keras[fn:3] uses 46 categories out of 90.  However, the 46 categories are still
highly unbalanced.  In our experiment, we preprocess Reuters and extract two
datasets from it.

***** Reuters-2
:PROPERTIES:
:CUSTOM_ID: h4-reuters-2-6baa5
:END:

It contains two most populous categories, i.e., =acq= and =earn=.  The =acq=
category contains 1650 training samples and 719 test samples.  Over 71%
sentences in the =acq= category have less than 160 tokens.  The =earn= category
contains 2877 training samples and 1087 test samples.  Over 83% sentences in
=earn= category have less then 160 tokens.  In order to balance the two
categories, for =earn=, we use 1650 samples out of 2877 for training, and 719
for testing.  The maximum sentence length of this binary classification dataset
is set to 100.

***** Reuters-5
:PROPERTIES:
:CUSTOM_ID: h4-reuters-5-2388e
:END:

It contains five categories, i.e., =crude=, =grain=, =interest=, =money-fx= and
=trade=.  Similar to Reuters-2, we balance the five categories by using 347
examples (the size of =interest= categories) for each category during training,
and 117 each for testing.  The maximum sentence length is set to 100.

*** Embedding
:PROPERTIES:
:CUSTOM_ID: h2-embedding-ed890
:END:

Our framework relies heavily on the /size/ and /quality/ of the embedding space.
More semantic alternatives would be helpful to improve the quality of generated
adversarial texts.  As a result, we use the GloVe cite:pennington2014-glove
pre-trained embedding in our experiment.  Specifically, we use the largest GloVe
embedding, =glove.840B.300d=, which embeds 840 billion tokens (approximately 2.2
million cased vocabularies) into a vector space of 300 dimensions.  The value
range of the word vectors are roughly \((-5.161, 5.0408)\).

*** Model
:PROPERTIES:
:CUSTOM_ID: h2-model-f41fe
:END:

In this work, we tested two commonly used architectures for sequence
classification problem.  The first one is a word-level convolution
network cite:kim2014-convolutional (as shown in Figure [[ref:fig:wordcnn]]).  This
architecture differs from the image models in two aspects:
#+BEGIN_EXPORT latex
\begin{enumerate*}[label=(\roman*)]
 \item an embedding layer is added right after the input to map the word indices
 to their corresponding vector representations, and
 \item the pooling layers are global max-pooling.
\end{enumerate*}
#+END_EXPORT

#+ATTR_LaTeX: :width .7\linewidth
#+CAPTION: Word-level CNN model for text classification.
#+NAME: fig:wordcnn
[[file:wordcnn.pdf]]

The other one is a character-aware model cite:kim2015-character.  The first
layer is the embedding layer, followed by convolution layers of /different/
filter sizes, which all go through a global max-pooling layer.  The outputs are
concatenated before going through highway layers cite:srivastava2015-training
and LSTMs.  Please refer to cite:kim2015-character for a detailed description.

The detailed parameter settings are available in our released code.  Note that
for models trained on binary classification tasks, DeepFool assumes the output
in the range \([-1, 1]\), instead of \([0, 1]\).  Thus we have two slightly
different models for each of the binary classification task (IMDB and
Reuters-2), the one with =sigmoid= output, and the other with =tanh=.  The
models with =tahn= output are trained with Adam cite:kingma2014-adam by
minimizing the root mean squared error (RMSE), while all the other models are
trained with Adam by minimizing the cross-entropy loss.  Despite the small
difference in architecture, =sigmoid=- and =tanh=-models on the same task have
almost identical accuracy.  As a result, in Table ref:tab:datasets, we report
only one result for IMDB and Reuters-2.  In the following sessions, we refer to
the word-level model as =WordCNN=, the character-level model as =CharLSTM=.
Wherever necessary, the binary classification model with =sigmoid= output is
suffixed with =-sigm=, e.g, =WordCNN-sigm=, the one with =tanh= output is
suffixed with =-tanh=, e.g., =WordCNN-tanh=.

*** Effectiveness and Quality Trade-off
:PROPERTIES:
:CUSTOM_ID: h2-effectiveness-and-quality-trade-off-811e9
:END:

If the model's accuracy on the adversarial texts are lower, then we say the
adversarial texts are more /effective/.  The quality of the adversarial texts
refers to grammar and syntactic correctness of the text piece.  We employ
several intuitive criteria to measure the quality of the adversarial texts,
i.e., the number of words changed (\(N\)) and the Word Mover's Distance (WMD).
The number-of-words measurement makes sense in our settings since our framework
will only replace words.  The trade-off between the effectiveness and quality of
the adversarial texts is controlled by the noise level.  As expected, large
noise level would generate more effective adversarial samples.  However, the
text quality will also degrade with larger noise.

#+BEGIN_EXPORT latex
\begin{figure}[ht]
 \centering
 \begin{minipage}{0.45\linewidth}
  \centering
  \includegraphics[width=\textwidth]{fgsm-acc-wmd.pdf}
 \end{minipage}\hfill
 \begin{minipage}{0.45\linewidth}
  \centering
  \includegraphics[width=\textwidth]{fgsm-acc-n.pdf}
 \end{minipage}
 \caption{\label{fig:wordcnn-fgsm-acc}Word-level model's accuracy with varying
   FGSM noise level.  The WMD and \(N\) (number of words changed) empirically
   show the quality of the adversarial texts.}
\end{figure}
#+END_EXPORT

Figure [[ref:fig:wordcnn-fgsm-acc]] shows the trade-off for FGSM method.  As we can
see, the quality of adversarial texts generated by FGSM deteriorates quickly as
we increase the noise level.  Albeit It becomes more effective toward the target
model.  Especially, the number of words changed grows rapidly.
Figure [[ref:fig:wordcnn-deepfool-acc]] shows the trade-off for DeepFool method.  It
follows a similar trend as FGSM in general.  However, we can see that DeepFool
generates much better adversarial texts then FGSM when they are similar in
effectiveness.  This is similar in the case of adversarial images.  FGSM tends
to add noise to all the dimension of the input, thus with larger noise, we would
expect most words are changed.  On the other hand, DeepFool usually changes only
a small subset of the input dimension.  Even with a larger noise, most words
remain unperturbed.

#+BEGIN_EXPORT latex
\begin{figure}[ht]
 \centering
 \begin{minipage}{0.45\linewidth}
  \centering
  \includegraphics[width=\textwidth]{deepfool-acc-wmd.pdf}
 \end{minipage}\hfill
 \begin{minipage}{0.45\linewidth}
  \centering
  \includegraphics[width=\textwidth]{deepfool-acc-n.pdf}
 \end{minipage}
 \caption{\label{fig:wordcnn-deepfool-acc}Word-level model's accuracy with
   varying DeepFool overshoot value.  The WMD and \(N\) (number of words
   changed) empirically show the quality of the adversarial texts.}
\end{figure}
#+END_EXPORT

The examples of adversarial texts generated via DeepFool at different noise
level are shown in Figure [[ref:fig:wordcnn-deepfool-noise-scale]].  The WMD and
number of words changed are also included to give an intuition about the
correspondence between the measurements and the text quality.

#+ATTR_LaTeX: :width \textwidth
#+CAPTION: Adversarial texts generated with Deepfool with different noise scale on word-level model.
#+NAME: fig:wordcnn-deepfool-noise-scale
[[file:wordcnn-deepfool-noise-level.pdf]]

*** Transferability
:PROPERTIES:
:CUSTOM_ID: h2-transferability-4864c
:END:

We test the transferability of adversarial texts generated on word-level models
and character-level models, respectively.  In our experiments, word-level
adversarial texts show very good transferability, even to character-level
models.  However character-level adversarial texts do not transfer well to
word-level models.

#+BEGIN_EXPORT latex
\begin{figure}[ht]
 \centering
 \begin{subfigure}{0.45\textwidth}
  \centering
  \includegraphics[width=\linewidth]{word-deepfool-transfer.pdf}
  \caption{\label{fig:word-deepfool-transfer}Transferability of adversarial
    texts generated via DeepFool on word-level.  The WordCNN-tanh is the model
    used to generated the adversarial texts.}
 \end{subfigure}
 \hfill
 \begin{subfigure}{0.45\textwidth}
  \centering
  \includegraphics[width=\linewidth]{char-hotflip-transfer.pdf}
  \caption{\label{fig:char-hotflip-transfer}Transferability of adversarial texts
    generated via Hotflip on character-level.  The CharLSTM-sigm is the model
    used to generated the adversarial texts.}
 \end{subfigure}
\end{figure}
#+END_EXPORT

Figure [[ref:fig:word-deepfool-transfer]] shows the transferability of word-level
adversarial texts generated in our framework via DeepFool.  The adversarial
texts are generated on WordCNN-tanh model.  The adversarial texts transfer
better to WordCNN-sigm which shares a similar structure as WordCNN-tanh except
for the output function.  Figure [[ref:fig:char-hotflip-transfer]] shows the
transferability of character-level adversarial texts generated via
Hotflip cite:ebrahimi2017-hotflip.  The character-level adversarial texts only
show transferability to character-level models, but not to word-level models.
The main reason is that the changes to character-level adversarial texts are
mainly within words.  In most cases, the perturbed words will be replaced by
unknown word placeholder (e.g., =<unk>= in our experiments) they are rarely
legit.  Thus the character-level adversarial texts basically degrade to
unknown-word adversarials for word-level models.  As expected, replacing only a
few words with =<unk>= is not enough to fool the word-level model.

*** Defense
:PROPERTIES:
:CUSTOM_ID: h2-defense-6a50e
:END:

#+ATTR_LaTeX: :width .5\textwidth
#+CAPTION: Defense against character-level adversarials
#+NAME: fig:defense-char
[[file:defense-char.pdf]]

The defense for character-level adversarial texts are relatively easy, most of
the errors can be easily corrected by auto-spelling applications, e.g.,
Grammerly[fn:4], Bing Spell Check API.  The incorrect spellings are easy
to detect and recover, e.g., /sontware/ is successfully corrected to
/software/.  However, if the character is replaced by punctuation characters,
the word will be not corrected, e.g., /qu{kly/ is not recognized and correct.

When generating the character-level adversarial texts, we want to change as few
characters as possible so that the resulting adversarial texts do not degrade
into garbage.  However, the fewer characters we change, the easier they are
corrected by auto-spelling applications.

*** COMMENT Results on Word-Level Model
:PROPERTIES:
:CUSTOM_ID: h2-results-on-word-level-model-b3fb0
:END:

#+BEGIN_EXPORT latex
\begin{table}[ht]
 \caption{\label{tab:acc} Word-level CNN accuracy under different parameter
   settings.  \(\epsilon\) is the noise scaling factor.}
 \centering
 \begin{tabular}{rl*{5}{c}}
   \toprule
   Method
   & Dataset
   &
   & \multicolumn{4}{c}{Accuracy} \\
   \midrule

   \multirow{5}{*}{FGSM}
   &
   & \(\epsilon\) & 0.40 & 0.35 & 0.30 & 0.25 \\
   \cmidrule(r){3-7}
   & IMDB      & & 0.1334 & 0.1990 & 0.4074 & 0.6770 \\
   & Reuters-2 & & 0.6495 & 0.7928 & 0.9110 & 0.9680 \\
   & Reuters-5 & & 0.5880 & 0.7162 & 0.7949 & 0.8462 \\
   \cmidrule(lr){1-7}

   \multirow{5}{*}{FGVM}
   &
   & \(\epsilon\) & 15 & 30 & 50 & 100 \\
   \cmidrule(r){3-7}
   & IMDB      & & 0.8538 & 0.8354 & 0.8207 & 0.7964 \\
   & Reuters-2 & & 0.7990 & 0.7538 & 0.7156 & 0.6523 \\
   & Reuters-5 & & 0.7983 & 0.6872 & 0.6085 & 0.5111\\
   \cmidrule(lr){1-7}

   \multirow{5}{*}{DeepFool}
  &
   & \(\epsilon\) & 20 & 30 & 40 & 50 \\
   \cmidrule(r){3-7}
   & IMDB      & & 0.8298 & 0.7225 & 0.6678 & 0.6416 \\
   & Reuters-2 & & 0.6766 & 0.5236 & 0.4910 & 0.4715 \\
   & Reuters-5 & & 0.4034 & 0.2222 & 0.1641 & 0.1402 \\
   \bottomrule
 \end{tabular}
\end{table}
#+END_EXPORT

The noise scale (\(\epsilon\) in Table [[ref:tab:acc]]) influences the effectiveness
of adversarial methods, as well as the the quality of generated adversarial
sentences.  The model accuracy under different noise scales are summarized in
Table ref:tab:acc.

Figure [[ref:fig:wordcnn-deepfool-noise-scale]] shows one example of adversarial
texts generated via DeepFool cite:moosavi-dezfooli2015-deepfool in our framework
with different noise levels.  The \(\epsilon\) in the first column is the noise
level (i.e., the overshoot value in DeepFool algorithm), the second column the
word mover's distance value, the third the number of word(s) changed.  All the
adversarial texts are generated from the same sample, the only difference is the
noise level.  As we could see, as we increase the noise level, more words are
changed as expected.  Furthermore, the WMD value increases as well.
Essentially, the noise level controls the balance between the quality and
effectiveness of the generated adversarial texts.

In the adversarial text examples, the
{{{tex(\colorbox{red!10}{\sout{original}})}}} words their corresponding
{{{tex(\colorbox{green!10}{adversarial})}}} words they are changed into are
highlighted respectively to aid reading.

**** Fast Gradient Sign Method (FGSM)
:PROPERTIES:
:CUSTOM_ID: h3-fast-gradient-method-56aea
:END:

#+ATTR_LaTeX: :width \textwidth
#+CAPTION: IMDB adversarial texts generated via FGSM on word-level model, \(\epsilon=0.08\).
#+NAME: fig:wordcnn-fgsm
[[file:wordcnn-fgsm-eps08.pdf]]

**** Fast Gradient Value Method (FGVM)
:PROPERTIES:
:CUSTOM_ID: h3-fast-gradient-value-method-fgvm-d99a9
:END:

#+ATTR_LaTeX: :width \textwidth
#+CAPTION: IMDB adversarial texts generated via FGVM on word-level model with varying \(\epsilon\).
#+NAME: fig:wordcnn-fgvm
[[file:wordcnn-fgvm.pdf]]

We first evaluate two versions of FGM, i.e., FGSM and FGVM.  Their example
results are shown in Figure [[ref:fig:wordcnn-fgsm]] and
Figure [[ref:fig:wordcnn-fgvm]], respectively.  With appropriate noise level, we can
change only a few words to alter the label of the whole text piece.

*** Results on Character-level Model
:PROPERTIES:
:CUSTOM_ID: h2-results-on-character-level-model-da172
:END:

#+ATTR_LaTeX: :width \textwidth
#+CAPTION: Adversarial texts generated with Deepfool with different noise scale on character-level model.  Both adversarial samples are generated from the same clean sample.  The second adversarial sample is generated by adding a very large noise.
#+NAME: fig:charlstm-noise-scale
[[file:charlstm-deepfool-noise-level.pdf]]

The results for character-level adversarials are more interesting.  One example
is shown in [[ref:fig:charlstm-noise-scale]].  For relatively small noise, we
observe similar adversarial texts, i.e., only a few characters are changed while
the whole sentence is still legit albeit its label is already different from the
clean sample.  On the other hand, if we tune up the noise level, the whole
sentence is changed to another somewhat legit sentence, which is rather
surprising.  With a large noise, we would expect that the whole sentence turn
into garbage, e.g., cite:liang2017-deep, albeit the resulting sentence does
change to a different category.  Our hypothesis is that the architecture of the
character-level model plays an important role.

1. The different width of feature maps encode different levels of contextual
   information around each character.
2. The highway and LSTM layer creates mingles the contextual information
   together so that the noise follows a certain direction.

*** COMMENT Discussion
:PROPERTIES:
:CUSTOM_ID: h2-discussion-45d4e
:END:

In contrary to the experiment in cite:liang2017-deep, our framework generates
much better adversarial texts with gradient methods.  One main reason is that
the embedding space preserves semantic relations among tokens.

Based on the generated text samples, DeepFool generates the adversarial texts
with the highest quality.  Our experiment confirms that the DeepFool's strategy
to search for the optimal direction is still effective in text models.  On the
other hand, the strong denoising process will help to smooth unimportant noise.
FGVM is slightly better than FGSM, which is quite similar to what we saw in
Figure ref:fig:mnistdemo.  By using \(\sign\nabla L\), FGSM applies the same
amount of noise to every feature it finds to be important, which ignores the
fact that some features are more important than others.  Since FGVM does not
follow the optimal direction as DeepFool does, it usually needs larger
perturbation.  In other words, compared to DeepFool, FGVM may change more words
in practice.

** Conclusion
:PROPERTIES:
:CUSTOM_ID: h1-conclusion-2763d
:END:

In this work, we propose a general framework to generate adversarial texts.  In
this framework, instead of constructing adversarials directly in the raw text
space, we first search for potential adversarial embeddings in the embedding
space, and then reconstruct the adversarial texts via nearest neighbor search.
We demonstrate the effectiveness of our method on three texts benchmark
problems.  In all experiments, our framework can successfully generate
adversarial samples with only a few words/characters changed.  In addition, we
also empirically demonstrate Word Mover's Distance (WMD) as a viable quality
measurement for adversarial texts.  Besides, we also demonstrate that the
character-level is not easily recovered by

Despite excellent performance on stationary test sets, deep neural networks
(DNNs) can fail to generalize to out-of-distribution (OoD) inputs, including
natural, non-adversarial ones, which are common in real-world settings. In this
paper, we present a framework for discovering DNN failures that harnesses 3D
renderers and 3D models. That is, we estimate the parameters of a 3D renderer
that cause a target DNN to misbehave in response to the rendered image. Using
our framework and a self-assembled dataset of 3D objects, we investigate the
vulnerability of DNNs to OoD poses of well-known objects in ImageNet. For
objects that are readily recognized by DNNs in their canonical poses, DNNs
incorrectly classify 97\% of their pose space. In addition, DNNs are highly
sensitive to slight pose perturbations. Importantly, adversarial poses transfer
across models and datasets. We find that 99.9\% and 99.4\% of the poses
misclassified by Inception-v3 also transfer to the AlexNet and ResNet-50 image
classifiers trained on the same ImageNet dataset, respectively, and 75.5\%
transfer to the YOLOv3 object detector trained on MS COCO.

* Defending Adversarial Samples
:PROPERTIES:
:CUSTOM_ID: part:defend-adversarial-samples
:END:

# Part 2 Defend Adversarial Samples

Deep neural networks have been successfully adopted to many life critical areas,
e.g., skin cancer detection cite:esteva2017-dermatologist, auto-driving
cite:santana2016-learning, traffic sign classification cite:ciresan2012-multi,
etc.  A recent study cite:szegedy2013-intriguing, however, discovered that deep
neural networks are susceptible to adversarial images.  Figure
ref:fig:adv-example shows an example of adversarial images generated via fast
gradient sign method cite:kurakin2016-adversarial,kurakin2016-adversarial-1 on
MNIST.  As we can see that although the adversarial and original clean images
are almost identical from the perspective of human beings, the deep neural
network will produce wrong predictions with very high confidence.  Similar
techniques can easily fool the image system into mistaking a stop sign for a
yield sign, a dog for a automobile, for example.  When leveraged by malicious
users, these adversarial images pose a great threat to the deep neural network
systems.

#+CAPTION: The adversarial images (second row) are generated from the first row via iterative FGSM.  The label of each image is shown below with prediction probability in parenthesis.  Our model achieves less then 1% error rate on the clean data.
#+NAME: fig:adv-example
[[file:ex_adv_mnist.pdf]]

Although adversarial and clean images appear visually indiscernible, their
subtle differences can successfully fool the deep neural networks.  This means
that deep neural networks are sensitive to these subtle differences.  So an
intuitively question to ask is: can we leverage these subtle differences to
distinguish between adversarial and clean images?  Our experiment suggests the
answer is positive.  In this paper we demonstrate that a simple binary
classifier can separate the adversarial from the original clean images with very
high accuracy (over 99%).  However, we also show that the binary classifier
approach suffers from the generalization limitation, i.e., it is sensitive 1) to
a hyper-parameter used in crafting adversarial dataset, and 2) to different
adversarial crafting algorithms.  In addition to that, we also discovered that
this limitation is also shared among other proposed methods against adversarial
attacking, e.g., defensive training
cite:huang2015-learning,kurakin2016-adversarial-1, knowledge distillation
cite:papernot2015-distillation, etc.  We empirically investigate the limitation
and propose the hypothesis that the adversarial and original dataset are, in
effect, two completely /different/ datasets, despite being visually similar.

Our key findings are summarized as follows:
1. Adversarial and clean images look drastically different from the perspective
   of the model, e.g., a neural network.
2. For FGSM/TGSM,

This article is organized as follows.  In Section ref:sec:related-work, we give
an overview of the current research in adversarial attack and defense, with a
focus on deep neural networks.  Then, it is followed by a brief summary of the
state-of-the-art adversarial crafting algorithms in Section
ref:sec:crafting-adversarials.  Section ref:sec:experiment presents our
experiment results and detailed discussions.  And we conclude in Section
ref:sec:conclusion.

** Related Work
:PROPERTIES:
:CUSTOM_ID: sec:related-work
:END:

The adversarial image attack on deep neural networks was first investigated in
cite:szegedy2013-intriguing.  The authors discovered that when added some
imperceptible carefully chosen noise, an image may be wrongly classified with
high confidence by a well-trained deep neural network.  They also proposed an
adversarial crafting algorithm based on optimization.  We will briefly summarize
it in section ref:sec:crafting-adversarials.  They also proposed the hypothesis
that the adversarial samples exist as a result of the high nonlinearity of deep
neural network models.

However, cite:goodfellow2014-explaining proposed a counter-intuitive hypothesis
explaining the cause of adversarial samples.  They argued that adversarial
samples are caused by the models being too /linear/, rather than /nonlinear/.
They proposed two adversarial crafting algorithms based on this hypothesis,
i.e., fast gradient sign method (FGSM) and least-likely class method (LLCM)
cite:goodfellow2014-explaining.  The least-likely class method is later
generalized to target class gradient sign method (TGSM) in
cite:kurakin2016-adversarial.

cite:papernot2015-limitations proposed another gradient based adversarial
algorithm, the Jacobian-based saliency map approach (JSMA) which can
successfully alter the label of an image to any desired category.

The adversarial images have been shown to be transferable among deep neural
networks cite:szegedy2013-intriguing,kurakin2016-adversarial.  This poses a
great threat to current learning systems in that the attacker needs not the
knowledge of the target system.  Instead, the attacker can train a different
model to create adversarial samples which are still effective for the target
deep neural networks.  What's worse, cite:papernot2016-transferability has shown
that adversarial samples are even transferable among different machine learning
techniques, e.g., deep neural networks, support vector machine, decision tree,
logistic regression, etc.

Small steps have been made towards the defense of adversarial images.
cite:kurakin2016-adversarial shows that some image transformations, e.g.,
Gaussian noise, Gaussian filter, JPEG compression, etc., can effectively recover
over 80% of the adversarial images.  However, in our experiment, the image
transformation defense does not perform well on images with low resolution,
e.g., MNIST.  Knowledge distillation is also shown to be an effective method
against most adversarial images cite:papernot2015-distillation.  The
restrictions of defensive knowledge distillation are 1) that it only applies to
models that produce categorical probabilities, and 2) that it needs model
training.  Adversarial training
cite:kurakin2016-adversarial-1,huang2015-learning was also shown to greatly
enhance the model robustness to adversarials.  However, as discussed in Section
ref:subsec:generalization-limitation, defensive distillation and adversarial
training suffers from, what we call, the generalization limitations.  Our
experiment suggests this seems to be an intrinsic property of adversarial
datasets.

** Crafting Adversarials
:PROPERTIES:
:CUSTOM_ID: sec:crafting-adversarials
:END:

The are mainly two categories of algorithms to generate adversarial samples,
model independent and model dependent.  We briefly summarize these two classes
of methods in this section.

By conventions, we use \(X\) to represent input image set (usually a 3-dimension
tensor), and \(Y\) to represent the label set, usually one-hot encoded.
Lowercase represents an individual data sample, e.g., \(x\) for one input image.
Subscript to data samples denotes one of its elements, e.g., \(x_i\) denotes one
pixel in the image, \(y_i\) denotes probability for the \(i\)-th target class.
\(f\) denotes the model, \(\theta\) the model parameter, \(J\) the loss
function.  We use the superscript /adv/ to denote adversarial related variables,
e.g., \(x^{adv}\) for one adversarial image.  \(\delta x\) denotes the
adversarial noise for one image, i.e., \(x^{adv} = x + \delta x\).  For clarity,
we also include the model used to craft the adversarial samples where necessary,
e.g., \(x^{adv(f_1)}\) denotes the adversarial samples created with model
\(f_1\).  \(\mathbb{D}\) denotes the image value domain, usually \([0, 1]\) or
\([0, 255]\).  And \(\epsilon\) is a scalar controlling the scale of the
adversarial noise, another hyper-parameter to choose.

*** Model Independent Method

A box-constrained minimization algorithm based on L-BFGS was the first algorithm
proposed to generate adversarial data cite:szegedy2013-intriguing.  Concretely
we want to find the smallest (in the sense of \(L^2\)-norm) noise \(\delta x\)
such that the adversarial image belongs to a different category, i.e.,
\(f(x^{adv})\neq f(x)\).
#+BEGIN_EXPORT latex
\begin{equation} \label{eq:guided-walk}
  \begin{split}
    \delta x &= \argmin_r c\norm{r}_\infty + J(x+r, y^{adv})\\
    &\text{ s.t. } x+r\in \mathbb{D}
  \end{split}
\end{equation}
#+END_EXPORT

*** Model Dependent Methods

There are mainly three methods that rely on model gradient, i.e., fast gradient
sign method (FGSM) cite:kurakin2016-adversarial, target class method
cite:kurakin2016-adversarial,kurakin2016-adversarial-1 (TGSM) and Jacobian-based
saliency map approach (JSMA) cite:papernot2015-limitations.  We will see in
Section ref:sec:experiment that despite that they all produce highly disguising
adversarials, FGSM and TGSM produce /compatible/ adversarial datasets which are
complete /different/ from adversarials generated via JSMA.

**** Fast Gradient Sign Method (FGSM)

FGSM tries to modify the input towards the direction where \(J\) increases,
i.e., \(\dv*{J(x, y^{adv})}{x}\), as shown in Equation ref:eq:fgsm.
#+BEGIN_EXPORT latex
\begin{equation} \label{eq:fgsm}
  \delta x = \epsilon\sign\left(\dv{J(x, \pred{y})}{x}\right)
\end{equation}
#+END_EXPORT

Originally cite:kurakin2016-adversarial proposes to generate adversarial samples
by using the true label i.e., \(y^{adv} = y^{true}\), which has been shown to
suffer from the label leaking problem cite:kurakin2016-adversarial-1.  Instead
of true labels, cite:kurakin2016-adversarial-1 proposes to use the /predicted/
label, i.e., \(\pred{y} = f(x)\), to generate adversarial examples.

This method can also be used iteratively as shown in Equation ref:eq:fgsm-iter.
Iterative FGSM has much higher success rate than the one-step FGSM.  However,
the iterative version is less robust to image transformation
cite:kurakin2016-adversarial.
#+BEGIN_EXPORT latex
\begin{equation} \label{eq:fgsm-iter}
  \begin{split}
    x^{adv}_{k+1} &= x^{adv}_k + \epsilon\sign\left(\dv{J(x^{adv}_k, \pred{y_k})}{x}\right)\\
    x^{adv}_0 &= x\\
    \pred{y_k} &= f(x^{adv}_k)
  \end{split}
\end{equation}
#+END_EXPORT

**** Target Class Gradient Sign Method (TGSM)

This method tries to modify the input towards the direction where
\(p(y^{adv}\given x)\) increases.
#+BEGIN_EXPORT latex
\begin{equation} \label{eq:tcm}
    \delta x = -\epsilon\sign\left(\dv{J(x, y^{adv})}{x}\right)
\end{equation}
#+END_EXPORT

Originally this method was proposed as the least-likely class method
cite:kurakin2016-adversarial where \(y^{adv}\) was chosen as the least-likely
class predicted by the model as shown in Equation ref:eq:llcm-y.
#+BEGIN_EXPORT latex
\begin{equation} \label{eq:llcm-y}
  y^{adv} = \text{OneHotEncode}\left(\argmin f(x)\right)
\end{equation}
#+END_EXPORT

And it was extended to a more general case where \(y^{adv}\) could be any
desired target class cite:kurakin2016-adversarial-1.

# The following table belongs to the "Efficiency and Robustness of the
# Classifier" section, place here only for typesetting.

#+BEGIN_EXPORT latex
\begin{table*}[htbp]
  \caption{\label{tbl:accuracy-summary}
    Accuracy on adversarial samples generated with FGSM/TGSM.}
  \centering
  \begin{tabular}{lcrrcrrrr}
    \toprule
    & \phantom{a} & \multicolumn{2}{c}{\(f_1\)} & \phantom{a} & \multicolumn{4}{c}{\(f_2\)} \\
    \cmidrule{3-4} \cmidrule{6-9}
    Dataset && \(X_{test}\) & \(X^{adv(f_1)}_{test}\) && \(X_{test}\) & \(X^{adv(f_1)}_{test}\) & \(\{X_{test}\}^{adv(f_2)}\) & \(\{X^{adv(f_1)}_{test}\}^{adv(f_2)}\) \\
    \midrule
    MNIST && 0.9914 & 0.0213 && 1.00 & 1.00 & 0.00 & 1.00\\
    CIFAR10 && 0.8279 & 0.1500 && 0.99 & 1.00 & 0.01 & 1.00\\
    SVHN && 0.9378 & 0.2453 && 1.00 & 1.00 & 0.00 & 1.00\\
    \bottomrule
  \end{tabular}
\end{table*}

#+END_EXPORT

#+CAPTION: Accuracy on adversarial samples generated with FGSM/TGSM.
#+NAME: tbl:accuracy-summary
#+ATTR_LaTeX: :booktabs true :align l|rr|rrrr :float multicolumn
|         |      \(f_1\) |                         |              |                         |                     \(f_2\) |                                        |
|---------+--------------+-------------------------+--------------+-------------------------+-----------------------------+----------------------------------------|
| Dataset | \(X_{test}\) | \(X^{adv(f_1)}_{test}\) | \(X_{test}\) | \(X^{adv(f_1)}_{test}\) | \(\{X_{test}\}^{adv(f_2)}\) | \(\{X^{adv(f_1)}_{test}\}^{adv(f_2)}\) |
|---------+--------------+-------------------------+--------------+-------------------------+-----------------------------+----------------------------------------|
| MNIST   |       0.9914 |                  0.0213 |         1.00 |                    1.00 |                        0.00 |                                   1.00 |
| CIFAR10 |       0.8279 |                  0.1500 |         0.99 |                    1.00 |                        0.01 |                                   1.00 |
| SVHN    |       0.9378 |                  0.2453 |         1.00 |                    1.00 |                        0.00 |                                   1.00 |

**** Jacobian-based Saliency Map Approach (JSMA)

Similar to the target class method, JSMA cite:papernot2015-limitations allows to
specify the desired target class.  However, instead of adding noise to the whole
input, JSMA changes only one pixel at a time.  A /saliency score/ is calculated
for each pixel and pixel with the highest score is chosen to be perturbed.
#+BEGIN_EXPORT latex
\begin{equation} \label{eq:jsma-saliency}
  \begin{split}
    s(x_i) &= \begin{cases}
      0 & \text{ if } s_t < 0 \text{ or } s_o > 0\\
      s_t\abs{s_o} & \text{ otherwise}
    \end{cases}\\
    s_t &= \pdv{y_t}{x_i}\qquad s_o = \sum_{j\neq t}\pdv{y_j}{x_i}
  \end{split}
\end{equation}
#+END_EXPORT

Concretely, \(s_t\) is the Jacobian value of the desired target class \(y_t\)
w.r.t an individual pixel, \(s_o\) is the sum of Jacobian values of all
non-target class.  Intuitively, saliency score indicates the sensitivity of each
output class w.r.t each individual pixel.  And we want to perturb the pixel
towards the direction where \(p(y_t\given x)\) increases the most.

** Experiment
:PROPERTIES:
:CUSTOM_ID: sec:experiment
:END:

Generally, we follow the steps below to test the effectiveness and limitation of
the binary classifier approach.

1. Train a deep neural network \(f_1\) on the original clean training data
   \(X_{train}\), and craft adversarial dataset from the original clean data,
   \(X_{train}\to X^{adv(f_1)}_{train}\), \(X_{test}\to X^{adv(f_1)}_{test}\).
   \(f_1\) is used to generate the attacking adversarial dataset which we want
   to filter out.
2. Train a binary classifier \(f_2\) on the combined (shuffled) training data
   \(\{X_{train}, X^{adv(f_1)}_{train}\}\), where \(X_{train}\) is labeled 0 and
   \(X^{adv(f_1)}_{train}\) labeled 1.
3. Test the accuracy of \(f_2\) on \(X_{test}\) and \(X^{adv(f_1)}_{test}\),
   respectively.
4. Construct second-round adversarial test data, \(\{X_{test},
   X^{adv(f_1)}_{test}\}\to \{X_{test}, X^{adv(f_1)}_{test}\}^{adv(f_2)}\) and
   test \(f_2\) accuracy on this new adversarial dataset.  Concretely, we want
   to test whether we could find adversarial samples 1) that can successfully
   bypass the binary classifier \(f_2\), and 2) that can still fool the target
   model \(f_1\) if they bypass the binary classifier.  Since adversarial
   datasets are shown to be transferable among different machine learning
   techniques cite:papernot2016-transferability, the binary classifier approach
   will be seriously flawed if \(f_2\) failed this second-round attacking test.

The code to reproduce our experiment are available
https://github.com/gongzhitaao/adversarial-classifier.

*** Efficiency and Robustness of the Classifier

We evaluate the binary classifier approach on MNIST, CIFAR10, and SVHN datasets.
Of all the datasets, the binary classifier achieved accuracy over 99% and was
shown to be robust to a second-round adversarial attack.  The results are
summarized in Table ref:tbl:accuracy-summary.  Each column denotes the model
accuracy on the corresponding dataset.  The direct conclusions from Table
ref:tbl:accuracy-summary are summarized as follows.
1. Accuracy on \(X_{test}\) and \(X^{adv(f_1)}_{test}\) suggests that the binary
   classifier is very effective at separating adversarial from clean dataset.
   Actually during our experiment, the accuracy on \(X_{test}\) is always near
   1, while the accuracy on \(X^{adv(f_1)}_{test}\) is either near 1
   (successful) or near 0 (unsuccessful).  Which means that the classifier
   either successfully detects the subtle difference completely or fails
   completely.  We did not observe any values in between.
3. Accuracy on \(\{X^{adv(f_1)}_{test}\}^{adv(f_2)}\) suggests that we were not
   successful in disguising adversarial samples to bypass the the classifier.
   In other words, the binary classifier approach is robust to a second-round
   adversarial attack.
4. Accuracy on \(\{X_{test}\}^{adv(f_2)}\) suggests that in case of the
   second-round attack, the binary classifier has very high false negative.  In
   other words, it tends to recognize them all as adversarials.  This, does not
   pose a problem in our opinion.  Since our main focus is to block adversarial
   samples.

*** Generalization Limitation
:PROPERTIES:
:CUSTOM_ID: subsec:generalization-limitation
:END:

Before we conclude too optimistic about the binary classifier approach
performance, however, we discover that it suffers from the /generalization
limitation/.
1. When trained to recognize adversarial dataset generated via FGSM/TGSM, the
   binary classifier is sensitive to the hyper-parameter \(\epsilon\).
2. The binary classifier is also sensitive to the adversarial crafting
   algorithm.

In out experiment, the aforementioned limitations also apply to adversarial
training cite:kurakin2016-adversarial-1,huang2015-learning and defensive
distillation cite:papernot2015-distillation.

**** Sensitivity to \(\epsilon\)

Table ref:tbl:eps-sensitivity-cifar10 summarizes our tests on CIFAR10.  For
brevity, we use \(\eval{f_2}_{\epsilon=\epsilon_0}\) to denote that the
classifier \(f_2\) is trained on adversarial data generated on \(f_1\) with
\(\epsilon=\epsilon_0\).  The binary classifier is trained on mixed clean data
and adversarial dataset which is generated via FGSM with \(\epsilon=0.03\).
Then we re-generate adversarial dataset via FGSM/TGSM with different
\(\epsilon\) values.

#+BEGIN_EXPORT latex
\begin{table}[htbp]
  \caption{\label{tbl:eps-sensitivity-cifar10}
    \(\epsilon\) sensitivity on CIFAR10}
  \centering
  \begin{tabular}{lcll}
    \toprule
    & \phantom{a} & \multicolumn{2}{c}{\(\eval{f_2}_{\epsilon=0.03}\)} \\
    \cmidrule{3-4}
    \(\epsilon\) && \(X_{test}\) & \(X^{adv(f_1)}_{test}\)\\
    \midrule
    0.3 && 0.9996 & 1.0000\\
    0.1 && 0.9996 & 1.0000\\
    0.03 && 0.9996 & 0.9997\\
    0.01 && 0.9996 & \textbf{0.0030}\\
    \bottomrule
  \end{tabular}
\end{table}
#+END_EXPORT

As shown in Table ref:tbl:eps-sensitivity-cifar10,
\(\eval{f_2}_{\epsilon=\epsilon_0}\) can correctly filter out adversarial
dataset generated with \(\epsilon\geq\epsilon_0\), but fails when adversarial
data are generated with \(\epsilon<\epsilon_1\).  Results on MNIST and SVHN are
similar.  This phenomenon was also observed in defensive training
cite:kurakin2016-adversarial-1.  To overcome this issue, they proposed to use
mixed \(\epsilon\) values to generate the adversarial datasets.  However, Table
ref:tbl:eps-sensitivity-cifar10 suggests that adversarial datasets generated
with smaller \(\epsilon\) are /superset/ of those generated with larger
\(\epsilon\).  This hypothesis could be well explained by the linearity
hypothesis cite:kurakin2016-adversarial,warde-farley2016-adversarial.  The same
conclusion also applies to adversarial training.  In our experiment, the results
of defensive training are similar to the binary classifier approach.

**** Disparity among Adversarial Samples

# #+ATTR_LaTeX: :float multicolumn
# #+CAPTION: Adversarial training \cite{huang2015-learning,kurakin2016-adversarial-1} does not work.  This is a church window plot \cite{warde-farley2016-adversarial}.  Each pixel \((i, j)\) (row index and column index pair) represents a data point \(\tilde{x}\) in the input space and \(\tilde{x} = x + \vb{h}\epsilon_j + \vb{v}\epsilon_i\), where \(\vb{h}\) is the direction computed by FGSM and \(\vb{v}\) is a random direction orthogonal to \(\vb{h}\).  The \(\epsilon\) ranges from \([-0.5, 0.5]\) and \(\epsilon_{(\cdot)}\) is the interpolated value in between.  The central black dot \tikz[baseline=-0.5ex]{\draw[fill=black] (0,0) circle (0.3ex)} represents the original data point \(x\), the orange dot (on the right of the center dot) \tikz[baseline=-0.5ex]{\draw[fill=orange,draw=none] (0,0) circle (0.3ex)} represents the last adversarial sample created from \(x\) via FGSM that is used in the adversarial training and the blue dot \tikz[baseline=-0.5ex]{\draw[fill=blue,draw=none] (0,0) circle (0.3ex)} represents a random adversarial sample created from \(x\) that cannot be recognized with adversarial training. The three digits below each image, from left to right, are the data samples that correspond to the black dot, orange dot and blue dot, respectively.  \tikz[baseline=0.5ex]{\draw (0,0) rectangle (2.5ex,2.5ex)} ( \tikz[baseline=0.5ex]{\draw[fill=black,opacity=0.1] (0,0) rectangle (2.5ex,2.5ex)} ) represents the data samples that are always correctly (incorrectly) recognized by the model.  \tikz[baseline=0.5ex]{\draw[fill=red,opacity=0.1] (0,0) rectangle (2.5ex,2.5ex)} represents the adversarial samples that can be correctly recognized without adversarial training only.  And \tikz[baseline=0.5ex]{\draw[fill=green,opacity=0.1] (0,0) rectangle (2.5ex,2.5ex)} represents the data points that were correctly recognized with adversarial training only, i.e., the side effect of adversarial training.
# #+name: fig:adv-training-not-working
# [[file:adv-training-not-working.pdf]]

In our experiment, we also discovered that the binary classifier is also
sensitive to the algorithms used to generate the adversarial datasets.

Specifically, the binary classifier trained on FGSM adversarial dataset achieves
good accuracy (over 99%) on FGSM adversarial dataset, but not on adversarial
generated via JSMA, and vise versa.  However, when binary classifier is trained
on a mixed adversarial dataset from FGSM and JSMA, it performs well (with
accuracy over 99%) on both datasets.  This suggests that FGSM and JSMA generate
adversarial datasets that are /far away/ from each other.  It is too vague
without defining precisely what is /being far away/.  In our opinion, they are
/far away/ in the same way that CIFAR10 is /far away/ from SVHN.  A well-trained
model on CIFAR10 will perform poorly on SVHN, and vise versa.  However, a
well-trained model on the the mixed dataset of CIFAR10 and SVHN will perform
just as well, if not better, on both datasets, as if it is trained solely on one
dataset.

The adversarial datasets generated via FGSM and TGSM are, however, /compatible/
with each other.  In other words, the classifier trained on one adversarial
datasets performs well on adversarials from the other algorithm.  They are
compatible in the same way that training set and test set are compatible.
Usually we expect a model, when properly trained, should generalize well to the
unseen data from the same distribution, e.g., the test dataset.

In effect, it is not just FGSM and JSMA are incompatible.  We can generate
adversarial data samples by a linear combination of the direction computed by
FGSM and another random orthogonal direction, as illustrated in a church plot
cite:warde-farley2016-adversarial Figure ref:fig:adv-training-not-working.
Figure ref:fig:adv-training-not-working visually shows the effect of adversarial
training cite:kurakin2016-adversarial-1.  Each image represents adversarial
samples generated from /one/ data sample, which is represented as a black dot in
the center of each image, the last adversarial sample used in adversarial
training is represented as an orange dot (on the right of black dot, i.e., in
the direction computed by FGSM).  The green area represents the adversarial
samples that cannot be correctly recognized without adversarial training but can
be correctly recognized with adversarial training.  The red area represents data
samples that can be correctly recognized without adversarial training but cannot
be correctly recognized with adversarial training.  In other words, it
represents the side effect of adversarial training, i.e., slightly reducing the
model accuracy.  The white (gray) area represents the data samples that are
always correctly (incorrectly) recognized with or without adversarial training.

As we can see from Figure ref:fig:adv-training-not-working, adversarial training
does make the model more robust against the adversarial sample (and adversarial
samples around it to some extent) used for training (green area).  However, it
does not rule out all adversarials.  There are still adversarial samples (gray
area) that are not affected by the adversarial training.  Further more, we could
observe that the green area largely distributes along the horizontal direction,
i.e., the FGSM direction.  In cite:nguyen2014-deep, they observed similar
results for fooling images.  In their experiment, adversarial training with
fooling images, deep neural network models are more robust against a limited set
of fooling images.  However they can still be fooled by other fooling images
easily.

** Conclusion
:PROPERTIES:
:CUSTOM_ID: sec:conclusion
:END:

We show in this paper that the binary classifier is a simple yet effective and
robust way to separating adversarial from the original clean images.  Its
advantage over defensive training and distillation is that it serves as a
preprocessing step without assumptions about the model it protects.  Besides, it
can be readily deployed without any modification of the underlying systems.
However, as we empirically showed in the experiment, the binary classifier
approach, defensive training and distillation all suffer from the generalization
limitation.  For future work, we plan to extend our current work in two
directions.  First, we want to investigate the disparity between different
adversarial crafting methods and its effect on the generated adversarial space.
Second, we will also carefully examine the cause of adversarial samples since
intuitively the linear hypothesis does not seem right to us.

* COMMENT Draft

** Introduction

For real-world technologies, such as self-driving cars cite:chen2015deepdriving,
autonomous drones cite:gandhi2017learning, and search-and-rescue robots
cite:sampedro2018fully, the test distribution may be non-stationary, and new
observations will often be out-of-distribution (OoD), i.e., not from the
training distribution cite:sugiyama2017dataset.

However, machine learning (ML) models frequently assign wrong labels with high
confidence to OoD examples, such as adversarial examples
cite:szegedy2013intriguing,nguyen2015deep, inputs specially crafted by an
adversary to cause a target model to misbehave.  But ML models are also
vulnerable to /natural/ OoD examples
cite:lambert2016understanding,uber2017killed,tian2017deeptest,tesla2016killed.
For example, when a Tesla autopilot car failed to recognize a white truck
against a bright-lit sky, an unusual view that might be OoD, it crashed into the
truck, killing the driver cite:tesla2016killed.

#+ATTR_LaTeX: :width .6\textwidth
#+CAPTION: The Google Inception-v3 classifier \cite{szegedy2016rethinking} correctly labels the canonical poses of objects (a), but fails to recognize out-of-distribution images of objects in unusual poses (b-d), including real photographs retrieved from the Internet (d).  The left 3x3 images (a-c) are found by our framework and rendered via a 3D renderer.  Below each image are its top-1 predicted label and confidence score.
#+NAME: fig:teaser
file:imgdemo.pdf

To understand such natural Type II classification errors, we searched for 6D
poses (i.e., 3D translations and 3D rotations) of 3D objects that caused DNNs to
misclassify.

Our results reveal that state-of-the-art image classifiers and object detectors
trained on large-scale image datasets
cite:russakovsky2015imagenet,lin2014microsoft misclassify most poses for many
familiar training-set objects.  For example, DNNs predict the front view of a
school bus, an object in the ImageNet dataset cite:russakovsky2015imagenet,
extremely well (Figure ref:fig:teaser (a)) but fail to recognize the same object
when it is too close or flipped over, i.e., in poses that are OoD yet exist in
the real world (Figure ref:fig:teaser (d)).

Addressing this type of OoD error is a non-trivial challenge.  First, objects on
roads may appear in an infinite variety of poses
cite:tesla2016killed,uber2017killed.  Second, these OoD poses come from known
objects and should be assigned known labels rather than being rejected as
unknown objects cite:hendrycks2016baseline,scheirer2013toward.  Moreover, a
self-driving car needs to correctly estimate at least some attributes of an
incoming, unknown object (instead of simply rejecting it) to handle the
situation gracefully and minimize damage.

In this paper, we propose a framework for finding OoD errors in computer vision
models in which iterative optimization in the parameter space of a 3D renderer
is used to estimate changes (e.g., in object geometry and appearance, lighting,
background, or camera settings) that cause a target DNN to misbehave
(Figure ref:fig:concept).  With our framework, we generated unrestricted 6D
poses of 3D objects and studied how DNNs respond to 3D translations and 3D
rotations of objects.  For our study, we built a dataset of 3D objects
corresponding to 30 ImageNet classes relevant to the self-driving car
application.  All code and data for our experiments will be available at
https://github.com/airalcorn2/strike-with-a-pose.  In addition, we will release
a simple GUI tool that allows users to generate their own adversarial poses of
an object.  Our main findings are:

1. ImageNet classifiers only correctly label \(3.09%\) of the entire 6D pose
   space of a 3D object, and misclassify many generated adversarial examples
   (AXs) that are human-recognizable (Figure ref:fig:teaser (b-c)).  A
   misclassification can be found via a change as small as \(10.31\degree\),
   \(8.02\degree\), and \(9.17\degree\) to the yaw, pitch, and roll,
   respectively.
2. 99.9% and 99.4% of AXs generated against Inception-v3 transfer to the
   AlexNet and ResNet-50 image classifiers, respectively, and 75.5% transfer to
   the YOLOv3 object detector.
3. Training on adversarial poses generated by the 30 objects (in addition to the
   original ImageNet data) did not help DNNs generalize well to held-out objects
   in the same class.

In sum, our work shows that state-of-the-art DNNs perform /image classification/
well but are still far from true /object recognition/.  While it might be possible
to improve DNN robustness through adversarial training with many more 3D
objects, we hypothesize that future ML models capable of visual reasoning may
instead benefit from strong 3D geometry priors.

#+CAPTION: To test a target DNN, we build a 3D scene (a) that consists of 3D objects (here, a school bus and a pedestrian), lighting, a background scene, and camera parameters. Our 3D renderer renders the scene into a 2D image, which the image classifier labels \class{school bus}. We can estimate the pose changes of the school bus that cause the classifier to misclassify by (1) approximating gradients via finite differences; or (2) back-propagating (\textcolor{red_dashed_line}{red} dashed line) through a differentiable renderer.
#+NAME: fig:concept
file:concept.pdf

** Framework

*** Problem formulation

Let \(f\) be an image classifier that maps an image \(\x \in \R^{H\times W\times
C}\) onto a softmax probability distribution over 1,000 output classes
cite:szegedy2016rethinking.

Let \(R\) be a 3D renderer that takes as input a set of parameters \(\phi\) and
outputs a render, i.e., a 2D image \(R(\phi) \in \R^{H\times W\times C}\) (see
Figure ref:fig:concept).  Typically, \(\phi\) is factored into mesh vertices
\(V\), texture images \(T\), a background image \(B\), camera parameters \(C\),
and lighting parameters \(L\), i.e., \(\phi = \{V, T, B, C, L\}\)
cite:kato2018neural.  To change the 6D pose of a given 3D object, we apply a set
of 3D rotations and 3D translations, parameterized by \(\w \in \R^6\), to the
original vertices \(V\), yielding a new set of vertices \(V^*\).  Here, we wish
to estimate only the pose transformation parameters \(\w\) (while keeping all
parameters in \(\phi\) fixed) such that the rendered image \(R(\w;\phi)\) causes
the classifier \(f\) to assign the highest probability (among all outputs) to an
incorrect target output at index \(t\).  Formally, we attempt to solve the below
optimization problem:

#+BEGIN_EXPORT latex
\begin{equation}
  \label{eq:min}
  \w^* = \argmax_{\w}(f_t(R(\w; \phi)))
\end{equation}
#+END_EXPORT

In practice, we minimize the cross-entropy loss \(\LL\) for the target class.
E.q. ref:eq:min may be solved efficiently via backpropagation if both \(f\) and
\(R\) are differentiable, i.e., we are able to compute \(\partial
\LL/\partial\w\).  However, standard 3D renderers, e.g., OpenGL
cite:woo1999opengl, typically include many non-differentiable operations and
cannot be inverted cite:marschner2015fundamentals.  Therefore, we attempted two
approaches:
1. harnessing a recently proposed differentiable renderer and performing
   gradient descent using its analytical gradients; and
2. harnessing a non-differentiable renderer and approximating the gradient via
   finite differences.

We will next describe the target classifier (Sec.~\ref{sec:target_dnn}), the
renderers (Sec.~\ref{sec:renderers}), and our dataset of 3D objects
(Sec.~\ref{sec:3d_object_dataset}) before discussing the optimization methods
(Sec.~\ref{sec:methods}).

*** Classification Networks

We chose the well-known, pre-trained Google Inception-v3 \cite{Szegedy2015} DNN
from the PyTorch model zoo \cite{torch2018vision} as the main image classifier
for our study (the default DNN if not otherwise stated).  The DNN has a 77.45%
top-1 accuracy on the ImageNet ILSVRC 2012 dataset cite:russakovsky2015imagenet
of 1.2 million images corresponding to 1,000 categories.

** 3D Renderers

*** Non-differentiable Renderer (NR)

We chose ModernGL cite:modernGL as our non-differentiable renderer.  ModernGL is
a simple Python interface for the widely used OpenGL graphics engine.  ModernGL
supports fast, GPU-accelerated rendering.  This renderer is referred as NR
hereafter.

*** Differentiable Renderer (DR)

To enable backpropagation through the non-differentiable rasterization process,
Kato et al. cite:kato2018neural replaced the discrete pixel color sampling step
with a linear interpolation sampling scheme that admits non-zero gradients.
While the approximation enables gradients to flow from the output image back to
the renderer parameters \(\phi\), the render quality is lower than that of our
non-differentiable renderer (see Figure ref:fig:compare_tessellation for a
comparison).  This renderer is referred as DR hereafter.

** 3D Object Dataset

*** Construction

Our main dataset consists of 30 unique 3D object models (purchased from many 3D
model marketplaces) corresponding to 30 ImageNet classes relevant to a traffic
environment (Figure ref:fig:dataset_A).  The 30 classes include 20 vehicles
(e.g., \class{school bus} and \class{cab}) and 10 street-related items (e.g.,
\class{traffic light}).  See Sec.~\ref{sec:SI_3d_object_dataset} for more
details.

Each 3D object is represented as a mesh, i.e., a list of triangular faces, each
defined by three vertices \cite{marschner2015fundamentals}.  The 30 meshes have
on average 9,908 triangles (Table ref:tab:num_triangles).  To maximize the
realism of the rendered images, we used only 3D models that have high-quality 2D
image textures.  We did not choose 3D models from public datasets, e.g.,
ObjectNet3D cite:xiang2016objectnet3d, because most of them do not have
high-quality image textures.  That is, the renders of such models may be
correctly classified by DNNs but still have poor realism.

*** Evaluation

We recognize that a reality gap will often exist between a render and a real
photo.

Therefore, we rigorously evaluated our renders to make sure the reality gap was
acceptable for our study.  From \(\sim\)100 initially-purchased 3D models, we
selected the 30 highest-quality models using the evaluation method below.
First, we quantitatively evaluated DNN predictions on the renders.  For each
object, we sampled 36 unique views (common in ImageNet) evenly divided into
three sets.  For each set, we set the object at the origin, the up direction to
\((0,1,0)\), and the camera position to \((0,0,-z)\) where \(z = \{4,6,8\}\).
We sampled 12 views per set by starting the object at a \(10^\circ{}\) yaw and
generating a render at every \(30^\degree\) yaw-rotation.  Across all objects
and all renders, the Inception-v3 top-1 accuracy was \(83.23%\) (compared to
\(77.45%\) on ImageNet images \cite{szegedy2016rethinking}) with a mean top-1
confidence score of \(0.78\) (Table~\ref{tab:avg_accuracy_30obj}).  See
Sec.~\ref{sec:SI_3d_object_dataset} for more details.  Second, we qualitatively
evaluated the renders by comparing them to real photos.  We produced 56 (real
photo, render) pairs via three steps:

1. we retrieved real photos of an object (e.g., a car) from the Internet;
2. we replaced the object with matching background content in Adobe Photoshop; and
3. we manually rendered the 3D object on the background such that its pose
   closely matched that in the reference photo.

Figure ref:fig:dataset_B shows example (real photo, render) pairs.  While
discrepancies can be spotted in our side-by-side comparisons, we found that most
of the renders passed our human visual Turing test if presented alone.

*** Background Images

Previous studies have shown that image classifiers may be able to correctly
label an image when foreground objects are removed (i.e., based on only the
background content) cite:zhu2016object.

Because the purpose of our study was to understand how DNNs recognize an object
itself, a non-empty background would have hindered our interpretation of the
results.  Therefore, we rendered all images against a plain background with RGB
values of \((0.485, 0.456, 0.406)\), i.e., the mean pixel of ImageNet images.
Note that the presence of a non-empty background should not alter our main
qualitative findings in this paper, adversarial poses can be easily found
against real background photos (Figure ref:fig:teaser).

# TODO(gongzhitaao): EDIT HERE

** Methods

We will describe the common pose transformations
(Sec.~\ref{sec:transformations}) used in the main experiments.  We were able to
experiment with non-gradient methods because: (1) the pose transformation space
\(\R^6\) that we optimize in is fairly low-dimensional; and (2) although the NR
is non-differentiable, its rendering speed is several orders of magnitude faster
than that of DR.  In addition, our preliminary results showed that the objective
function considered in Eq.~\ref{eq:min} is highly non-convex (see
Figure ref:fig:landscape), therefore, it is interesting to compare (1) random
search vs.  (2) gradient descent using finite-difference (FD) approximated
gradients vs.  (3) gradient descent using the DR gradients.  \subsection{Pose
transformations} \label{sec:transformations} We used standard computer graphics
transformation matrices to change the pose of 3D objects
\cite{marschner2015fundamentals}.  Specifically, to rotate an object with
geometry defined by a set of vertices \(V = \{v_i\}\), we applied the linear
transformations in Eq.~\ref{eq:rot} to each vertex \(v_{i} \in \R^3\):

\begin{equation}
\label{eq:rot}
v_{i}^{R} = R_{y}R_{p}R_{r}v_{i}
\end{equation}
\noindent where \(R_{y}\), \(R_{p}\), and \(R_{r}\) are the \(3\times 3\) rotation
matrices for yaw, pitch, and roll, respectively (the matrices can be found in
Sec.~\ref{sec:trans-mat}).

We then translated the rotated object by adding a vector $T = \begin{bmatrix}
x_{\delta} & y_{\delta} & z_{\delta} \end{bmatrix}^\top$ to each vertex:
\begin{equation}
v_{i}^{R,T} = T + v_{i}^{R}
\end{equation}
In all experiments, the center \(c \in \R^3\) of the object was constrained to be
inside a sub-volume of the camera viewing frustum.  That is, the \(x\)-, \(y\)-, and
\(z\)-coordinates of \(c\) were within \([-s,s]\), \([-s,s]\), and \([-28,0]\),
respectively, with \(s\) being the maximum value that would keep \(c\) within the
camera frame.  Specifically, \(s\) is defined as:
\begin{equation} \label{eq:max_trans}
s = d \cdot\tan(\theta_{v})
\end{equation}
\noindent where \(\theta_{v}\) is one half the camera's angle of view (i.e.,
\(8.213\degree\) in our experiments) and \(d\) is the absolute value of the
difference between the camera's \(z\)-coordinate and \(z_{\delta}\).

\subsection{Random search} \label{sec:random_search} In reinforcement learning
problems, random search (RS) can be surprisingly effective compared to more
sophisticated methods \cite{such2017deep}.  For our RS procedure, instead of
iteratively following some approximated gradient to solve the optimization
problem in Eq.~\ref{eq:min}, we simply randomly selected a new pose in each
iteration.  The rotation angles for the matrices in Eq.~\ref{eq:rot} were
uniformly sampled from \((0, 2\pi)\).  \(x_{\delta}\), \(y_{\delta}\), and
\(z_{\delta}\) were also uniformly sampled from the ranges defined in
Sec.~\ref{sec:transformations}.  \subsection{\(z_{\delta}\)-constrained random
search}

Our preliminary RS results suggest the value of \(z_{\delta}\) (which is a proxy
for the object's size in the rendered image) has a large influence on a DNN's
predictions.  Based on this observation, we used a \(z_{\delta}\)-constrained
random search (ZRS) procedure both as an initializer for our gradient-based
methods and as a naive performance baseline (for comparisons in
Sec.~\ref{sec:comparing_methods}).  The ZRS procedure consisted of generating 10
random samples of \((x_{\delta}, y_{\delta}, \theta_{y}, \theta_{p}, \theta_{r})\)
at each of 30 evenly spaced \(z_{\delta}\) from \(-28\) to \(0\).  When using ZRS for
initialization, the parameter set with the maximum target probability was
selected as the starting point.  When using the procedure as an attack method,
we first gathered the maximum target probabilities for each \(z_{\delta}\), and
then selected the best two \(z_{\delta}\) to serve as the new range for RS.
\subsection{Gradient descent with finite-difference} \label{sec:fd} We
calculated the first-order derivatives via finite central differences and
performed vanilla gradient descent to iteratively minimize the cross-entropy
loss \(\LL\) for a target class.  That is, for each parameter \(\w_{i}\), the
partial derivative is approximated by:

\begin{equation} \label{eq:fd}
\frac{\partial \LL}{\partial \w_{i}} = \frac{\LL(\w_{i} + \frac{h}{2}) - \LL(\w_{i} - \frac{h}{2})}{h}
\end{equation}
\noindent Although we used an \(h\) of 0.001 for all parameters, a different step
size can be used per parameter.  Because radians have a circular topology (i.e.,
a rotation of 0 radians is the same as a rotation of \(2\pi\) radians, \(4\pi\)
radians, etc.), we parameterized each rotation angle \(\theta_i\) as
\((\cos(\theta_i), \sin(\theta_i))\)---a technique commonly used for pose
estimation cite:Osadchy2005 and inverse kinematics cite:Choi1992---which maps
the Cartesian plane to angles via the \(atan2\) function.  Therefore, we optimized
in a space of \(3 + 2 \times 3 = 9\) parameters.  The approximate gradient $\nabla
\LL$ obtained from Equation~\eqref{eq:fd} served as the gradient in our gradient
descent.  We used the vanilla gradient descent update rule:
\begin{equation}
\w \coloneqq \w - \gamma{\nabla \LL}(\w)
\end{equation}
\noindent with a learning rate \(\gamma\) of 0.001 for all parameters and
optimized for \(100\) steps (no other stopping criteria).

\section{Experiments and results}

\subsection{Neural networks are easily confused by object rotations and
translations} \label{sec:easily_confused}

\begin{figure}[h]
\begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=\linewidth]{high_conf_params.pdf}
    \caption{Incorrect classifications}\label{fig:high_conf_params}
\end{subfigure}
\begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=\linewidth]{high_conf_correct_params.pdf}
    \caption{Correct classifications}\label{fig:high_conf_correct_params}
\end{subfigure}
\caption{The distributions of individual pose parameters for (a) high-confidence (\(p \geq 0.7\)) incorrect classifications and (b) correct classifications obtained from the random sampling procedure described in Sec.~\ref{sec:random_search}.
\(x_{\delta}\) and \(y_{\delta}\) have been normalized w.r.t.  their corresponding \(s\) from Eq.~\ref{eq:max_trans}.}
\label{fig:param_distributions}
\end{figure}

\subsec{Experiment} To test DNN robustness to object rotations and translations,
we used RS to generate samples for every 3D object in our dataset.  In addition,
to explore the impact of lighting on DNN performance, we considered three
different lighting settings: \bright, \medium, and \dark (example renders in
Figure ref:fig:light_intensity).  In all three settings, both the directional
light and the ambient light were white in color, i.e., had RGB values of $(1.0,
1.0, 1.0)\(, and the directional light was oriented at \)(0, -1, 0)$ (i.e.,
pointing straight down).  The directional light intensities and ambient light
intensities were \((1.2, 1.6)\), \((0.4, 1.0)\), and \((0.2, 0.5)\) for the \bright,
\medium, and \dark settings, respectively.  All other experiments used the
\medium lighting setting.  \subsec{Misclassifications uniformly cover the pose
space} For each object, we calculated the DNN accuracy (i.e., percent of
correctly classified samples) across all three lighting settings
(Table~\ref{tab:sampling_stats}).

The DNN was wrong for the vast majority of samples, i.e., the median percent of
correct classifications for all 30 objects was only 3.09%.  Moreover,
high-confidence misclassifications (\(p \geq 0.7\)) are largely uniformly
distributed across every pose parameter (Figure ref:fig:high_conf_params), i.e.,
AXs can be found throughout the parameter landscape (see Figure ref:fig:30_ax
for examples).  In contrast, correctly classified examples are highly multimodal
w.r.t.  the rotation axis angles and heavily biased towards \(z_{\delta}\) values
that are closer to the camera (Figure ref:fig:high_conf_correct_params).

\subsec{An object can be misclassified as many different labels}

Previous research has shown that it is relatively easy to produce AXs
corresponding to many different classes when optimizing input images
cite:szegedy2013intriguing or 3D object textures cite:Athalye2017, which are
very high-dimensional.  When finding adversarial poses, one might
expect---because all renderer parameters, including the original object geometry
and textures, are held constant---the success rate to depend largely on the
similarities between a given 3D object and examples of the target in ImageNet.
Interestingly, across our 30 objects, RS discovered \(990/1000\) different
ImageNet classes (132 of which were shared between all objects).  When only
considering high-confidence (\(p \geq 0.7\)) misclassifications, our 30 objects
were still misclassified into \(797\) different classes with a median number of
240 incorrect labels found per object (see
Figure ref:fig:common_failures_per_label and Figure ref:fig:tsne_img_ax for
examples).  Across all adversarial poses and objects, DNNs tend to be more
confident when correct than when wrong (the median of median probabilities were
0.41 vs.  0.21, respectively).

\begin{figure}[h]
\begin{subfigure}{\linewidth}

    \includegraphics[width=0.92\linewidth]{firetruck01_49_roll_pitch.pdf}
    \caption{}\label{fig:fire_truck_roll_pitch}
\end{subfigure}
\begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=0.85\linewidth]{firetruck01_49_collage.pdf}
    \caption{}\label{fig:fire_truck_pitch_roll_collage}
\end{subfigure}
\caption{
Inception-v3's ability to correctly classify images is highly localized in the rotation and translation parameter space.
(a) The classification landscape for a fire truck object when altering \(\theta_{r}\) and \(\theta_{p}\) and holding \((x_{\delta}, y_{\delta}, z_{\delta}, \theta_{y})\) at \((0, 0, -3, \frac{\pi}{4})\).
Light regions correspond to correct classifications while dark regions correspond to incorrect classifications.
Green and red circles indicate correct and incorrect classifications, respectively, corresponding to the fire truck object poses found in (b).
}\label{fig:landscape}
\end{figure}

\subsection{Common object classifications are shared across different lighting
settings}

Here, we analyze how our results generalize across different lighting
conditions.  From the data produced in Sec.~\ref{sec:easily_confused}, for each
object, we calculated the DNN accuracy under each lighting setting.  Then, for
each object, we took the absolute difference of the accuracies for all three
lighting combinations (i.e., \bright vs.~\medium, \bright vs.~\dark, and \medium
vs.~\dark) and recorded the maximum of those values.  The median ``maximum
absolute difference'' of accuracies for all objects was 2.29% (compared to the
median accuracy of \(3.09%\) across all lighting settings).  That is, DNN
accuracy is consistently low across all lighting conditions.  Lighting changes
would not alter the fact that DNNs are vulnerable to adversarial poses.  We also
recorded the 50 most frequent classes for each object under the different
lighting settings (\(S_{b}\), \(S_{m}\), and \(S_{d}\)).  Then, for each object, we
computed the intersection over union score \(o_{S}\) for these sets:

\vspace*{-0.3cm}
\begin{equation}
o_{S} = 100 \cdot \frac{|S_{b} \cap S_{m} \cap S_{d}|}{|  S_{b} \cup S_{m} \cup S_{d} |}
\end{equation}

\noindent The median \(o_{S}\) for all objects was 47.10%.  That is, for 15 out
of 30 objects, 47.10% of the 50 most frequent classes were shared across
lighting settings.  While lighting does have an impact on DNN misclassifications
(as expected), the large number of shared labels across lighting settings
suggests ImageNet classes are strongly associated with certain adversarial poses
regardless of lighting.

\subsection{Correct classifications are highly localized in the rotation and
translation landscape} \label{sec:landscape} To gain some intuition for how
Inception-v3 responds to rotations and translations of an object, we plotted the
probability and classification landscapes for paired parameters (e.g.,
Figure ref:fig:landscape; pitch vs.  roll) while holding the other parameters
constant.  We qualitatively observed that the DNN's ability to recognize an
object (e.g., a fire truck) in an image varies radically as the object is rotated
in the world (Figure ref:fig:landscape).

\subsec{Experiment} To quantitatively evaluate the DNN's sensitivity to
rotations and translations, we tested how it responded to single parameter
disturbances.  For each object, we randomly selected 100 distinct starting poses
that the DNN had correctly classified in our random sampling runs.  Then, for
each parameter (e.g., yaw rotation angle), we randomly sampled 100 new
values\footnote{\label{note:sampling}using the random sampling procedure
described in Sec.~\ref{sec:random_search}} while holding the others constant.
For each sample, we recorded whether or not the object remained correctly
classified, and then computed the failure (i.e., misclassification) rate for a
given (object,~parameter) pair.  Plots of the failure rates for all
(object,~parameter) combinations can be found in Figure ref:fig:sensitivity.
Additionally, for each parameter, we calculated the median of the median failure
rates.  That is, for each parameter, we first calculated the median failure rate
for all objects, and then calculated the median of those medians for each
parameter.  Further, for each (object,~starting pose,~parameter) triple, we
recorded the magnitude of the smallest parameter change that resulted in a
misclassification.  Then, for each (object,~parameter) pair, we recorded the
median of these minimum values.  Finally, we again calculated the median of
these medians across objects (Table~\ref{tab:fail_rates}).  \subsec{Results} As
can be seen in Table~\ref{tab:fail_rates}, the DNN is highly sensitive to all
single parameter disturbances, but it is especially sensitive to disturbances
along the depth (\(z_{\delta}\)), pitch (\(\theta_{p}\)), and roll (\(\theta_{r}\)).
Note that a change in rotation as small as \(8.02\degree\) can cause an object to
be misclassified (see Table~\ref{tab:fail_rates}).  We also observed that
correctly classified poses are highly similar while misclassified poses are
diverse by comparing two t-SNE plots of these two sets of poses
(Figure ref:fig:tsne_img_correct vs.  Figure ref:fig:tsne_img_ax).
\begin{table}[h]
    \begin{center}
        \begin{tabular}{cccc}
            \toprule
            Parameter & Fail Rate (%) & Min.  \(\Delta\) & Int.  \(\Delta\) \\
            \midrule
            \(x_{\delta}\) & 42 & 0.09 & 2.0 \\
            \(y_{\delta}\) & 49 & 0.10 & 4.5 \\
            \(z_{\delta}\) & 81 & 0.77 & 5.4% \\
            \(\theta_{y}\) & 69 & 0.18 & \(10.31\degree\) \\
            \(\theta_{p}\) & 83 & 0.14 & \(8.02\degree\) \\
            \(\theta_{r}\) & 81 & 0.16 & \(9.17\degree\) \\
            \bottomrule
        \end{tabular}
    \end{center}
    \caption{The median of the median failure rates and the median of the median minimum disturbances (Min.  \(\Delta\)) for the single parameter sensitivity tests described in Section \ref{sec:landscape}.
    Int.  \(\Delta\) translates the values in Min.  \(\Delta\) to more interpretable units.
    For \(x_{\delta}\) and \(y_{\delta}\), the units are pixels.
    For \(z_{\delta}\), the unit is the percent change in the area of the bounding box containing the object.
        See main text and Figure ref:fig:sensitivity for additional information.
    }
    \label{tab:fail_rates}
\end{table}

\subsection{Optimization methods can effectively generate targeted adversarial
poses} \label{sec:comparing_methods}

Given a challenging, highly non-convex objective landscape
(Figure ref:fig:landscape), we wish to evaluate the effectiveness of two
different types of approximate gradients at targeted attacks, i.e., finding
adversarial examples misclassified as a target class
\cite{szegedy2013intriguing}.  Here, we compare (1) random search; (2) gradient
descent with finite-difference gradients (FD-G); and (3) gradient descent with
analytical, approximate gradients provided by a differentiable renderer (DR-G)
\cite{kato2018neural}.  \subsec{Experiment} Because our adversarial pose attacks
are inherently constrained by the fixed geometry and appearances of a given 3D
object (see Sec.~\ref{sec:easily_confused}),

we defined the targets to be the 50 most frequent incorrect classes found by our
RS procedure for each object.  For each (object, target) pair, we ran 50
optimization trials using ZRS, FD-G, and DR-G.  All treatments were initialized
with a pose found by the ZRS procedure and then allowed to optimize for 100
iterations.

\subsec{Results} For each of the 50 optimization trials, we recorded both
whether or not the target was hit and the maximum target probability obtained
during the run.  For each (object,~target) pair, we calculated the percent of
target hits and the median maximum confidence score of the target labels (see
Table~\ref{tab:optim_stats}).  As shown in Table~\ref{tab:optim_stats}, FD-G is
substantially more effective than ZRS at generating targeted adversarial poses,
having both higher median hit rates and confidence scores.  In addition, we
found the approximate gradients from DR to be surprisingly noisy, and DR-G
largely underperformed even non-gradient methods (ZRS) (see Sec.~\ref{sec:kr}).

\begin{table}[h]
  \centering
  \begin{tabular}{lcrr}
    \toprule
    & Hit Rate (%) & Target Prob.  \\
    \midrule
    ZRS ~~~~random search & 78 & 0.29 \\
    FD-G ~~gradient-based & \textbf{92} & \textbf{0.41} \\
    DR-G\(^\dagger\) gradient-based & 32 & 0.22 \\
    \bottomrule
  \end{tabular}
  \caption{The median percent of target hits and the median of the median target probabilities
    for random search (ZRS), gradient descent with finite difference gradients (FD-G), and DR gradients (DR-G).
    All attacks are targeted and initialized with \(z_{\delta}\)-constrained random search.
  \(^\dagger\)DR-G is not directly comparable to FD-G and ZRS
  (details in Sec.~\ref{sec:dr-expr}).
    }
  \label{tab:optim_stats}
\end{table}
\subsection{Adversarial poses transfer to different image classifiers and object
detectors}

The most important property of previously documented AXs is that they transfer
across ML models, enabling black-box attacks cite:Yuan2017.  Here, we
investigate the transferability of our adversarial poses to (a) two different
image classifiers, AlexNet cite:Krizhevsky2012 and ResNet-50 cite:He2016,
trained on the same ImageNet dataset; and (b) an object detector YOLOv3
cite:Redmon2018 trained on the MS COCO dataset cite:lin2014microsoft.

For each object, we randomly selected 1,350 AXs that were misclassified by
Inception-v3 with high confidence (\(p \geq 0.9\)) from our untargeted RS
experiments in Sec.~\ref{sec:easily_confused}.  We exposed the AXs to AlexNet
and ResNet-50 and calculated their misclassification rates.  We found that
almost all AXs transfer with median misclassification rates of 99.9% and 99.4%
for AlexNet and ResNet-50, respectively.  In addition, 10.1% of AlexNet
misclassifications and 27.7% of ResNet-50 misclassifications were identical to
the Inception-v3 predicted labels.  There are two orthogonal hypotheses for this
result.  First, the ImageNet training-set images themselves may contain a strong
bias towards common poses, omitting uncommon poses
(Sec.~\ref{sec:nearest_neighbors} shows supporting evidence from a
nearest-neighbor test).  Second, the models themselves may not be robust to even
slight disturbances of the known, in-distribution poses.

\subsec{Object detectors} Previous research has shown that object detectors can
be more robust to adversarial attacks than image classifiers
cite:lu2017-standard.  Here, we investigate how well our AXs transfer to a
state-of-the-art object detector---YOLOv3.  YOLOv3 was trained on MS COCO, a
dataset of bounding boxes corresponding to 80 different object classes.

We only considered the 13 objects that belong to classes present in both the
ImageNet and MS COCO datasets.  We found that 75.5% of adversarial poses
generated for Inception-v3 are also misclassified by YOLOv3 (see
Sec.~\ref{sec:yolo} for more details).  These results suggest the adversarial
pose problem transfers across datasets, models, and tasks.


\subsection{Adversarial training}\label{sec:adversarial_training} One of the
most effective methods for defending against OoD examples has been adversarial
training \cite{Goodfellow2014}, i.e., augmenting the training set with AXs---also
a common approach in anomaly detection \cite{chandola2009anomaly}.  Here, we
test whether adversarial training can improve DNN robustness to new poses
generated for (1) our 30 training-set 3D objects; and (2) seven held-out 3D
objects.

\subsec{Training} We augmented the original 1,000-class ImageNet dataset with an
additional 30 AX classes.  Each AX class included 1,350 randomly selected
high-confidence (\(p \geq 0.9\)) misclassified images split 1,300/50 into
training/validation sets.  Our AlexNet trained on the augmented dataset (AT)
achieved a top-1 accuracy of 0.565 for the original ImageNet validation set and
a top-1 accuracy\footnote{In this case, a classification was ``correct'' if it
matched /either/ the original ImageNet positive label /or/ the
negative, object label.} of 0.967 for the AX validation set.

\begin{table}[h]
\begin{center}
  \begin{tabular}{lcc}
    \toprule
     & PT & AT \\
    \midrule
    Error (T) & 99.67 & 6.7 \\
    Error (H) & 99.81 & 89.2 \\
    \midrule
    High-confidence Error (T) & 87.8 & 1.9 \\        High-confidence Error (H) & 48.2 & 33.3 \\
    \bottomrule
\end{tabular}
\end{center}
\caption{The median percent of misclassifications (Error) and high-confidence (i.e., \(p > 0.7\)) misclassifications by the pre-trained AlexNet (PT) and our AlexNet trained with adversarial examples (AT) on random poses of training-set objects (T) and held-out objects (H).}
\label{tab:ax_stats}
\end{table}

\subsec{Evaluation} To evaluate our AT model vs.  a pre-trained AlexNet (PT), we
used RS to generate \(10^6\) samples for each of our 3D training objects.  In
addition, we collected seven held-out 3D objects not included in the training
set that belong to the same classes as seven training-set objects (example
renders in Figure ref:fig:7_pairs).  We followed the same sampling procedure for
the held-out objects to evaluate whether our AT generalizes to unseen objects.

For each of these \(30 + 7 = 37\) objects and for both the PT and our AT, we
recorded two statistics: (1) the percent of misclassifications, i.e., errors; and
(2) the percent of high-confidence (i.e., \(p \geq 0.7\)) misclassifications
(Table~\ref{tab:ax_stats}).  Following adversarial training, the accuracy of the
DNN substantially increased for /known/ objects (Table~\ref{tab:ax_stats};
\(99.67%\) vs.  \(6.7%\)).  However, our AT still misclassified the adversarial
poses of held-out objects at an 89.2% error rate.  We hypothesize that
augmenting the dataset with many more 3D objects may improve DNN generalization
on held-out objects.  Here, AT might have used (1) the grey background to
separate the 1,000 original ImageNet classes from the 30 AX classes; and (2)
some non-geometric features sufficient to discriminate among only 30 objects.
However, as suggested by our work (Sec.~\ref{sec:3d_object_dataset}), acquiring
a large-scale, high-quality 3D object dataset is costly and labor-intensive.
Currently, no such public dataset exists, and thus we could not test this
hypothesis.

\section{Related work}

\subsec{Out-of-distribution detection} OoD classes, i.e., classes not found in
the training set, present a significant challenge for computer vision
technologies in real-world settings cite:scheirer2013toward.  Here, we study an
orthogonal problem---correctly classifying OoD poses of objects from
/known/ classes.  While rejecting to classify is a common approach for
handling OoD examples cite:hendrycks2016baseline,scheirer2013toward, the OoD
poses in our work come from known classes and thus /should be/ assigned
correct labels.


\subsec{2D adversarial examples} Numerous techniques for crafting AXs that fool
image classifiers have been discovered cite:Yuan2017.  However, previous work
has typically optimized in the 2D input space cite:Yuan2017, e.g., by
synthesizing an entire image cite:nguyen2015deep, a small patch
cite:karmon2018lavan,evtimov2017robust, a few pixels cite:carlini2017towards, or
only a single pixel cite:su2017one.  But pixel-wise changes are uncorrelated
cite:nguyen2017plug, so pixel-based attacks may not transfer well to the real
world cite:Lu2017,Luo2015 because there is an infinitesimal chance that such
specifically crafted, uncorrelated pixels will be encountered in the vast
physical space of camera, lighting, traffic, and weather configurations.

\subsec{3D adversarial examples} Athalye et al. cite:Athalye2017 used a 3D
renderer to synthesize textures for a 3D object such that, under a wide range of
camera views, the object was still rendered into an effective AX.  We also used
3D renderers, but instead of optimizing textures, we optimized the poses of
known objects to cause DNNs to misclassify (i.e., we kept the textures, lighting,
camera settings, and background image constant).

\subsec{Concurrent work} We describe below two concurrent attempts that are
closely related but orthogonal to our work.  First, Liu et al. cite:Liu2018
proposed a differentiable 3D renderer and used it to perturb both an object's
geometry and the scene's lighting to cause a DNN to misbehave.  However, their
geometry perturbations were constrained to be infinitesimal so that the
visibility of the vertices would not change.  Therefore, their result of
minutely perturbing the geometry is effectively similar to that of perturbing
textures cite:Athalye2017.  In contrast, we performed 3D rotations and 3D
translations to move an object inside a 3D space (i.e., the viewing frustum of
the camera).

Second, an anonymous ICLR 2019 submission cite:anonymous2019a showed how simple
rotations and translations of an image can cause DNNs to misclassify.  However,
these manipulations were still applied to the entire 2D image and thus do not
reveal the type of adversarial poses discovered by rotating 3D objects (e.g., a
flipped-over school bus; Figure ref:fig:teaserd).

To the best of our knowledge, our work is the first attempt to harness 3D
objects to study the OoD poses of well-known training-set objects that cause
state-of-the-art ImageNet classifiers and MS COCO detectors to misclassify.

\section{Discussion and conclusion} In this paper, we revealed how DNNs'
understanding of objects like ``school bus'' and ``fire truck'' is quite
naive---they can correctly label only a small subset of the entire pose space
for 3D objects.  Note that we can also find real-world OoD poses by simply
taking photos of real objects (Figure ref:fig:real_ax).

We believe classifying an arbitrary pose into one of the object classes is an
ill-posed task, and that the adversarial pose problem might be alleviated via
multiple orthogonal approaches.  The first is addressing biased data
\cite{torralba2011unbiased}.  Because ImageNet and MS COCO datasets are
constructed from photographs taken by people, the datasets reflect the aesthetic
tendencies of their captors.  Such biases can be somewhat alleviated through
data augmentation, specifically, by harnessing images generated from 3D
renderers \cite{shrivastava2017learning,alhaija2018geometric}.

From the modeling view, we believe DNNs would also benefit from strong 3D
geometric priors \cite{alhaija2018geometric}.  Finally, our work introduced a
new promising method (Figure ref:fig:concept) for testing computer vision DNNs
by harnessing 3D renderers and 3D models.  While we only optimize a single
object here, the framework could be extended to jointly optimize lighting,
background image, and multiple objects, all in one ``adversarial world''.  Not
only does our framework enable us to enumerate test cases for DNNs, but it also
serves as an interpretability tool for extracting useful insights about these
black-box models' inner functions.

\section*{Acknowledgement} We thank Hiroharu Kato and Nikos Kolotouros for their
valuable discussions and help with the differentiable renderer.  We also thank
Rodrigo Sardinas for his help with some GPU servers used in the project.  AN is
supported by multiple funds from Auburn University, a donation from Adobe Inc.,
and computing credits from Amazon AWS.


{\small \bibliographystyle{style/ieee} \bibliography{references} }

\clearpage

\renewcommand{\thesection}{S\arabic{section}}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}

\newcommand{\beginsupplementary}{% \setcounter{table}{0}
            \renewcommand{\thetable}{S\arabic{table}}% \setcounter{figure}{0}
            \renewcommand{\thefigure}{S\arabic{figure}}% \setcounter{section}{0}
            } \newcommand{\suptitle}{Supplementary materials for:\\\papertitle}

\newcommand{\toptitlebar}{ \hrule height 4pt \vskip 0.25in \vskip -\parskip% }
    \newcommand{\bottomtitlebar}{ \vskip 0.29in \vskip -\parskip% \hrule height
    1pt \vskip 0.09in% }

\beginsupplementary%

\newcommand{\maketitlesupp}{ \newpage \onecolumn%[ \null% \vskip .375in
        \begin{center}
            {\Large \bf \suptitle\par}
            \vspace*{24pt}
            {
                \large
                \lineskip=.5em


                \par
            }
            \vskip .5em
            \vspace*{12pt}
        \end{center}
} \maketitlesupp%

\section{Extended description of the 3D object dataset and its evaluation}
\label{sec:SI_3d_object_dataset} \subsection{Dataset
construction}\label{sec:construction}

\subsec{Classes} Our main dataset consists of 30 unique 3D object models
corresponding to 30 ImageNet classes relevant to a traffic environment.  The 30
classes include 20 vehicles (e.g., \class{school bus} and \class{cab}) and 10
street-related items (e.g., \class{traffic light}).  See Figure ref:fig:dataset_A
for example renders of each object.

\subsec{Acquisition} We collected 3D objects and constructed our own datasets
for the study.  3D models with high-quality image textures were purchased from
turbosquid.com, free3d.com, and cgtrader.com.  To make sure
the renders were as close to real ImageNet photos as possible, we used only 3D
models that had high-quality 2D image textures.  We did not choose 3D models
from public datasets, e.g., ObjectNet3D \cite{xiang2016objectnet3d}, because most
of them do not have high-quality image textures.  While the renders of such
models may be correctly classified by DNNs, we excluded them from our study
because of their poor realism.  We also examined the ImageNet images to ensure
they contained real-world examples qualitatively similar to each 3D object in
our 3D dataset.

\subsec{3D objects} Each 3D object is represented as a mesh, i.e., a list of
triangular faces, each defined by three vertices
\cite{marschner2015fundamentals}.  The 30 meshes have on average \(9,908\)
triangles (see Table~\ref{tab:num_triangles} for specific numbers).

  \begin{table}[h]
    \centering
    \begin{tabular}{lrr}
        \toprule
        3D object        & Tessellated \(N_T\) & Original \(N_O\) \\
        \midrule
        \class{ambulance}            & 70,228 & 5,348 \\
        \class{backpack}             & 48,251 & 1,689\\
        \class{bald eagle}           & 63,212 & 2,950\\
        \class{beach wagon}          & 220,956 & 2,024\\
        \class{cab}                  & 53,776 & 4,743\\
        \class{cellphone}   & 59,910 & 502\\
        \class{fire engine}          & 93,105 & 8,996\\
        \class{forklift}             & 130,455 & 5,223\\
        \class{garbage truck}        & 97,482 & 5,778\\
        \class{German shepherd}      & 88,496 & 88,496\\
        \class{golf cart}            & 98,007 & 5,153\\
        \class{jean}                 & 17,920 & 17,920\\
        \class{jeep}                 & 191,144 & 2,282\\
        \class{minibus}              & 193,772 & 1,910\\
        \class{minivan}              & 271,178 & 1,548\\
        \bottomrule
    \end{tabular}
    \begin{tabular}{lrr}
        \toprule
        3D object        & Tessellated \(N_T\) & Original \(N_O\) \\
        \midrule
        \class{motor scooter}        & 96,638 & 2,356\\
        \class{moving van}           & 83,712 & 5,055\\
        \class{park bench}           & 134,162 & 1,972\\
        \class{parking meter}        & 37,246 & 1,086\\
        \class{pickup}               & 191,580 & 2,058\\
        \class{police van}           & 243,132 & 1,984\\
        \class{recreational vehicle} & 191,532 & 1,870\\
        \class{school bus}           & 229,584 & 6,244\\
        \class{sports car}           & 194,406 & 2,406\\
        \class{street sign}          & 17,458 & 17,458\\
        \class{tiger cat}            & 107,431 & 3,954\\
        \class{tow truck}            & 221,272 & 5,764\\
        \class{traffic light}        & 392,001 & 13,840\\
        \class{trailer truck}        & 526,002 & 5,224\\
        \class{umbrella}             & 71,410 & 71,410\\
        \bottomrule
    \end{tabular}
    \caption{The triangle number for the 30 objects used in our study.
        \(N_O\) shows the number of
        triangles for the original 3D objects, and      \(N_T\) shows the same number after tessellation.
          Across 30 objects, the average triangle count increases \(\sim15\)x from \(\overline{N_O} = 9,908\) to
          \(\overline{N_T} = 147,849\).}\label{tab:num_triangles}
\end{table}
\subsection{Manual object tessellation for experiments using the Differentiable
Renderer} In contrast to ModernGL \cite{modernGL}---the non-differentiable
renderer (NR) in our paper---the differentiable renderer (DR) by Kato et.  al
cite:kato2018neural does not perform tessellation, a standard process to
increase the resolution of renders.  Therefore, the render quality of the DR is
lower than that of the NR.  To minimize this gap and make results from the NR
more comparable with those from the DR, we manually tessellated each 3D object
as a pre-processing step for rendering with the DR.  Using the manually
tessellated objects, we then (1) evaluated the render quality of the DR
(Sec.~\ref{sec:evaluation}); and (2) performed research experiments with the DR
(i.e., the DR-G method in Sec.~\ref{sec:comparing_methods}).\\

\subsec{Tessellation} We used the /Quadify Mesh Modifier/ feature (quad
size of 2%) in 3ds~Max 2018 to tessellate objects, increasing the average
number of faces \(\sim\)15x from \(9,908\) to \(147,849\) (see
Table~\ref{tab:num_triangles}).  The render quality after tessellation is
sharper and of a higher resolution (see Figure ref:fig:compare_tessellationa vs.
b).  Note that the NR pipeline already performs tessellation for every input 3D
object.  Therefore, we did not perform manual tessellation for 3D objects
rendered by the NR.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.7\linewidth]{compare_kr_and_gl.png}
    \end{center}
    \vspace*{-0.3cm}
    \centering
{
    \hspace{2.7cm}
    (a) DR without tessellation
    \hspace{0.35cm} (b) DR with tessellation
    \hspace{0.4cm} (c) NR with tessellation
    \hfill
}
    \caption{A comparison of 3D object renders (here, \class{ambulance} and \class{school bus}) before and after tessellation.
        \\(a) Original 3D models rendered by the differentiable renderer (DR) \cite{kato2018neural} without tessellation.
        \\(b) DR renderings of the same objects after manual tessellation.
        \\(c) The non-differentiable renderer (NR), i.e., ModernGL \cite{modernGL}, renderings of the original objects.
        \\After manual tessellation, the render quality of the DR appears to be sharper (a vs.  b) and closely matches that of the NR, which also internally tessellates objects (b vs.  c).
        }
    \label{fig:compare_tessellation}
\end{figure}

\subsection{Evaluation} \label{sec:evaluation}

We recognize that a reality gap will often exist between a render and a real
photo.

Therefore, we rigorously evaluated our renders to make sure the reality gap was
acceptable for our study.  From \(\sim\)100 initially-purchased 3D object models,
we selected the 30 highest-quality objects that both (1) passed a visual human
Turing test; and (2) were correctly recognized with high confidence by the
Inception-v3 classifier cite:szegedy2016rethinking.

\subsubsection{Qualitative evaluation} \label{sec:qualitative_eval} We did not
use the 30 objects chosen for the main dataset (Sec.~\ref{sec:construction}) to
evaluate the /general/ quality of the DR renderings of high-quality objects
on realistic background images.  Instead, we randomly chose a separate set of 17
high-quality image-textured objects for evaluation.  Using the 17 objects, we
generated 56 renders that matched 56 reference (real) photos.  Then, we
qualitatively evaluated the renders both separately and in a side-by-side
comparison with real photos.  Specifically, we produced 56 (real photo, render)
pairs (see Figure ref:fig:dataset_B) via the following steps:

\begin{enumerate}
    \item We retrieved \(\sim\)3 real photos for each 3D object (e.g., a car) from the Internet (using descriptive information, e.g., a car's make, model, and year).
    \item For each real photo, we replaced the object with matching background content via Adobe Photoshop's Context-Aware Fill-In feature to obtain a background-only photo \(B\) (i.e., no foreground objects).

    \item We rendered the 3D object with the differentiable renderer on the background \(B\) obtained in Step 2.
    We then manually aligned the pose of the 3D object such that it closely matched that in the reference photo.
    \item We evaluated pairs of (photo, render) in a side-by-side comparison.
\end{enumerate}


While discrepancies can be visually spotted in our side-by-side comparisons, we
found that most of the renders passed our human visual Turing test if presented
alone.  That is, it is not easy for humans to tell whether a render is a real
photo or not (if they are not primed with the reference photos).  We only show
pairs rendered by the DR because the NR qualitatively has a slightly higher
rendering quality (Figure ref:fig:compare_tessellationb vs.  c).
\subsubsection{Quantitative evaluation} \label{sec:quantitative_eval}

In addition to the qualitative evaluation, we also quantitatively evaluated the
Google Inception-v3 \cite{szegedy2016rethinking}'s top-1 accuracy on renders
that use either (a) an empty background or (b) real background images.
\subsubsection*{a.  Evaluation of the renders of 30 objects on an empty
background} Because the experiments in the main text used our self-assembled
30-object dataset (Sec.~\ref{sec:construction}), we describe the process and the
results of our quantitative evaluation for only those objects.

We rendered the objects on a white background with RGB values of (1.0, 1.0,
1.0), an ambient light intensity of 0.9, and a directional light intensity of
0.5.  For each object, we sampled 36 unique views (common in ImageNet) evenly
divided into three sets.  For each set, we set the object at the origin, the up
direction to \((0,1,0)\), and the camera position to \((0,0,-z)\) where $z = \{4, 6,
8\}\(.  We sampled 12 views per set by starting the object at a \)10^\circ{}$ yaw
and generating a render at every \(30^\circ{}\) yaw-rotation.  Across all objects
and all renders, the Inception-v3 top-1 accuracy is \(83.23%\) (comparable to
\(77.45%\) on ImageNet images \cite{szegedy2016rethinking}) with a mean top-1
confidence score of \(0.78\).  The top-1 and top-5 average accuracy and confidence
scores are shown in Table~\ref{tab:avg_accuracy_30obj}.

\begin{table}[h]
    \centering
    \begin{tabular}{*{5}{lccc}}
        \toprule
        Distance &  4 & 6 & 8 & Average \\ \midrule
        top-1 mean accuracy & 84.2% & 84.4%& 81.1%& 83.2%\\
        top-5 mean accuracy & 95.3% & 98.6%& 96.7%& 96.9% \\
        top-1 mean confidence score & 0.77 & 0.80 & 0.76& 0.78 \\ \bottomrule
    \end{tabular}
    \caption{The top-1 and top-5 average accuracy and confidence scores for Inception-v3 cite:szegedy2016rethinking on the renders of the 30 objects in our dataset.}\label{tab:avg_accuracy_30obj}
\end{table}
\subsubsection*{b.  Evaluation of the renders of test objects on real
backgrounds} In addition to our qualitative side-by-side (real photo, render)
comparisons (Figure ref:fig:dataset_B), we quantitatively compared
Inception-v3's predictions for our renders to those for real photos.

We found a high similarity between real photos and renders for DNN predictions.
That is, across all 56 pairs (Sec.~\ref{sec:qualitative_eval}), the top-1
predictions match 71.43% of the time.  Across all pairs, 76.06% of the top-5
labels for real photos match those for renders.

\begin{figure*}
    \centering
    \includegraphics[width=0.9\columnwidth]{30_objects.jpg}
    \caption{
        We tested Inception-v3's predictions on the renders generated by the differentiable renderer (DR).
        We show here the top-5 predictions for one random pose per object.
        However, in total, we generated 36 poses for each object by (1) varying the object distance to the camera; and (2) rotating the object around the yaw axis.
        See https://goo.gl/7LG3Cy for all the renders and DNN top-5 predictions.
        Across all 30 objects, on average, Inception-v3 correctly recognizes 83.2% of the renders.
        See Sec.~\ref{sec:quantitative_eval} for more details.
    }\label{fig:dataset_A}
\end{figure*}
 \begin{figure*}[t]
    \begin{center}
        \includegraphics[width=\linewidth]{random_pairs_kr_vs_real.jpg}
    \end{center}
    \caption{
        12 random pairs of renders (left) and real photos (right) among 56 pairs produced in total for our 3D object rendering evaluation (Sec.~\ref{sec:qualitative_eval}).
        The renders are produced by the differentiable renderer by Kato et al. cite:kato2018neural.
        More images are available at https://goo.gl/8z42zL.
        While discrepancies can be spotted in our side-by-side comparisons, we found that most of the renders passed our human visual Turing test if presented alone.
    }
    \label{fig:dataset_B}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1.0\columnwidth]{tsne_grid_correct.png}
    \caption{
        For each object, we collected 30 high-confidence (\(p \geq 0.9\)) correctly classified images by Inception-v3.
        The images were generated via the random search procedure.
        We show here a grid t-SNE of AlexNet \cite{Krizhevsky2012} \layer{fc7} features for all \(30\) objects \(\times\) \(30\) images = \(900\) images.
        Correctly classified images for each object tend to be similar and clustered together.
        The original, high-resolution figure is available at https://goo.gl/TGgPgB.
        \\To better visualize the clusters, we plotted the same t-SNE but used unique colors to denote the different 3D objects in the renders  (Figure ref:fig:tsne_color_correct).
        Compare and contrast this plot with the t-SNE of only misclassified poses
        (Figs.~\ref{fig:tsne_img_ax} \&~\ref{fig:tsne_color_ax}).
    }\label{fig:tsne_img_correct}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=1.0\columnwidth]{tsne_grid_label_correct.png}
    \caption{
        The same t-SNE found in Figure ref:fig:tsne_img_correct but using a unique color to denote the 3D object found in each rendered image.  Here, each color also corresponds to a unique Inception-v3 label.
        Compare and contrast this plot with the t-SNE of only misclassified poses
        (Figure ref:fig:tsne_color_ax).


        The original, high-resolution figure is available at https://goo.gl/TGgPgB.
    }\label{fig:tsne_color_correct}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=1.0\columnwidth]{tsne_grid_incorrect.jpg}
    \caption{
        Following the same process as described in Figure ref:fig:tsne_img_correct, we show here a grid t-SNE of generated adversarial poses.
        For each object, we assembled 30 high-confidence (\(p \geq 0.9\)) adversarial examples generated via a random search against Inception-v3 \cite{szegedy2016rethinking}.
        The t-SNE was generated from the AlexNet \cite{Krizhevsky2012} \layer{fc7} features for \(30\) objects \(\times\) \(30\) images = \(900\) images.
        The original, high-resolution figure is available at https://goo.gl/TGgPgB.
        Adversarial poses were found to be both common across different objects (e.g., the top-right corner) and unique to specific objects (e.g., the \class{traffic sign} and \class{umbrella} objects in the middle left).
        \\To better understand how similar misclassified poses can be found across many objects, see Figure ref:fig:tsne_color_ax.
        Compare and contrast this plot with the t-SNE of correctly classified poses
        (Figs.~\ref{fig:tsne_img_correct} \&~\ref{fig:tsne_color_correct}).

    }\label{fig:tsne_img_ax}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=1.0\columnwidth]{tsne_grid_label_incorrect.png}
    \caption{
        The same t-SNE as that in Figure ref:fig:tsne_img_ax but using a unique color to denote the 3D object used to render the adversarial image (i.e., Inception-v3's misclassification labels are not shown here).
        The original, high-resolution figure is available at https://goo.gl/TGgPgB.\\
        Compare and contrast this plot with the t-SNE of correctly classified poses
        (Figure ref:fig:tsne_color_correct).

    }\label{fig:tsne_color_ax}
\end{figure*}
\section{Transferability from the Inception-v3 classifier to the YOLO-v3
detector} \label{sec:yolo} Previous research has shown that object detectors can
be more robust to adversarial attacks than image classifiers
cite:lu2017-standard.  Here, we investigate how well our AXs generated for an
Inception-v3 classifier trained to perform 1,000-way image classification on
ImageNet cite:russakovsky2015imagenet transfer to YOLO-v3, a state-of-the-art
object detector trained on MS COCO cite:lin2014microsoft.

Note that while ImageNet has 1,000 classes, MS COCO has bounding boxes
classified into only 80 classes.  Therefore, among 30 objects, we only selected
the 13 objects that (1) belong to classes found in both the ImageNet and MS COCO
datasets; and (2) are also well recognized by the YOLO-v3 detector in common
poses.

\subsection{Class mappings from ImageNet to MS COCO} See
Table~\ref{tab:yolo_transfer_stats}a for 13 mappings from ImageNet labels to MS
COCO labels.  \subsection{Selecting 13 objects for the transferability test} For
the transferability test (Sec.~\ref{sec:transferability}), we identified the 13
objects (out of 30) that are well detected by the YOLO-v3 detector via the two
tests described below.  \subsubsection{YOLO-v3 correctly classifies 93.80% of
poses generated via yaw-rotation} We rendered 36 unique views for each object by
generating a render at every \(30^\circ{}\) yaw-rotation (see
Sec.~\ref{sec:quantitative_eval}).  Note that, across all objects, these
yaw-rotation views have an average accuracy of \(83.2%\) by the Inception-v3
classifier.  We tested them against YOLO-v3 to see whether the detector was able
to correctly find one single object per image and label it correctly.  Among 30
objects, we removed those that YOLO-v3 had an accuracy \(\leq 70%\), leaving 13
for the transferability test.  Across the remaining 13 objects, YOLO-v3 has an
accuracy of 93.80% on average (with an NMS threshold of \(0.4\) and a confidence
threshold of \(0.5\)).  Note that the accuracy was computed as the total number of
correct labels over the total number of bounding boxes detected (i.e., we did not
measure bounding-box IoU errors).  See class-specific statistics in
Table~\ref{tab:yolo_transfer_stats}.  This result shows that YOLO-v3 is
substantially more accurate than Inception-v3 on the standard object poses
generated by yaw-rotation (93.80% vs.  83.2%).  \subsubsection{YOLO-v3
correctly classifies 81.03% of poses correctly classified by Inception-v3}
Additionally, as a sanity check, we tested whether poses \emph{correctly
classified} by Inception-v3 transfer well to YOLO-v3.  For each object, we
randomly selected 30 poses that were \(100%\) correctly classified by
Inception-v3 with high confidence (\(p \geq 0.9\)).  The images were generated via
the random search procedure in the main text experiment
(Sec.~\ref{sec:random_search}).  Across the final 13 objects, YOLO-v3 was able
to correctly detect one single object per image and label it correctly at a
81.03% accuracy (see Table~\ref{tab:yolo_transfer_stats}c).

\subsection{Transferability test: YOLO-v3 fails on 75.5% of adversarial poses
misclassified by Inception-v3} \label{sec:transferability} For each object, we
collected 1,350 random adversarial poses (i.e., incorrectly classified by
Inception-v3) generated via the random search procedure
(Sec.~\ref{sec:random_search}).  Across all 13 objects and all adversarial
poses, YOLO-v3 obtained an accuracy of only \(24.50%\) (compared to \(81.03%\)
when tested on images correctly classified by Inception-v3).  In other words,
75.5% of adversarial poses generated for Inception-v3 also escaped the
detection\footnote{We were not able to check how many misclassification labels
by YOLO-v3 were the same as those by Inception-v3 because only a small set of 80
the MS COCO classes overlap with the 1,000 ImageNet classes.} of YOLO-v3 (see
Table~\ref{tab:yolo_transfer_stats}d for class-specific statistics).  Our result
shows adversarial poses transfer well across tasks (image classification \(\to\)
object detection), models (Inception-v3 \(\to\) YOLO-v3), and datasets (ImageNet
\(\to\) MS COCO).

\begin{table*}
    \centering
    \begin{tabular}{rllrrrrrrr}
        \toprule
        & \multicolumn{2}{c}{(a) Label mapping} & \multicolumn{2}{c}{(b) Accuracy on} & \multicolumn{2}{c}{(c) Accuracy on} & \multicolumn{3}{c}{(d) Accuracy on} \\
        & \multicolumn{2}{c}{} & \multicolumn{2}{c}{yaw-rotation poses} & \multicolumn{2}{c}{random poses} & \multicolumn{3}{c}{adversarial poses} \\
        \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-10}
        & ImageNet & MS COCO & \#/36 & acc (%) & \#/30 & acc (%) & \#/1350 & acc (%) & \(\Delta\)acc (%) \\
        \midrule
        1 & park bench & bench & 31 & 86.11 & 22 & 73.33 & 211 & 15.63 & 57.70 \\
        2 & bald eagle & bird & 34 & 94.11 & 24 & 80.00 & 597 & 44.22 & 35.78 \\
        3 & school bus & bus & 36 & 100.00 & 18 & 60.00 & 4 & 0.30 & 69.70 \\
        4 & beach wagon & car & 34 & 94.44 & 30 & 100.00 & 232 & 17.19 & 82.81 \\
        5 & tiger cat & cat & 26 & 72.22 & 25 & 83.33 & 181 & 13.41 & 69.93 \\
        6 & German shepherd & dog & 32 & 88.89 & 28 & 93.33 & 406 & 30.07 & 63.26 \\
        7 & motor scooter & motorcycle & 36 & 100.00 & 18 & 60.00 & 384 & 28.44 & 31.56 \\
        8 & jean & person & 36 & 100.00 & 29 & 96.67 & 943 & 69.85 & 26.81 \\
        9 & street sign & stop sign & 31 & 86.11 & 26 & 86.67 & 338 & 25.04 & 61.15 \\
        10 & moving van & truck & 36 & 100.00 & 24 & 80.00 & 15 & 1.11 & 78.89 \\
        11 & umbrella & umbrella & 35 & 97.22 & 25 & 83.33 & 907 & 67.19 & 16.15 \\
        12 & police van & car & 36 & 100.00 & 25 & 83.33 & 55 & 4.07 & 79.26 \\
        13 & trailer truck & truck & 36 & 100.00 & 22 & 73.33 & 26 & 1.93 & 71.41 \\
        \midrule
        & &Average & & 93.80 & & 81.03 & &24.50 & 56.53 \\ \bottomrule
    \end{tabular}
    \caption{
        Adversarial poses generated for a state-of-the-art ImageNet image classifier (here, Inception-v3) transfer well to an MS COCO detector (here, YOLO-v3).
        The table shows the YOLO-v3 detector's accuracy on: (b) object poses generated by a standard process of yaw-rotating the object; (c) random poses that are \(100%\) correctly classified by Inception-v3 with high confidence (\(p \geq 0.9\)); and (d) adversarial poses, i.e., \(100%\) misclassified by Inception-v3.\\\\
        (a) The mappings of 13 ImageNet classes onto 12 MS COCO classes.\\
        (b) The accuracy (``acc (%)'') of the YOLO-v3 detector on 36 yaw-rotation poses per object.\\
        (c) The accuracy of YOLO-v3 on 30 random poses per object that were correctly classified by Inception-v3.\\
        (d) The accuracy of YOLO-v3 on 1,350 adversarial poses (``acc (%)'') and the differences between c and d (``\(\Delta\)acc (%)'').
    }
    \label{tab:yolo_transfer_stats}
\end{table*}
\clearpage

\section{Experimental setup for the differentiable renderer}\label{sec:dr-expr}
For the gradient descent method (DR-G) that uses the approximate gradients
provided by the differentiable renderer \cite{kato2018neural} (DR), we set up
the rendering parameters in the DR to closely match those in the NR.  However,
there were still subtle discrepancies between the DR and the NR that made the
results (DR-G vs.  FD-G in Sec.~\ref{sec:comparing_methods}) not directly
comparable.  Despite these discrepancies (described below), we still believe the
FD gradients are more stable and informative than the DR gradients (i.e., FD-G
outperformed DR-G).\footnote{In preliminary experiments with only the DR (not
the NR), we also empirically found FD-G to be more stable and effective than
DR-G (data not shown).}\\
\subsec{DR setup} For all experiments with the DR, the camera was centered at
\((0, 0, 16)\) with an up direction \((0, 1, 0)\).  The object's spatial
location was constrained such that the object center was always within the
frame.  The depth values were constrained to be within \([-14, 14]\).  Similar
to experiments with the NR, we used the \medium lighting setting.

The ambient light color was set to white with an intensity 1.0, while the
directional light was set to white with an intensity 0.4.
Figure ref:fig:dr-demo shows an example school bus rendered under this \medium
lighting at different distances.
\begin{figure}[ht]
  \centering
  \subcaptionbox{School bus at \((0, 0, -14)\)}{{\includegraphics[width=.32\textwidth]{dr_demo_bus2.png}}}
  \subcaptionbox{School bus at \((0, 0, 0)\)}{{\includegraphics[width=.32\textwidth]{dr_demo_bus0.png}}}
  \subcaptionbox{School bus at \((0, 0, 14)\)}{{\includegraphics[width=.32\textwidth]{dr_demo_bus1.png}}}
  \caption{School bus rendered by the DR at different distances.}\label{fig:dr-demo}
\end{figure}

The known discrepancies between the experimental setups of FD-G (with the NR)
vs.  DR-G (with the DR) are:
\begin{enumerate}
    \item The exact \medium lighting parameters for the NR described in the main text (Sec.~\ref{sec:easily_confused}) did not produce similar lighting effects in the DR.
    Therefore, the DR lighting parameters described above were the result of manually tuning to qualitatively match the effect produced by the NR \medium lighting parameters.
    \item While the NR uses a built-in tessellation procedure that automatically tessellates input objects before rendering, we had to perform an extra pre-processing step of manually tessellating each object for the DR.
    While small, a discrepancy still exists between the two rendering results (Figure ref:fig:compare_tessellationb vs.  c).
\end{enumerate}


















\section{Gradient descent with the DR gradients}\label{sec:kr} In preliminary
experiments (data not shown), we found the DR gradients to be relatively noisy
when using gradient descent to find targeted adversarial poses (i.e., DR-G
experiments).  To mitigate this problem, we experimented with (1) parameter
augmentation (Sec.~\ref{sec:param-augm}); and (2) multi-view optimization
(Sec.~\ref{sec:multi-camera-optim}).  In short, we found parameter augmentation
helped and used it in DR-G.  However, when using the DR, we did not find
multiple cameras improved optimization performance and thus only performed
regular single-view optimization for DR-G.  \subsection{Parameter augmentation}%
\label{sec:param-augm} We performed gradient descent using the DR gradients
(DR-G) in an augmented parameter space corresponding to 50 rotations and one
translation to be applied to the original object vertices.  That is, we
backpropagated the DR gradients into the parameters of these pre-defined
transformation matrices.  Note that DR-G is given the same budget of \(100\) steps
per optimization run as FD-G and ZRS for comparison in
Sec.~\ref{sec:comparing_methods}.  The final transformation matrix is
constructed by a series of rotations followed by one translation, i.e.,
\begin{align*}
M = T\cdot R_{n-1}R_{n-2}\cdots R_0
\end{align*}
\noindent where \(M\) is the final transformation matrix, \(R_i\) the rotation
matrices, and \(T\) the translation matrix.  We empirically found that
increasing the number of rotations per step helped (a) improve the success rate
of hitting the target labels; (b) increase the maximum confidence score of the
found AXs; and (c) reduce the number of steps, i.e., led to faster convergence
(see Figure ref:fig:param-augm).  Therefore, we empirically chose \(n=50\) for all
DR-G experiments reported in the main text.

\begin{figure}[h]
  \centering
  \subcaptionbox{\(y\)-axis: success rate}{\includegraphics[width=0.25\linewidth]{n_rots_success.pdf}}%
  \subcaptionbox{\(y\)-axis: max confidence}{\includegraphics[width=0.25\linewidth]{n_rots_max_prob.pdf}}%
  \subcaptionbox{\(y\)-axis: mean number of steps}{\includegraphics[width=0.25\linewidth]{n_rots_expense.pdf}}
    \caption{
    We found that increasing the number of rotations (displayed in \(x\)-axes) per step helped:\\
    (a) improve the success rate of hitting the target labels;\\
    (b) increase the maximum confidence score of the found adversarial examples;\\
    (c) reduce the average number of steps required to find an AX, i.e., led to faster convergence.\\


}
\label{fig:param-augm}
\end{figure}
\subsection{Multi-view optimization}% \label{sec:multi-camera-optim}
Additionally, we attempted to harness multiple views (from multiple cameras) to
increase the chance of finding a target adversarial pose.  Multi-view
optimization did not outperform single-view optimization using the DR in our
experiments.  Therefore, we only performed regular single-view optimization for
DR-G.  We briefly document our negative results below.

Instead of backpropagating the DR gradient to a single camera looking at the
object in the 3D scene, one may set up multiple cameras, each looking at the
object from a different angle.  This strategy intuitively allows gradients to
still be backpropagated into the vertices that may be occluded in one view but
visible in some other view.  We experimented with six cameras and
backpropagating to all cameras in each step.  However, we only updated the
object following the gradient from the view that yielded the lowest loss among
all views.  One hypothesis is that having multiple cameras might improve the
chance of hitting the target.  In our experiments with the DR using 100 steps
per optimization run, multi-view optimization performed worse than single-view
in terms of both the success rate and the number of steps to converge.  We did
not compare all 30 objects due to the expensive computational cost, and only
report the results from optimizing two objects \class{bald eagle} and
\class{tiger cat} in Table~\ref{tab:v1v6}.  Intuitively, multi-view optimization
might outperform single-view optimization given a large enough number of steps.

\begin{table}[ht]
  \centering
  \begin{tabular}{l*{4}{r}}
    \toprule
    & \multicolumn{2}{c}{
        \class{bald eagle}} & \multicolumn{2}{c}{\class{tiger cat}} \\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}
    & Steps & Success rate & Steps & Success rate \\
    \midrule
    Single-view  & 71.80 & 0.44 & 90.70 & 0.15 \\
    Multi-view & 81.28 & 0.23 & 96.84 & 0.04 \\
    \bottomrule
  \end{tabular}
  \caption{
    Multi-view optimization performed worse than single-view optimization in both (a) the number of steps to converge and (b) success rates.
    We show here the results of two runs of optimizing with the \class{bald eagle} and \class{tiger cat} objects.
    The results are averaged over \(50\) target labels \(\times 50\) trials = \(2,500\) trials.
    Each optimization trial for both single- and multi-view settings is given the budget of \(100\) steps.
    \label{tab:v1v6}}
\end{table}
\newpage \section{3D Transformation Matrix}\label{sec:trans-mat} A rotation of
\(\theta\) around an arbitrary axis \((x, y, z)\) is given by the following
homogeneous transformation matrix.
\begin{equation}
  \label{eq:rotation-matrix}
  R =
  \begin{vmatrix*}[l]
    x x(1 - c) + c & x y(1 - c) - z s & x z(1 - c) + y s & 0\\
    x y(1 - c) + z s & y y(1 - c) + c & y z(1 - c) - x s & 0\\
    x z(1 - c) - y s & y z(1 - c) + x s & y z(1 - c) + c & 0\\
    0 & 0 & 0 & 1
  \end{vmatrix*}
\end{equation}
\noindent where \(s = \sin\theta\), \(c = \cos\theta\), and the axis is
normalized, i.e., \(x^2 + y^2 + z^2 = 1\).  Translation by a vector \((x, y, z)\)
is given by the following homogeneous transformation matrix.
\begin{equation}
  \label{eq:translation-matrix}
  T =
  \begin{vmatrix*}[l]
    1 & 0 & 0 & x\\
    0 & 1 & 0 & y\\
    0 & 0 & 1 & z\\
    0 & 0 & 0 & 1
  \end{vmatrix*}
\end{equation}
Note that in the optimization experiments with random search (RS) and
finite-difference gradients (FD-G), we dropped the homogeneous component for
simplicity, i.e., the rotation matrices of yaw, pitch, and roll are all $3 \times
3$.  The homogeneous component is only necessary for translation, which can be
achieved via simple vector addition.  However, in DR-G, we used the homogeneous
component because we had some experiments interweaving translation and rotation.
The matrix representation was more convenient for the DR-G experiments.  As they
are mathematically equivalent, this arbitrary implementation choice should not
alter our results.  \newpage
\begin{table*}[h]
    \begin{center}
        \begin{tabular}{lr}
            \toprule
            Object          & Accuracy (%) \\
            \midrule
            ambulance       & 3.64       \\
            backpack        & 8.63       \\
            bald eagle      & 13.26      \\
            beach wagon     & 0.60       \\
            cab             & 2.64       \\
            cell phone      & 14.97      \\
            fire engine     & 4.31       \\
            forklift        & 5.20       \\
            garbage truck   & 4.88       \\
            German shepherd & 9.61       \\
            \bottomrule
        \end{tabular}
        \begin{tabular}{lr}
            \toprule
            Object        & Accuracy (%) \\
            \midrule
            golfcart      & 2.14       \\
            jean          & 2.71       \\
            jeep          & 0.29       \\
            minibus       & 0.83       \\
            minivan       & 0.66       \\
            motor scooter & 20.49      \\
            moving van    & 0.45       \\
            park bench    & 5.72       \\
            parking meter & 1.27       \\
            pickup        & 0.86       \\
            \bottomrule
        \end{tabular}
        \begin{tabular}{lr}
            \toprule
            Object               & Accuracy (%) \\
            \midrule
            police van           & 0.95       \\
            recreational vehicle & 2.05       \\
            school bus           & 3.48       \\
            sports car           & 2.50       \\
            street sign          & 26.32      \\
            tiger cat            & 7.36       \\
            tow truck            & 0.87       \\
            traffic light        & 14.95      \\
            trailer truck        & 1.27       \\
            umbrella             & 49.88      \\
            \bottomrule
        \end{tabular}
    \end{center}
    \caption{The percent of three million random samples
        that were correctly
        classified by Inception-v3 \cite{szegedy2016rethinking} for each object.
        That is, for each lighting setting in \(\{ \bright, \medium, \dark\}\), we generated \(10^6\) samples.
        See Sec.~\ref{sec:random_search}
        for details on the sampling procedure.
    }
    \label{tab:sampling_stats}
\end{table*}





\begin{figure}[ht]
    \centering
    \subcaptionbox{\bright}{{\includegraphics[width=.32\textwidth]{bright_final.jpg}}}
    \subcaptionbox{\medium}{{\includegraphics[width=.32\textwidth]{medium_final.jpg}}}
    \subcaptionbox{\dark}{{\includegraphics[width=.32\textwidth]{dark_final.jpg}}}
    \caption{Renders of the \class{school bus} object using the NR \cite{modernGL} at three different lighting settings.
    The directional light intensities and ambient light intensities were \((1.2, 1.6)\), \((0.4, 1.0)\), and \((0.2, 0.5)\) for the \bright, \medium, and \dark settings, respectively.}\label{fig:light_intensity}
\end{figure}
\section{Adversarial poses were not found in ImageNet classes via a
nearest-neighbor search} \label{sec:nearest_neighbors} We performed a
nearest-neighbor search to check whether adversarial poses generated (in
Sec.~\ref{sec:easily_confused}) can be found in the ImageNet dataset.
~\\\subsec{Retrieving nearest neighbors from a single class corresponding to the
3D object} We retrieved the five nearest training-set images for each
adversarial pose (taken from a random selection of adversarial poses) using the
\layer{fc7} feature space from a pre-trained AlexNet \cite{Krizhevsky2012}.  The
Euclidean distance was used to measure the distance between two \layer{fc7}
feature vectors.  We did not find qualitatively similar images despite comparing
all \(\sim\)1,300 class images corresponding to the 3D object used to generate the
adversarial poses (e.g., \class{cellphone}, \class{school bus}, and
\class{garbage truck} in
Figs.~\ref{fig:nearest_cellphone},~\ref{fig:nearest_schoolbus},
and~\ref{fig:nearest_garbagetruck}).  This result supports the hypothesis that
the generated adversarial poses are out-of-distribution.  ~\\\subsec{Searching
from the validation set} We also searched the entire 50,000-image validation set
of ImageNet.  Interestingly, we found the top-5 nearest images were sometimes
from the same class as the /targeted/ misclassification label (see
Figure ref:fig:nearest_val_images).






\begin{figure*}
    \centering
    \includegraphics[width=0.9\columnwidth]{cellular_telephone_neighbors.jpg}
    \caption{
    For each adversarial example (leftmost), we retrieved the five nearest neighbors (five rightmost photos) from all \(\sim\)1,300 images in the \class{cellular phone} class.
    The Euclidean distance between a pair of images was computed in the \layer{fc7} feature space of a pre-trained AlexNet \cite{Krizhevsky2012}.
    The nearest photos from the class are mostly different from the adversarial poses.
    This result supports the hypothesis that the generated adversarial poses are out-of-distribution.
    The original, high-resolution figure is available at https://goo.gl/X31VXh.}
    \label{fig:nearest_cellphone}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=0.9\columnwidth]{school_bus_neighbors.jpg}
    \caption{
    For each adversarial example (leftmost), we retrieved the five nearest neighbors (five rightmost photos) from all \(\sim\)1,300 images in the \class{school bus} class.
    The Euclidean distance between a pair of images was computed in the \layer{fc7} feature space of a pre-trained AlexNet \cite{Krizhevsky2012}.
    The nearest photos from the class are mostly different from the adversarial poses.
    This result supports the hypothesis that the generated adversarial poses are out-of-distribution.
    The original, high-resolution figure is available at https://goo.gl/X31VXh.}
    \label{fig:nearest_schoolbus}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=0.9\columnwidth]{garbage_truck_neighbors.jpg}
    \caption{
    For each adversarial example (leftmost), we retrieved the five nearest neighbors (five rightmost photos) from all \(\sim\)1,300 images in the \class{garbage truck} class.
    The Euclidean distance between a pair of images was computed in the \layer{fc7} feature space of a pre-trained AlexNet \cite{Krizhevsky2012}.
    The nearest photos from the class are mostly different from the adversarial poses.
    This result supports the hypothesis that the generated adversarial poses are out-of-distribution.
    The original, high-resolution image is available at https://goo.gl/X31VXh.}
    \label{fig:nearest_garbagetruck}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.76\columnwidth]{7_pairs.jpg}
    \caption{
    In Sec.~\ref{sec:adversarial_training}, we trained an AlexNet classifier on the 1000-class ImageNet dataset augmented with 30 additional classes that contain adversarial poses corresponding to the 30 /known/ objects used in the main experiments.
    We also tested this model on 7 /held-out/ objects.
    Here, we show the renders of 7 pairs of (training-set object, held-out object).

    The 3D objects are rendered by the NR \cite{modernGL} at a distance of \((0,0,4)\).
    Below each image is its top-5 predictions by Inception-v3 \cite{szegedy2016rethinking}.
    The original, high-resolution figure is available at https://goo.gl/Li1eKU.}
    \label{fig:7_pairs}
\end{figure*}

\begin{figure*}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=1.0\columnwidth]{ambulance_30_AXs.jpg}
        \caption{\class{ambulance}}\label{fig:30_ax_ambulance}
    \end{subfigure}
   \begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=1.0\columnwidth]{school_bus_30_AXs.jpg}
    \caption{\class{school bus}}\label{fig:30_ax_schoolbus}
   \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=1.0\columnwidth]{street_sign_30_AXs.jpg}
        \caption{\class{street sign}}\label{fig:30_ax_stopsign}
    \end{subfigure}
    \caption{
    30 random adversarial examples misclassified by Inception-v3 \cite{szegedy2016rethinking} with high confidence (\(p \geq 0.9\)) generated from 3 objects: \class{ambulance}, \class{school bus}, and \class{street sign}.
    Below each image is the top-1 prediction label and confidence score.
    The original, high-resolution figures for all 30 objects are available at https://goo.gl/rvDzjy.}\label{fig:30_ax}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1.0\columnwidth]{5_unique_objects_per_class.jpg}
    \caption{
    For each target class (e.g., \class{accordion piano}), we show five adversarial poses generated from five unique 3D objects.
    Adversarial poses are interestingly found to be homogeneous for some classes, e.g., \class{safety pin}.
    However, for most classes, the failure modes are heterogeneous.
    The original, high-resolution figure is available at https://goo.gl/37HYcE.
    }\label{fig:common_failures_per_label}
\end{figure*}

\begin{figure*}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.9\columnwidth]{cellphone_real_AXs.jpg}
        \caption{\class{cellular phone}}\label{fig:real_ax_cellphone}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.9\columnwidth]{jeans_real_AXs.jpg}
        \caption{\class{jeans}}\label{fig:real_ax_jeans}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.9\columnwidth]{stop_sign_real_AXs.jpg}
        \caption{\class{street sign}}\label{fig:real_ax_stopsign}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.9\columnwidth]{umbrella_real_AXs.jpg}
        \caption{\class{umbrella}}\label{fig:real_ax_umbrella}
    \end{subfigure}
\caption{
     Real-world, high-confidence adversarial poses can be found by taking photos from strange angles of a familiar object, here, \class{cellular phone}, \class{jeans}, \class{street sign}, and \class{umbrella}.
     While Inception-v3 \cite{szegedy2016rethinking} can correctly predict the object in canonical poses (the top-left image in each panel), the model misclassified the same objects in unusual poses.
     Below each image is its top-1 prediction label and confidence score.

     We took real-world videos of these four objects and extracted these misclassified poses from the videos.
    The original, high-resolution figures are available at https://goo.gl/zDWcjG.
}\label{fig:real_ax}
\end{figure*}


\newpage
\begin{figure*}[t]
    \begin{center}
        \includegraphics[width=\textwidth]{sensitivity.pdf}
    \end{center}
    \caption{
        Inception-v3 \cite{szegedy2016rethinking} is sensitive to single parameter disturbances of object poses that had originally been correctly classified.

        For each object, we found 100 correctly classified 6D poses via a random sampling procedure (Sec.~\ref{sec:landscape}).
        Given each such pose, we re-sampled one parameter (shown on top of each panel, e.g., yaw) 100 times, yielding 100 classifications, while holding the other five pose parameters constant.
        In each panel, for each object (e.g., \class{ambulance}), we show an error plot for all resultant \(100 \times 100 = 10,000\) classifications.
        Each circle denotes the mean misclassification rate (``Fail Rate'') for each object, while the bars enclose one standard deviation.
        Across all objects, Inception-v3 is more sensitive to changes in yaw, pitch, roll, and depth (``z\_delta'') than spatial changes (``x\_delta'' and ``y\_delta'').
    }
    \label{fig:sensitivity}
\end{figure*}

\newpage
\begin{figure*}[t]
    \begin{center}
        \includegraphics[width=1.0\textwidth]{nearest_val_images.jpg}
    \end{center}
    \caption{
        For each adversarial example (leftmost), we retrieved the five nearest neighbors (five rightmost photos) from the 50,000-image ImageNet validation set.
        The Euclidean distance between a pair of images was computed in the \layer{fc7} feature space of a pre-trained AlexNet \cite{Krizhevsky2012}.
        Below each adversarial example (AX) is its Inception-v3 \cite{szegedy2016rethinking} top-1 prediction label and confidence score.
        The associated ground-truth ImageNet label is beneath each retrieved photo.
        Here, we show an interesting, cherry-picked collection of cases where the nearest photos (in the \layer{fc7} feature space) are also qualitatively similar to the reference AX and sometimes come from the exact same class as the AX's predicted label.
        More examples are available at https://goo.gl/8ib2PR.
        }
    \label{fig:nearest_val_images}
\end{figure*}

* Harness Adversarial Samples
:PROPERTIES:
:CUSTOM_ID: part:harness-adversarial-samples
:END:

* COMMENT Conclusion
:PROPERTIES:
:CUSTOM_ID: part:conclusion
:END:

* COMMENT References                                                         :ignore:

#+LaTeX: \printbibliography

* Footnotes

[fn:1] https://github.com/gongzhitaao/adversarial-text

[fn:2] https://github.com/spotify/annoy

[fn:3] https://keras.io/

[fn:4] http://www.daviddlewis.com/resources/testcollections/reuters21578/
